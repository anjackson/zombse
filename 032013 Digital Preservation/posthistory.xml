<?xml version="1.0" encoding="utf-8"?>
<posthistory>
  <row Id="1" PostHistoryTypeId="2" PostId="1" RevisionGUID="99f5e9eb-7966-4f4a-adac-325876b3fee3" CreationDate="2013-02-26T21:29:42.120" UserId="16" Text="Our genealogy society has seven 900-page printed volumes, published in the 1870s and 1980s-1990s. The font is typewriter-like, which does not work well with OCR. We own the publishing rights, and have experimented with tearing apart individual volumes to feed through a sheet-fed scanner. The results are good for creating an image PDF but not for text. As you can imagine, an OCR error which changes a marriage date from 1868 to 1888 is significant, and unacceptable.&#xD;&#xA;&#xD;&#xA;It's a simple matter to strip a book and create a book on CD as our books go out of print. However, moving forward into the 21st century, we would like to convert the information to digital form (a standard genealogy database). After three years of trying to figure something out, our only solution is to type the thousands of pages from scratch into our genealogy database.&#xD;&#xA;&#xD;&#xA;One thing which makes OCR particularly difficult is that the tails of letters tend to be faint. That is, the bottom of the &quot;g&quot; in &quot;Strong&quot; is OCR'd as a &quot;q&quot;. Since this is the Strong family genealogy, that's a problem! I can search and replace, but again the error rate remains too high. With a typewriter font, my OCR program &quot;cleans&quot; nearly all punctuation out, so no periods or semicolons remain.&#xD;&#xA;&#xD;&#xA;We'd be grateful for any suggestions in converting our print books to database form." />
  <row Id="2" PostHistoryTypeId="1" PostId="1" RevisionGUID="99f5e9eb-7966-4f4a-adac-325876b3fee3" CreationDate="2013-02-26T21:29:42.120" UserId="16" Text="Converting our printed genealogies into digital form" />
  <row Id="3" PostHistoryTypeId="3" PostId="1" RevisionGUID="99f5e9eb-7966-4f4a-adac-325876b3fee3" CreationDate="2013-02-26T21:29:42.120" UserId="16" Text="&lt;ocr&gt;&lt;digitize&gt;&lt;genealogy&gt;" />
  <row Id="4" PostHistoryTypeId="2" PostId="2" RevisionGUID="4dc7b6f2-02d3-47e3-aa0d-744eb0fbd568" CreationDate="2013-02-26T21:42:21.077" UserId="22" Text="Although this doesn't solve your OCR problem, you might have a look at [LeafSeek](http://www.leafseek.com/). It's a free and open source program for creating searchable online databases specifically about genealogy. It's fairly robust and sports an attractive interface.  &#xD;&#xA;&#xD;&#xA;Regarding OCR, what resolution are you scanning at? I've had fairly good results with typewriter documents by scanning at 300 PPI, sometimes 600 PPI. Also, the software you're using affects the output. ABBY FineReader usually does a good job, but I tend to use Adobe Acrobat's ClearScan OCR (I think it's in versions 9+). It also improves the image quality." />
  <row Id="5" PostHistoryTypeId="2" PostId="3" RevisionGUID="c2359594-3f43-45ae-a521-b0b7933db531" CreationDate="2013-02-26T21:46:49.267" UserId="14" Text="OK, let's get this thing going ... I've downloaded JHOVE2 and attempted to build it, using Maven through Eclipse with the target &quot;compile&quot;. I get the following error:&#xD;&#xA;&#xD;&#xA;[ERROR] Failed to execute goal on project jhove2: Could not resolve dependencies for project org.jhove2:jhove2:jar:2.0.0: Failed to collect dependencies for [commons-logging:commons-logging-api:jar:1.1 (compile), commons-logging:commons-logging:jar:1.1.1 (compile), org.springframework:spring-context:jar:2.5.6 (compile), org.springframework:spring-test:jar:2.5.6 (test), log4j:log4j:jar:1.2.14 (compile), junit:junit:jar:4.4 (test), jdom:jdom:jar:1.0 (compile), soap:soap:jar:2.3.1 (compile), xerces:xercesImpl:jar:2.9.1 (compile), xml-resolver:xml-resolver:jar:1.2 (compile), net.sf:jargs:jar:1.0 (compile), org.mvel:mvel2:jar:2.0.18 (compile), org.geotools:gt-main:jar:2.6.5 (compile), org.geotools:gt-shapefile:jar:2.6.5 (compile), com.sleepycat:je:jar:4.0.103 (compile)]: Failed to read artifact descriptor for net.sf:jargs:jar:1.0: Could not transfer artifact net.sf:jargs:pom:1.0 from/to JBOSS (http://repository.jboss.org/maven2/): Access denied to http://repository.jboss.org/maven2/net/sf/jargs/1.0/jargs-1.0.pom. Error code 403, Forbidden -&gt; [Help 1]&#xD;&#xA;&#xD;&#xA;This may be because JHOVE2 hasn't been updated in a few years, so the dependencies in its pom.xml perhaps are no longer valid. On the other hand, every time I touch Maven something bad happens, so I may just be doing something dumb. Are there any changes to pom.xml or other tricks which can get the build working?" />
  <row Id="6" PostHistoryTypeId="1" PostId="3" RevisionGUID="c2359594-3f43-45ae-a521-b0b7933db531" CreationDate="2013-02-26T21:46:49.267" UserId="14" Text="How do I get JHOVE2 to build?" />
  <row Id="7" PostHistoryTypeId="3" PostId="3" RevisionGUID="c2359594-3f43-45ae-a521-b0b7933db531" CreationDate="2013-02-26T21:46:49.267" UserId="14" Text="&lt;software&gt;&lt;jhove2&gt;" />
  <row Id="8" PostHistoryTypeId="2" PostId="4" RevisionGUID="6e5ff3d7-2f0b-43ef-bff6-b50fe08fd69f" CreationDate="2013-02-26T21:46:51.753" UserId="30" Text="For example, 8&quot; floppy disks are not ideal because the rate of change in storage technology has rendered them obsolete.&#xD;&#xA;&#xD;&#xA;Given that that will continue to occur, what factors go into deciding on future-proofing our backup media choices?" />
  <row Id="9" PostHistoryTypeId="1" PostId="4" RevisionGUID="6e5ff3d7-2f0b-43ef-bff6-b50fe08fd69f" CreationDate="2013-02-26T21:46:51.753" UserId="30" Text="What physical format(s) are going to be easiest to open in the future?" />
  <row Id="10" PostHistoryTypeId="3" PostId="4" RevisionGUID="6e5ff3d7-2f0b-43ef-bff6-b50fe08fd69f" CreationDate="2013-02-26T21:46:51.753" UserId="30" Text="&lt;backup-and-recovery&gt;&lt;future-proofing&gt;" />
  <row Id="11" PostHistoryTypeId="2" PostId="5" RevisionGUID="a76f122d-671f-427e-9c17-40a62a3f41dd" CreationDate="2013-02-26T21:48:22.687" UserId="26" Text="I'm looking for a collection of the most reputable sources on the lifetime of various types of media (hard drives, optical discs, magnetic tape, flash memory).&#xD;&#xA;&#xD;&#xA;The ultimate question would be, what type of media is currently the best option for long-term storage?" />
  <row Id="12" PostHistoryTypeId="1" PostId="5" RevisionGUID="a76f122d-671f-427e-9c17-40a62a3f41dd" CreationDate="2013-02-26T21:48:22.687" UserId="26" Text="Comparative lifetimes of digital media" />
  <row Id="13" PostHistoryTypeId="3" PostId="5" RevisionGUID="a76f122d-671f-427e-9c17-40a62a3f41dd" CreationDate="2013-02-26T21:48:22.687" UserId="26" Text="&lt;storage&gt;&lt;lifetime&gt;" />
  <row Id="14" PostHistoryTypeId="2" PostId="6" RevisionGUID="b4b9175b-ac69-400f-a9a7-d3aa9f2e121c" CreationDate="2013-02-26T21:49:59.723" UserId="30" Text="I was recently asked to help recover years of sermon and book notes from a pastor friend. They were all done in WP5.1 and saved to floppy disks.&#xD;&#xA;&#xD;&#xA;Given that I both do not have a floppy drive and no access to old word processing software (specifically one compatible with WordPerfect 5.1), what is the best way to recover this data?&#xD;&#xA;&#xD;&#xA;Given the likelihood of similar requests arising in the future, what is the best way *now* to start modernizing and maintaining my and others' digital archives so they will always be readily referenceable?" />
  <row Id="15" PostHistoryTypeId="1" PostId="6" RevisionGUID="b4b9175b-ac69-400f-a9a7-d3aa9f2e121c" CreationDate="2013-02-26T21:49:59.723" UserId="30" Text="How can I recover old digital formats whose readers are no longer extant?" />
  <row Id="16" PostHistoryTypeId="3" PostId="6" RevisionGUID="b4b9175b-ac69-400f-a9a7-d3aa9f2e121c" CreationDate="2013-02-26T21:49:59.723" UserId="30" Text="&lt;record-modernization&gt;&lt;out-of-date-format&gt;&lt;data-recovery&gt;" />
  <row Id="17" PostHistoryTypeId="2" PostId="7" RevisionGUID="4115d50f-192f-42ff-b8d5-8225ee2d706d" CreationDate="2013-02-26T21:56:39.737" UserId="26" Text="If the floppy is 3.5&quot;, then you could purchase a floppy [reader][1] that connects to a USB port.&#xD;&#xA;&#xD;&#xA;And if the floppy is 5.25&quot;, you can still buy an old internal floppy drive on Ebay, and use [this][2] adapter to connect it via USB.&#xD;&#xA;&#xD;&#xA;As for software, you can easily find ancient software on abandonware and/or torrent sites. Even if the precise version of WordPerfect can't be found, I would think that a slightly later version would be able to open and convert the files.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.newegg.com/Product/Product.aspx?Item=N82E16821105004&#xD;&#xA;  [2]: http://www.deviceside.com/fc5025.html" />
  <row Id="18" PostHistoryTypeId="2" PostId="8" RevisionGUID="fd571b8c-4dc8-4e84-a962-ef200237fe94" CreationDate="2013-02-26T21:57:09.317" UserId="32" Text="Organizations have the option of building the expertise to navigate the intricacies of imaging and copying from extremely varied digital media or outsourcing these tasks to other professionals who are already experts (ie data recovery pros, forensics experts, digital video preservation services). Assume that any outsourcing would be within specific parameters assigned by the organization." />
  <row Id="19" PostHistoryTypeId="1" PostId="8" RevisionGUID="fd571b8c-4dc8-4e84-a962-ef200237fe94" CreationDate="2013-02-26T21:57:09.317" UserId="32" Text="What are some advantages and disadvantages of outsourcing extraction/transfer of digital content from media?" />
  <row Id="20" PostHistoryTypeId="3" PostId="8" RevisionGUID="fd571b8c-4dc8-4e84-a962-ef200237fe94" CreationDate="2013-02-26T21:57:09.317" UserId="32" Text="&lt;media&gt;&lt;forensics&gt;&lt;imaging&gt;" />
  <row Id="21" PostHistoryTypeId="6" PostId="6" RevisionGUID="93a8ad2a-7b9e-479d-8b49-f50d066c687f" CreationDate="2013-02-26T22:01:47.023" UserId="19" Comment="edited tags" Text="&lt;record-modernization&gt;&lt;out-of-date-format&gt;&lt;data-recovery&gt;&lt;forensics&gt;" />
  <row Id="22" PostHistoryTypeId="2" PostId="9" RevisionGUID="c1a359a0-9e68-4d17-aa45-71dcf3199548" CreationDate="2013-02-26T22:16:51.950" UserId="32" Text="Here is a good source for CDs and DVDs: http://www.cci-icc.gc.ca/publications/notes/19-1-eng.aspx (CCI Notes N19/1 Longevity of Recordable CDs and DVDs (2010)&#xD;&#xA;, Canadian Conservation Institute)re" />
  <row Id="24" PostHistoryTypeId="2" PostId="10" RevisionGUID="12988bad-e191-49fd-8962-4f2be829e42b" CreationDate="2013-02-26T22:29:44.213" UserId="30" Text="For archival purposes, we are planning to scan about a hundred years' worth of physical records (from timecards to schematics) for our railway historical society.&#xD;&#xA;&#xD;&#xA;What is the best way to evaluate large-format scanners for this task? This is a new front for us, so I am not sure where to start looking beyond CDW." />
  <row Id="25" PostHistoryTypeId="1" PostId="10" RevisionGUID="12988bad-e191-49fd-8962-4f2be829e42b" CreationDate="2013-02-26T22:29:44.213" UserId="30" Text="Evaluating large-format scanners" />
  <row Id="26" PostHistoryTypeId="3" PostId="10" RevisionGUID="12988bad-e191-49fd-8962-4f2be829e42b" CreationDate="2013-02-26T22:29:44.213" UserId="30" Text="&lt;digitize&gt;&lt;scanner&gt;" />
  <row Id="27" PostHistoryTypeId="2" PostId="11" RevisionGUID="5050bbda-135a-47d3-8639-0c4ebc3c69f8" CreationDate="2013-02-26T22:33:46.490" UserId="42" Text="Speaking as an individual who has a fair amount of content to convert (home movies and family photos), there are basically two paths for in-house conversion: professional equipment and minimalist equipment.&#xD;&#xA;&#xD;&#xA;Professional equipment for converting some types of media (photos) can be extremely expensive, but professional equipment is also much better at handling large volumes.&#xD;&#xA;&#xD;&#xA;Minimalist equipment is much less expensive and slower, and may also produce lower quality results (depending on the media type and investment).&#xD;&#xA;&#xD;&#xA;Taking photos as an example, there are four types of solutions that I am familiar with:&#xD;&#xA;&#xD;&#xA;a) desktop scanner - cost $100-200&#xD;&#xA;&#xD;&#xA;b) automatic feed/work group scanner - cost around $500&#xD;&#xA;&#xD;&#xA;c) DSLR and copy stand/table - cost $1000 and up&#xD;&#xA;&#xD;&#xA;d) Professional scanner - cost (I don't know myself, but it's in the $10,000 or more range)&#xD;&#xA;&#xD;&#xA;When I started to work on my own photo conversion project, I talked to a senior editor at a photography magazine and his professional advice was that solutions a and b would yield mediocre quality. On the other hand, solution c would give results that were moderately close to d. Solutions a and c are the slowest, b and d the fastest.&#xD;&#xA;&#xD;&#xA;So when an organization is considering a conversion project, they first have to figure out the quality, budget and schedule constraints that they are working under. Then they have to determine if there are any special factors, such as proprietary material that you don't really want to have to ship out.&#xD;&#xA;&#xD;&#xA;There are really too many possible scenarios to give any hard and fast rules as to whether it's better to do it in-house or contract. Organizations should do their research. Be realistic about the time, cost and inconvenience, and it should become clear which solution is the most cost effective." />
  <row Id="28" PostHistoryTypeId="2" PostId="12" RevisionGUID="b121e36c-b993-4f2a-9e83-3607be2f7bac" CreationDate="2013-02-26T22:51:41.177" UserId="42" Text="All media degrades over time, not even considering obsolescence of the equipment itself. As a result, you have to be prepared to revisit your technology choices on a regular basis.&#xD;&#xA;&#xD;&#xA;So the best solution will take into account speed, reliability, media lifetime, cost per unit of data, and the total amount of data. Many organizations may find that a cloud solution such as CrashPlan offers an extremely attractive solution for redundant off-site backups. The cheap solutions (such as tape) can have hidden costs in terms of time, convenience and risk of data loss. It is important for organizations to be realistic and careful to insure proper redundancy." />
  <row Id="30" PostHistoryTypeId="2" PostId="13" RevisionGUID="84a8eb0a-b240-4c07-bf12-b6ac70edc9e5" CreationDate="2013-02-26T23:12:27.833" UserId="62" Text="As far as I can tell PDF/A is a very well-considered format for archiving. It requires documents to contain no external dependencies like fonts or images." />
  <row Id="31" PostHistoryTypeId="2" PostId="14" RevisionGUID="0b050869-65f6-4504-b082-777ae262a295" CreationDate="2013-02-26T23:17:36.213" UserId="60" Text="You're right... the repository that was used at the time that the JHOVE2 pom.xml was created has been changed (so it can't resolve its dependency on jargs).  You can fix this problem by making a simple change to the pom.xml file which is in the root directory of the JHOVE2 project that you downloaded.&#xD;&#xA;&#xD;&#xA;If you open the pom.xml file with a text editor look for the following section:&#xD;&#xA;&#xD;&#xA;    &lt;repositories&gt;&#xD;&#xA;            &lt;!-- for jargs GNU Command Line Parser --&gt;&#xD;&#xA;            &lt;repository&gt;&#xD;&#xA;                    &lt;id&gt;JBOSS&lt;/id&gt;&#xD;&#xA;                    &lt;name&gt;JBoss Repository&lt;/name&gt;&#xD;&#xA;&#xD;&#xA;You'll see there is a &amp;lt;url&amp;gt; element right after the &amp;lt;name&amp;gt; element listed above.  Change that to:&#xD;&#xA;&#xD;&#xA;    &lt;url&gt;https://repository.jboss.org/nexus/content/repositories/thirdparty-releases/&lt;/url&gt;&#xD;&#xA;&#xD;&#xA;then save the file.  The next step would be to run Maven to install it (usually done at the command line with: `mvn install` (but you can do it from within Eclipse too)). However when I do that with a fresh download of JHOVE2 it fails with errors in the tests that are a part of the build.  If I instead build it with:&#xD;&#xA;&#xD;&#xA;    mvn -DskipTests=true install&#xD;&#xA;&#xD;&#xA;It will build JHOVE2 while skipping the tests.  I didn't look into why the tests are failing (and they might not for you -- I just grabbed the latest download from JHOVE2's BitBucket page).  Hope this helps!" />
  <row Id="32" PostHistoryTypeId="2" PostId="15" RevisionGUID="077a9fd7-e438-40f8-a658-6a39a4ed7c7e" CreationDate="2013-02-26T23:47:52.803" UserId="57" Text="I don't think this can be answered categorically if it's an electronic format. Like most hard questions, the answer correct here is **&quot;it depends&quot;**.&#xD;&#xA;&#xD;&#xA;The answer is going to depend on when you expect *&quot;the future&quot;* to be at.  &#xD;&#xA;&#xD;&#xA; - Is it 1 year?  Normal storage, like USB and external hard drives, are probably good enough.  Any of a number of on online storage services would also fit the bill here.&#xD;&#xA; - Is it 10 years?  You'd need something that doesn't degrade over time, so magnetic media could be out of the question.  Although there's stories of people firing up 8&quot; drives and getting data off from old floppies, I wouldn't count on that.  Something like storing your data on the [M-Disc from Milleniatta][1], along with at least two or three DVD drives should be enough.  USB seems to have staying power (and backward compatibility up to now), so you should be able to find a USB drive somewhere to connect it to.&#xD;&#xA; - Is it 50 years?  Now it gets interesting.  Think about this.  50 years ago, computers were only used by the government or big corporations.  So, what will 50 more years do to computing?  What we now know as &quot;computers&quot; might actually be invisible and wireless, so you'd have nothing to &quot;read&quot; your data with.  Electricity should still be available (assuming no nuclear winter, asteroid hitting earth, etc), so maybe saving all your data on something like the M-Disc mentioned above, plus a fully functional laptop computer, with all the applications/drivers needed to read the data AND full instructions on how to use it (fifty years into the future, it might not be yourself that's reading the data), including electricity/voltage specs.  At least you'll be able to read the data, even if you can't find a 50-year old computer by then (have you seen an old UNIVAC lying around these days?  Yeah, didn't think so).&#xD;&#xA; - Is it 100+ years?  At this point, honestly, I wouldn't trust any company's claim of &quot;eternal&quot;, &quot;permanent&quot;, etc.  Also, I wouldn't trust there being electricity at all.  From the archival/historical point of view, the only things that have withstood the test of time are well-cared-for papyrus/books, and carved rock (think the Rosetta Stone).  You could think about printing all your data in binary format in archival-quality paper, and storing all that in an airtight container. Think big QR codes with lots of redundancy/error correction built in, and/or hex values.  You'd have to make sure the printer/ink you use is permanent, doesn't fade, and doesn't get &quot;sticky&quot; over time.  Make sure you include a plain-english (maybe even multiple languages) document specifying how to decode the QR codes if you go this way, and a basic explanation of hex numbers.  A &quot;primer&quot; if you will.&#xD;&#xA;&#xD;&#xA;- Is it 1000+ years?  Only carved rock has withstood thousands of years in the past.  Even huge metal monuments have been melted down for ammunition and weapons, or stolen, while carved rock is usually left alone during times of hardship.  You'd probably have to go with granite or maybe even pure quartz, along with a computer-driven laser and sandblaster.  The language to use could be debatable, but you'd have to include a very basic primer (both numbers and letters, think [&quot;Contact&quot;][2]) to make sure people thousands of years from now are still able to decode it.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.mdisc.com&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Contact_%28film%29" />
  <row Id="33" PostHistoryTypeId="2" PostId="16" RevisionGUID="f09329d5-7099-42fe-8a10-acc8d71f53e5" CreationDate="2013-02-26T23:49:08.993" UserId="64" Text="I have some live music recorded in 1971 on reel-to-reel tapes that I transferred to a digital format around 1987.  I no longer have access to the reel-to-reel tapes.  At the time, there was no PCM recording option.  The recording studio had a large box that took analog input and converted it to digital and stored the digital signal on the video section of VHS cassettes as a video signal that encoded the audio.&#xD;&#xA;&#xD;&#xA;I do not recall the encoder but I have the VHS tapes and would like to recover the audio.&#xD;&#xA;&#xD;&#xA;How could I go about recovering the audio that is encoded on these VHS tapes?  What might have been the machine encoding scheme used and if identified, how could this be decoded today?" />
  <row Id="34" PostHistoryTypeId="1" PostId="16" RevisionGUID="f09329d5-7099-42fe-8a10-acc8d71f53e5" CreationDate="2013-02-26T23:49:08.993" UserId="64" Text="How can I recover audio that was encoded on VHS cassettes around 1987?" />
  <row Id="35" PostHistoryTypeId="3" PostId="16" RevisionGUID="f09329d5-7099-42fe-8a10-acc8d71f53e5" CreationDate="2013-02-26T23:49:08.993" UserId="64" Text="&lt;forensics&gt;&lt;out-of-date-format&gt;&lt;data-recovery&gt;&lt;vhs&gt;&lt;pcm&gt;" />
  <row Id="36" PostHistoryTypeId="2" PostId="17" RevisionGUID="bfd40ca1-2890-466c-b476-7d434a3913a8" CreationDate="2013-02-27T00:15:11.630" UserId="67" Text="I have many photos taken on analog that I'd like to scan to digital form to preserve them. Some will be scanned in using a flatbed and some using a slide scanner.&#xD;&#xA;&#xD;&#xA;What format (JPEG?) and resolution should I be aiming for ? I see resolution on digital cameras has evolved quite fast. We've gone from 1 megabit to 15 megabit and that might increase further. Should I use just JPEG or use a non-lossy format in parallel to accomodate further developments ?&#xD;&#xA;&#xD;&#xA;Furthermore, if these are family photos or the like, should anything be done to adjust colours or would a simple scan be fine ?" />
  <row Id="37" PostHistoryTypeId="1" PostId="17" RevisionGUID="bfd40ca1-2890-466c-b476-7d434a3913a8" CreationDate="2013-02-27T00:15:11.630" UserId="67" Text="What is the ideal format to scan analog photos?" />
  <row Id="38" PostHistoryTypeId="3" PostId="17" RevisionGUID="bfd40ca1-2890-466c-b476-7d434a3913a8" CreationDate="2013-02-27T00:15:11.630" UserId="67" Text="&lt;digitize&gt;&lt;scanner&gt;&lt;photos&gt;" />
  <row Id="39" PostHistoryTypeId="2" PostId="18" RevisionGUID="b1dc9198-132a-4004-96f0-729bd99cbbb3" CreationDate="2013-02-27T00:27:00.893" UserId="29" Text="It's better to think of media migrations instead of relying on a single &quot;best&quot; format to ensure data integrity over time.&#xD;&#xA;&#xD;&#xA;Preservation is an active process. No matter what format you choose, magnetic, optical, ink, carved-into-stone, it will degrade over time. Future-proofing data integrity requires that your storage technology loses bits in a consistent, correctable manner over its service life, and that you can migrate your data to a new media relatively easily when that service life ends.&#xD;&#xA;&#xD;&#xA;Given that, the best storage media for any organization today depends on the budget and staff expertise of the organization. Larger organizations like [LDS](http://documents.el-una.org/923/1/ELUNA_2012_Presentation_Final_2012_5_7.pdf) can afford a large tape array, while smaller organizations can get away with a cloud service [Backblaze](http://www.backblaze.com/) or [Crashplan](http://www.crashplan.com). Just remember to make it easy to get your data back out." />
  <row Id="40" PostHistoryTypeId="2" PostId="19" RevisionGUID="333be15a-c3e6-441c-bb3b-bb65d6017fac" CreationDate="2013-02-27T00:48:25.733" UserId="68" Text="My parents used a Macintosh SE which they bought back in 1988 for their publishing and tutoring business. This was running System 6 and they also used to do their accounts on this machine. However since this machine cost an arm and a leg at the time and didn't feel the need to buy a new computer anyway, they kept on using it almost on a daily basis until 2003 when they bought a Sony VAIO. The mac however, I kept checking up on it and playing around with it until it finally packed in back around 2006.&#xD;&#xA;&#xD;&#xA;As a result they gathered a large number of floppy disks, most of them 1.4 MB floppies, but most interestingly are a few System 6 disks and other software. The last time I inserted them into the machine, they still worked ok. I'm wondering what the best use of these would be. Is it worth preserving them and if so how? And how would I go about backing up the data on these disks?" />
  <row Id="41" PostHistoryTypeId="1" PostId="19" RevisionGUID="333be15a-c3e6-441c-bb3b-bb65d6017fac" CreationDate="2013-02-27T00:48:25.733" UserId="68" Text="Extracting data from and preserving 3.5in floppy disks used in a Macintosh SE (1988)" />
  <row Id="42" PostHistoryTypeId="3" PostId="19" RevisionGUID="333be15a-c3e6-441c-bb3b-bb65d6017fac" CreationDate="2013-02-27T00:48:25.733" UserId="68" Text="&lt;out-of-date-format&gt;&lt;software&gt;" />
  <row Id="43" PostHistoryTypeId="2" PostId="20" RevisionGUID="248a15c3-3770-43fd-b87f-c302b3dcbf1d" CreationDate="2013-02-27T00:50:31.620" UserId="29" Text="The largest factor in deciding whether to outsource or to convert in-house is the scale of the project.&#xD;&#xA;&#xD;&#xA;Outsourcing is a variable cost. If the project expands in scope, the costs will increase roughly linearly.&#xD;&#xA;&#xD;&#xA;In-house digitization has a fixed cost (equipment) and variable cost (staff time). If the project expands in scope, the fixed cost per digitized object will decrease inversely while the variable cost will increase roughly linearly.&#xD;&#xA;&#xD;&#xA;This breakdown holds true for other assets. For instance, outsourcing allows you direct access to expertise, at a price that scales with the size of the project. Inhouse builds the expertise of your staff. The larger the project the less the cost of this expertise, but also the greater time staff will have to devote to the project.&#xD;&#xA;&#xD;&#xA;At a given project size, it makes sense to invest in the equipment and staff to keep a project in-house, but smaller projects will likely be more economical to outsource." />
  <row Id="44" PostHistoryTypeId="2" PostId="21" RevisionGUID="20aaaca6-dec2-4196-b634-9af5d00b06ab" CreationDate="2013-02-27T00:55:17.273" UserId="26" Text="When scanning analog photos, the JPG format should be sufficient (with a &quot;high&quot; quality setting; i.e. low compression).  There's little point in storing it in a lossless format. The reason why DSLR cameras save photos in a Raw format is to preserve as much data as possible from the original sensor for postprocessing.  But since the analog photo is already developed, there's no point in trying to squeeze out detail that doesn't exist.&#xD;&#xA;&#xD;&#xA;As for resolution, I've actually found that 300 dpi is quite enough. Any more would be going beyond the &quot;resolution&quot; of the physical photo paper, and wouldn't really add any more detail. (And 3000 dpi for slides)&#xD;&#xA;&#xD;&#xA;You shouldn't need to adjust the colors, either (except to make sure the scanner is scanning the photo correctly in the first place)." />
  <row Id="45" PostHistoryTypeId="2" PostId="22" RevisionGUID="13981b63-c5e7-4738-a2c1-754275042ce0" CreationDate="2013-02-27T01:07:08.150" UserId="74" Text="I have large collection of audio casettes, I'm looking for the way to digitize them preserving relatively good audio quality, and being home-budgeted in the same time.&#xD;&#xA;&#xD;&#xA;I've thought about connecting the casette player through mini-jack audio to laptop (microphone plug) and playing the casettes, recording audio in the same time on laptop. But is that the best way to preserve audio quality?&#xD;&#xA;&#xD;&#xA;Another topic is speed, this would take about 1 hour to digitize 1 casette, changing the side after ~30 minutes. I wonder if there are devices specialized for digitizing  casettes, possibly having own storage or connected through USB cable, being able to digitize with faster speed than normal playing? Are such devices affordable for home user?&#xD;&#xA;&#xD;&#xA;What&quot;s the best way to digitize my collection?" />
  <row Id="46" PostHistoryTypeId="1" PostId="22" RevisionGUID="13981b63-c5e7-4738-a2c1-754275042ce0" CreationDate="2013-02-27T01:07:08.150" UserId="74" Text="How to digitize audio casettes?" />
  <row Id="47" PostHistoryTypeId="3" PostId="22" RevisionGUID="13981b63-c5e7-4738-a2c1-754275042ce0" CreationDate="2013-02-27T01:07:08.150" UserId="74" Text="&lt;digitize&gt;&lt;audio&gt;" />
  <row Id="48" PostHistoryTypeId="2" PostId="23" RevisionGUID="288c1555-ae7d-42d7-9d3c-c3c1bc763b78" CreationDate="2013-02-27T01:11:49.133" UserId="74" Text="I have my master thesis in PDF. While the printed original is stored on the faculty where I have studied, the digital version in nowhere published. The free hostings will typically perish after some years, and the payed ones will be removed as soon as you don't pay the next fee.&#xD;&#xA;&#xD;&#xA;This thesis is nothing very special (in other case I would be doctor now :) but it was a lot effort anyway and someone else could find this useful.&#xD;&#xA;&#xD;&#xA;Where can I publish such resources, so that they will be preserved for future generations? Is there a site specialized in storing such resources, or a sharing community from that it would be copied to many places (interested users), preserving it from disapearing? " />
  <row Id="49" PostHistoryTypeId="1" PostId="23" RevisionGUID="288c1555-ae7d-42d7-9d3c-c3c1bc763b78" CreationDate="2013-02-27T01:11:49.133" UserId="74" Text="Where to publish master's thesis (in PDF)?" />
  <row Id="50" PostHistoryTypeId="3" PostId="23" RevisionGUID="288c1555-ae7d-42d7-9d3c-c3c1bc763b78" CreationDate="2013-02-27T01:11:49.133" UserId="74" Text="&lt;publishing&gt;" />
  <row Id="51" PostHistoryTypeId="2" PostId="24" RevisionGUID="7ac857c3-fa47-4d2d-83df-d79f3a15b399" CreationDate="2013-02-27T01:23:19.837" UserId="26" Text="There are actually devices that can digitize your cassettes at a faster rate (up to 8x), but they're generally quite expensive:&#xD;&#xA;http://www.bhphotovideo.com/bnh/controller/home?O=&amp;sku=562786&amp;Q=&amp;is=REG&amp;A=details&#xD;&#xA;&#xD;&#xA;The alternative would be to do exactly what you describe: hook up a cassette player to your computer, and use recording software (like Audacity) to save the audio.&#xD;&#xA;&#xD;&#xA;Just make sure that you have a good-quality cassette player, and a good-quality cable. On the player, make sure to turn up the volume as high as possible, but check that it doesn't clip the audio." />
  <row Id="52" PostHistoryTypeId="2" PostId="25" RevisionGUID="54031482-e6c2-4240-b676-77992b76d83a" CreationDate="2013-02-27T01:23:26.343" UserId="14" Text="You may want to digitize to a non-lossy format if you're going to do any processing (e.g., cropping or color correction to adjust for scanner issues) on the image. Editing a JPEG image can result in further degradation. A big archive would keep a permanent lossless image for the same reason, but that's most likely overkill for personal archiving. Once you're satisfied with any adjustments you've made, you can archive them to a high-quality JPEG file." />
  <row Id="53" PostHistoryTypeId="6" PostId="19" RevisionGUID="7b4bb34c-98d9-42d0-bb96-9f06051f7613" CreationDate="2013-02-27T01:32:28.503" UserId="68" Comment="edited tags" Text="&lt;out-of-date-format&gt;&lt;software&gt;&lt;vintage-computing&gt;&lt;mac&gt;" />
  <row Id="54" PostHistoryTypeId="2" PostId="26" RevisionGUID="384c0591-31dc-42ca-b7cf-bd42b385b14c" CreationDate="2013-02-27T01:37:34.403" UserId="76" Text="So [Fedora][1] is intended as a framework for building digital repositories. With that said, some organizations chose to store digital objects and files inside the Fedora system while others chose to reference them from inside Fedora. What are the relative merits of each of these options? In particular, what preservation risks do you mitigate and expose your self to with either option. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://fedora-commons.org/" />
  <row Id="55" PostHistoryTypeId="1" PostId="26" RevisionGUID="384c0591-31dc-42ca-b7cf-bd42b385b14c" CreationDate="2013-02-27T01:37:34.403" UserId="76" Text="When using a Fedora Repository what are the relative merits of storing files inside and outside the system?" />
  <row Id="56" PostHistoryTypeId="3" PostId="26" RevisionGUID="384c0591-31dc-42ca-b7cf-bd42b385b14c" CreationDate="2013-02-27T01:37:34.403" UserId="76" Text="&lt;software&gt;&lt;fedora&gt;&lt;repository-system&gt;" />
  <row Id="57" PostHistoryTypeId="5" PostId="7" RevisionGUID="6b48e3b2-e16c-4657-a437-c8625f0e3350" CreationDate="2013-02-27T01:40:51.790" UserId="26" Comment="added 241 characters in body" Text="If the floppies are 3.5&quot;, then you could purchase a floppy [reader][1] that connects to a USB port.&#xD;&#xA;&#xD;&#xA;And if the floppies are 5.25&quot;, you can still buy an old internal floppy drive on Ebay, and use [this][2] adapter to connect it via USB.&#xD;&#xA;&#xD;&#xA;As for software, you can easily find ancient software on abandonware and/or torrent sites (or still available for purchase). Even if the precise version of WordPerfect can't be found, I would think that a slightly later version would be able to open and convert the files.&#xD;&#xA;&#xD;&#xA;If the software is too old to run in Windows, you may need to run it in an emulator (such as DosBox). Fortunately, there are forums and FAQs for pretty much every type of emulated system out there.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.newegg.com/Product/Product.aspx?Item=N82E16821105004&#xD;&#xA;  [2]: http://www.deviceside.com/fc5025.html" />
  <row Id="58" PostHistoryTypeId="2" PostId="27" RevisionGUID="7afab156-ed2d-496f-9a35-56de08057ad0" CreationDate="2013-02-27T02:08:58.517" UserId="76" Text="Working with 3.5 floppies from the late 1980s I have come across some strange file formats that I am having trouble making sense of. These include, .ATP .DSP and .QT2 file extensions. If I open them in a text editor I can read a good bit of text. Based on reading this text, the file names, and the contextual information written on the disks I am relatively confident that these are transcripts created by some kind of dictation software. I'm happy that I can read some of the text in the files, but I would like to be able to really work with them. &#xD;&#xA;&#xD;&#xA;I goggled around a bit but couldn't find any good leads for what particular application made these files and if there are any current programs that can open them? If not, I would be interested in any suggestions folks have for how to go about working with these files beyond opening them in a text editor and reading the parts that come through as text. " />
  <row Id="59" PostHistoryTypeId="1" PostId="27" RevisionGUID="7afab156-ed2d-496f-9a35-56de08057ad0" CreationDate="2013-02-27T02:08:58.517" UserId="76" Text="Opening Obscure Audio Transcription Files" />
  <row Id="60" PostHistoryTypeId="3" PostId="27" RevisionGUID="7afab156-ed2d-496f-9a35-56de08057ad0" CreationDate="2013-02-27T02:08:58.517" UserId="76" Text="&lt;file-formats&gt;&lt;archival-material&gt;&lt;born-digital&gt;&lt;1980s&gt;" />
  <row Id="61" PostHistoryTypeId="6" PostId="22" RevisionGUID="7c40c7fe-762f-48ef-a4a9-aa0d2f6285e9" CreationDate="2013-02-27T02:34:15.000" UserId="26" Comment="edited tags" Text="&lt;digitize&gt;&lt;audio&gt;&lt;cassettes&gt;" />
  <row Id="62" PostHistoryTypeId="2" PostId="28" RevisionGUID="83447e5b-1ac3-40ce-b1f6-c4901345519d" CreationDate="2013-02-27T02:56:05.070" UserId="76" Text="With the long term in mind, (50-100 years) what situations is it good to normalize video for preservation to particular formats that have less preservation risks and when is it better to just stick with the originals?" />
  <row Id="63" PostHistoryTypeId="1" PostId="28" RevisionGUID="83447e5b-1ac3-40ce-b1f6-c4901345519d" CreationDate="2013-02-27T02:56:05.070" UserId="76" Text="When to normalize born digital video for preservation and when to keep heterogeneous original file formats" />
  <row Id="64" PostHistoryTypeId="3" PostId="28" RevisionGUID="83447e5b-1ac3-40ce-b1f6-c4901345519d" CreationDate="2013-02-27T02:56:05.070" UserId="76" Text="&lt;file-formats&gt;" />
  <row Id="65" PostHistoryTypeId="2" PostId="29" RevisionGUID="30dcf692-391b-4c50-8736-b7ca8648bdd3" CreationDate="2013-02-27T03:49:50.267" UserId="78" Text="I am going to agree with the recommendations above for getting the content off the media. If possible use a USB device. I originally tried to use a Ubuntu machine (9.04) but the kernel no longer Auto-supports 5.25 floppy drives. This is totally fixable but requires a bit of config adjusting.  &#xD;&#xA;&#xD;&#xA;Remember that the FC5025 connected drive will need to be powered off of your motherboard so be prepared to do a little tinkering. I recommend finding some non-collection disks to experiment with before you try to use media you might care about. &#xD;&#xA;&#xD;&#xA;If the first drive fails, don't be discouraged. we went through several drives trying to find one that worked.&#xD;&#xA;&#xD;&#xA;Emulators are good, but you might be able to get what you are looking for by opening the files in a text editor as well, this has proved useful to me.&#xD;&#xA;&#xD;&#xA;As for the future Digital Archiving, LOCKSS, a good migration policy, and PDF/A seems to be the best practice.&#xD;&#xA;&#xD;&#xA;If your a bit more concerned use MD5 checksums to ensure your data remain uncorrupted.&#xD;&#xA;&#xD;&#xA;As for wordperfect, looks to be a community [here][1] that might be helpful.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.wpuniverse.com/" />
  <row Id="66" PostHistoryTypeId="5" PostId="13" RevisionGUID="ef0747fa-3401-46b4-add8-8a0619bba633" CreationDate="2013-02-27T04:38:43.467" UserId="62" Comment="quoted from the question, linked to wikipedia" Text="&gt; what is the best way now to start modernizing and maintaining my and others' digital archives so they will always be readily referenceable?&#xD;&#xA;&#xD;&#xA;As far as I can tell [PDF/A](http://en.wikipedia.org/wiki/PDF/A) is a very well-considered format for archiving. It requires documents to contain no external dependencies like fonts or images." />
  <row Id="67" PostHistoryTypeId="5" PostId="12" RevisionGUID="b3c80cbc-2f54-45e1-99c3-9a03f4b33a96" CreationDate="2013-02-27T04:42:10.677" UserId="30" Comment="formatting / readability" Text="All media degrades over time, not even considering obsolescence of the equipment itself. As a result, you have to be prepared to revisit your technology choices on a regular basis.&#xD;&#xA;&#xD;&#xA;So the best solution will take into account:&#xD;&#xA;&#xD;&#xA;- speed&#xD;&#xA;- reliability&#xD;&#xA;- media lifetime&#xD;&#xA;- cost per unit of data, and&#xD;&#xA;- total amount of data &#xD;&#xA;&#xD;&#xA;Many organizations may find that a cloud solution such as CrashPlan offers an extremely attractive solution for redundant off-site backups. The cheap solutions (such as tape) can have hidden costs in terms of time, convenience and risk of data loss. It is important for organizations to be realistic and careful to insure proper redundancy." />
  <row Id="68" PostHistoryTypeId="24" PostId="12" RevisionGUID="b3c80cbc-2f54-45e1-99c3-9a03f4b33a96" CreationDate="2013-02-27T04:42:10.677" Comment="Proposed by 30 approved by 42 edit id of 2" />
  <row Id="70" PostHistoryTypeId="2" PostId="30" RevisionGUID="5f61bf42-a33e-4b70-98f2-6040fba3dde6" CreationDate="2013-02-27T04:51:33.910" UserId="42" Text="I have found that a DSLR with a low distortion macro lens will do a much better job of digitizing photos than any scanner that an ordinary person can afford. Scanners are fine for the big picture (and better than nothing) but for preserving fine details, the DSLR does a much better job. The secret is lots of light. The best way is to use a copy table/stand, making sure that the arm holding the camera is rated for its weight.&#xD;&#xA;&#xD;&#xA;As to the color, I generally try a few different adjustments to the color to see what happens and use whatever looks best - which is sometimes not making any change at all.&#xD;&#xA;&#xD;&#xA;For file format, JPG is fine, but use a high quality mode." />
  <row Id="71" PostHistoryTypeId="2" PostId="31" RevisionGUID="77f561c3-fa6c-4866-bda1-4117cad7e826" CreationDate="2013-02-27T05:19:18.640" UserId="62" Text="Perhaps the best way at present is to arrange to have it snagged by the [Wayback Machine](http://archive.org/web/web.php) from your homepage at your current institution. That way you'll have a more or less permanent URL, as long as the Internet Archive lasts.&#xD;&#xA;&#xD;&#xA;If it can be formatted as straight text, one or another usenet group would likely be thrilled to see it (and automatically archive it in numerous usenet archives throughout the world). I've been doing this with little programs and ideas, mainly so I can find my own work again after my computer inevitably dies. :)&#xD;&#xA;&#xD;&#xA;I was about to say Google Docs, but I see it's now turned into Google Drive, and that strikes me as a little unstable." />
  <row Id="72" PostHistoryTypeId="2" PostId="32" RevisionGUID="c8ee98e6-0aac-44a1-8397-add8cf4bfaab" CreationDate="2013-02-27T07:01:16.743" UserId="74" Text="What is the best format for storing digital photos, having digital preservation issues in mind?&#xD;&#xA;&#xD;&#xA;There are a plenty of formats nowadays, such as JPG and PNG, but how **standarized** they are, having in mind that after 50 or 100 years they may cease to exist and be replaced by much better formats? &#xD;&#xA;&#xD;&#xA;The ideal format for photo preservation should be good documented and relatively simple to implement, so that in worst case, when after those 50 or 100 years someone will find forgotten collection, and there will be no software opening such old formats, it would be relatively quick to implement the converter? " />
  <row Id="73" PostHistoryTypeId="1" PostId="32" RevisionGUID="c8ee98e6-0aac-44a1-8397-add8cf4bfaab" CreationDate="2013-02-27T07:01:16.743" UserId="74" Text="What is the best format for storing digital photos?" />
  <row Id="74" PostHistoryTypeId="3" PostId="32" RevisionGUID="c8ee98e6-0aac-44a1-8397-add8cf4bfaab" CreationDate="2013-02-27T07:01:16.743" UserId="74" Text="&lt;file-formats&gt;&lt;photos&gt;" />
  <row Id="75" PostHistoryTypeId="5" PostId="15" RevisionGUID="c82bdc7d-3eef-459f-bc25-524a48d22063" CreationDate="2013-02-27T08:15:40.803" UserId="57" Comment="fixed indentation" Text="I don't think this can be answered categorically if it's an electronic format. Like most hard questions, the answer correct here is **&quot;it depends&quot;**.&#xD;&#xA;&#xD;&#xA;The answer is going to depend on when you expect *&quot;the future&quot;* to be at.  &#xD;&#xA;&#xD;&#xA; - Is it 1 year?  Normal storage, like USB and external hard drives, are probably good enough.  Any of a number of on online storage services would also fit the bill here.&#xD;&#xA; - Is it 10 years?  You'd need something that doesn't degrade over time, so magnetic media could be out of the question.  Although there's stories of people firing up 8&quot; drives and getting data off from old floppies, I wouldn't count on that.  Something like storing your data on the [M-Disc from Milleniatta][1], along with at least two or three DVD drives should be enough.  USB seems to have staying power (and backward compatibility up to now), so you should be able to find a USB drive somewhere to connect it to.&#xD;&#xA; - Is it 50 years?  Now it gets interesting.  Think about this.  50 years ago, computers were only used by the government or big corporations.  So, what will 50 more years do to computing?  What we now know as &quot;computers&quot; might actually be invisible and wireless, so you'd have nothing to &quot;read&quot; your data with.  Electricity should still be available (assuming no nuclear winter, asteroid hitting earth, etc), so maybe saving all your data on something like the M-Disc mentioned above, plus a fully functional laptop computer, with all the applications/drivers needed to read the data AND full instructions on how to use it (fifty years into the future, it might not be yourself that's reading the data), including electricity/voltage specs.  At least you'll be able to read the data, even if you can't find a 50-year old computer by then (have you seen an old UNIVAC lying around these days?  Yeah, didn't think so).&#xD;&#xA; - Is it 100+ years?  At this point, honestly, I wouldn't trust any company's claim of &quot;eternal&quot;, &quot;permanent&quot;, etc.  Also, I wouldn't trust there being electricity at all.  From the archival/historical point of view, the only things that have withstood the test of time are well-cared-for papyrus/books, and carved rock (think the Rosetta Stone).  You could think about printing all your data in binary format in archival-quality paper, and storing all that in an airtight container. Think big QR codes with lots of redundancy/error correction built in, and/or hex values.  You'd have to make sure the printer/ink you use is permanent, doesn't fade, and doesn't get &quot;sticky&quot; over time.  Make sure you include a plain-english (maybe even multiple languages) document specifying how to decode the QR codes if you go this way, and a basic explanation of hex numbers.  A &quot;primer&quot; if you will.&#xD;&#xA; - Is it 1000+ years?  Only carved rock has withstood thousands of years in the past.  Even huge metal monuments have been melted down for ammunition and weapons, or stolen, while carved rock is usually left alone during times of hardship.  You'd probably have to go with granite or maybe even pure quartz, along with a computer-driven laser and sandblaster.  The language to use could be debatable, but you'd have to include a very basic primer (both numbers and letters, think [&quot;Contact&quot;][2]) to make sure people thousands of years from now are still able to decode it.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.mdisc.com&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Contact_%28film%29" />
  <row Id="76" PostHistoryTypeId="2" PostId="33" RevisionGUID="71a22612-a994-4549-9ab0-d62700f554ee" CreationDate="2013-02-27T08:39:57.287" UserId="89" Text="General rule:&#xD;&#xA;-------------&#xD;&#xA;&#xD;&#xA; 1. Put your publication under an [open license][1].&#xD;&#xA; 2. Make it available online. If it is worth it, I suppose net community will start to care for it then.&#xD;&#xA;&#xD;&#xA;More specific to your question:&#xD;&#xA;-------------------------------&#xD;&#xA;&#xD;&#xA;Address yourself to your faculty first. Maybe they run a subject oriented [repository in publishing][2] or they know a scientific society which does this for your field of study. If they don’t do, ask your university’s library for it and do not hesitate to continue to your regional library and—if they don’t provide for that—move on to your national library as well. Make sure your thesis is in PDF/A format and be prepared to seperately provide some so called “metadata” for cataloging, mainly&#xD;&#xA;&#xD;&#xA; - the full list of contributors,&#xD;&#xA; - the title,&#xD;&#xA; - any alternative titles,&#xD;&#xA; - the abstract,&#xD;&#xA; - some subject keywords,&#xD;&#xA; - the georaphic and temporal coverage,&#xD;&#xA; - the dates the thesis was created / started, last modified / finished, submitted to and accepted by the department&#xD;&#xA; - the language(s) used (think of foreign language cites, too),&#xD;&#xA; - the table of contents,&#xD;&#xA; - the bibliography or&#xD;&#xA; - the format (here: PDF) and extend (may be&#xD;&#xA; - the number of pages or &#xD;&#xA; - file size as well).&#xD;&#xA;&#xD;&#xA;Last: If you think you’ve created something of worth anyhow, it maybe would make sense to make a scientific article of it and aim for publishing it in a scientific magazine.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://creativecommons.org/licenses/&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Repository_%28publishing%29" />
  <row Id="77" PostHistoryTypeId="2" PostId="34" RevisionGUID="c0b1f3c1-23b8-4ca9-81bd-362e863c35c9" CreationDate="2013-02-27T09:15:34.027" UserId="91" Text="I have not seen these file extensions before. On the other hand, there are other ways to find out what a file contains. Unixoid systems such as Linux and Mac OS X come with a little tool called `file` which can probe the first bytes of any file and then make an educated guess about the format it contains. To do that, it uses a database of characteristic patterns which is usually located in `/usr/share/misc/magic`. The magic database on my computer contains 11,543 non-comment lines, so I guess it can detect more than 10,000 file formats. There is a good chance it will be able to identify your file formats.&#xD;&#xA;&#xD;&#xA;The Java world has its own file format identification tools: [Tika][1] calls itself a “content analysis toolkit”, and a slightly dated alternative is called [DROID][2] for “Digital Record and Object Identification”.&#xD;&#xA;&#xD;&#xA;All of these tools are free and open-source software.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://tika.apache.org/&#xD;&#xA;  [2]: http://digital-preservation.github.com/droid/" />
  <row Id="78" PostHistoryTypeId="2" PostId="35" RevisionGUID="ac7adf97-7aaa-446b-8719-ded56c84652b" CreationDate="2013-02-27T09:36:45.280" UserId="91" Text="The first thing I would do with old WP5.1 files is to open them in WP5.1. I have recently found a copy of WP5.1 for MS-DOS on an abandonware website, and it worked well within DosBox on Linux.&#xD;&#xA;&#xD;&#xA;Probably the best way to preserve the original layout and formatting of these documents is to print them from WP5.1 into a PostScript file that can then be converted into PDF/A using separate, modern software. In order to archive the contents, you could also save copies in Rich Text Format (RTF) and plain text (which you should then convert to UTF-8 Unicode unless it is pure US-ASCII)." />
  <row Id="79" PostHistoryTypeId="2" PostId="36" RevisionGUID="bbe592ed-a030-41c5-8133-0760e9c11b39" CreationDate="2013-02-27T10:02:33.110" UserId="91" Text="I am planning to set up a [LOCKSS][1] (“Lots Of Copies Keep Stuff Safe”) node for a small PLN (“Private Locks Network”) intended to safely store research and publication data from a few universities.&#xD;&#xA;&#xD;&#xA;A partner in this PLN recommended against using a RAID (“Redundant Array of Inexpensive Disks”) for storage because RAID controllers can be buggy, leading to a complete loss of all data stored on this node. On the other hand, most data centers rely on RAID storage, so it cannot be all that bad. LOCKSS was designed for commodity hardware, but at a university, it can be more difficult to get a consumer-grade computer with a cheap external USB hard drive than to get a server with redundant storage or a virtual machine running on enterprise-class hardware.&#xD;&#xA;&#xD;&#xA;In my mind, the two technologies should complement each other fairly well: LOCKSS guards against bitrot and other inconsistencies in the files, and RAID avoids data loss when a hard drive fails. Are there serious reasons for not running a LOCKSS node on a server with RAID storage? How about using a virtual machine?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.lockss.org/" />
  <row Id="80" PostHistoryTypeId="1" PostId="36" RevisionGUID="bbe592ed-a030-41c5-8133-0760e9c11b39" CreationDate="2013-02-27T10:02:33.110" UserId="91" Text="What storage hardware should I use for a LOCKSS node?" />
  <row Id="81" PostHistoryTypeId="3" PostId="36" RevisionGUID="bbe592ed-a030-41c5-8133-0760e9c11b39" CreationDate="2013-02-27T10:02:33.110" UserId="91" Text="&lt;storage&gt;&lt;raid&gt;&lt;lockss&gt;&lt;bitrot&gt;" />
  <row Id="82" PostHistoryTypeId="2" PostId="37" RevisionGUID="04b7a164-4e66-41ac-bece-7383fc64a513" CreationDate="2013-02-27T10:09:24.460" UserId="-1" Text="" />
  <row Id="83" PostHistoryTypeId="2" PostId="38" RevisionGUID="92ce2f42-e046-441a-91d6-5ffbd6207914" CreationDate="2013-02-27T10:09:24.460" UserId="-1" Text="" />
  <row Id="84" PostHistoryTypeId="2" PostId="39" RevisionGUID="38108451-eb76-449d-81b9-be2072965a99" CreationDate="2013-02-27T10:29:51.097" UserId="89" Text="Is there an application, or a module/plug-in in a long time preservation framework, which makes a storage system systematically loose information you *don’t* need any longer while preserving the data you *will* need again in the future?&#xD;&#xA;&#xD;&#xA;I think of a system that, e.g., distinguishes accesses to the data which indicate that it was used (e.g. reading in a file for a longer time that suggests that a user really reads it, or re-opening the same file without using search in prior, or with a certain frequency per month, year, ...) from accesses which are not likely to suggest the data to be of any worth any more (e.g. virus scanners or search systems inspecting those files or users flipping through files found from search results for a short time only, then going on to the next hit suggesting they didn’t find the information they were looking for in them) and finally forgetting / overwriting uninteresting data in case that storage becomes short while keeping the worthful data.&#xD;&#xA;&#xD;&#xA;I observe this for a couple of times whenever I migrated to new desktop computer hardware, I kept a backup but usually I only need very few things out of it later on, either because old software cannot be used in a sensible way any longer (what to do with a DOS screen saver on Windows?) or, for personal data, because of interests simply shifting. (For example, I am no longer interested in things I cared for as a teenager.) The same can be observed in libraries: It may happen that, if you randomly pick a book from a shelf, its pages stick together, indicating noone has read in it for a couple of centuries.&#xD;&#xA;&#xD;&#xA;Are there (problably software) approaches that address the problem of sensible forgetting in digital age?" />
  <row Id="85" PostHistoryTypeId="1" PostId="39" RevisionGUID="38108451-eb76-449d-81b9-be2072965a99" CreationDate="2013-02-27T10:29:51.097" UserId="89" Text="How to handle sensibly lossful digital long term preservation?" />
  <row Id="86" PostHistoryTypeId="3" PostId="39" RevisionGUID="38108451-eb76-449d-81b9-be2072965a99" CreationDate="2013-02-27T10:29:51.097" UserId="89" Text="&lt;storage&gt;" />
  <row Id="87" PostHistoryTypeId="2" PostId="40" RevisionGUID="1af90aa5-1ad6-4343-a91a-7c94482af016" CreationDate="2013-02-27T10:29:51.097" UserId="89" Text="[Freenet][1] is an open source software that uses a decentralized distributed data store to store information. Servers contribute memory from their own hardware to “the net”, but the data store is limited to the sum of these contributions. Whenever a user requests a file it is copied peer-to-peer, leaving copies on the systems it passes. So files often requested are highly available, files never requested fall off aback after some time.&#xD;&#xA;&#xD;&#xA;This is not exactly what I am looking for but it is the best approach I currently know about.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Freenet" />
  <row Id="88" PostHistoryTypeId="2" PostId="41" RevisionGUID="ff9784ad-2e2a-4923-9cfd-a79ad96e3e7a" CreationDate="2013-02-27T11:21:38.307" UserId="94" Text="A couple of additional points to Nick's excellent answer...&#xD;&#xA;&#xD;&#xA;Depending on the target of the data recovery, specialist equipment may well be required which may be [excessively expensive and/or difficult to obtain due it to being obsolete or simply in short supply][1].&#xD;&#xA;&#xD;&#xA;Utilising an external data recovery company means taking your data off site. This introduces some degree of risk. This approach may be impossible if you are dealing with sensitive data, or simply have an organisational policy which precludes taking data off site.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://libraries.stackexchange.com/questions/1261/is-there-a-hardware-controller-option-for-acquiring-data-images-off-floppy-disks" />
  <row Id="89" PostHistoryTypeId="2" PostId="42" RevisionGUID="33c0aaaa-adf4-4d8a-8328-d7ae50c4074e" CreationDate="2013-02-27T11:28:09.653" UserId="83" Text="I had some files encrypted using encryption facility provided by windows.I had those files in my hard disk.Now,I have formatted the system with new windows.Now the files have become inaccessible.Is their a way the files can be accessed?" />
  <row Id="90" PostHistoryTypeId="1" PostId="42" RevisionGUID="33c0aaaa-adf4-4d8a-8328-d7ae50c4074e" CreationDate="2013-02-27T11:28:09.653" UserId="83" Text="How to access encrypted files ,if transferred to other PC?" />
  <row Id="91" PostHistoryTypeId="3" PostId="42" RevisionGUID="33c0aaaa-adf4-4d8a-8328-d7ae50c4074e" CreationDate="2013-02-27T11:28:09.653" UserId="83" Text="&lt;windows&gt;&lt;encryption&gt;" />
  <row Id="92" PostHistoryTypeId="2" PostId="43" RevisionGUID="9a4915c9-b620-4110-bf06-d5c8aa2ac11a" CreationDate="2013-02-27T11:32:55.190" UserId="95" Text="De Montfort University is a part of the UK LOCKSS Alliance and public LOCKSS network. We considered and rejected using RAID and currently have 4 separate disks in a separate server.&#xD;&#xA;&#xD;&#xA; - If the RAID disks were bought at the same time they could all be a part of the same batch with similar vulnerabilities;&#xD;&#xA; - The duplication within RAID can be considered wasteful given the duplication of content with a PLN.&#xD;&#xA;&#xD;&#xA;LOCKSS can be put on a virtual server, but is a busy service and may not play nicely with any other co-hosted servers.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="94" PostHistoryTypeId="2" PostId="44" RevisionGUID="e843c1ff-c928-4f57-92a9-8eca05fac6e4" CreationDate="2013-02-27T11:42:02.380" UserId="29" Text="I work for MetaArchive, a LOCKSS PLN. We publish the technical specifications for our LOCKSS nodes [here](http://www.metaarchive.org/public/resources/charter_member/2013_Technical_Specifications.pdf).&#xD;&#xA;&#xD;&#xA;We recommend the use of at least RAID 5 on all of our servers. RAID 5 dedicates a portion of each drive for parity data on all the drives. In our recommended server configuration, we ask for 8 2TB drives, but only 14 TB of this is usable for storage and 2 TB is used for parity. If a hard drive crashes, and it will happen, the parity data ensures that you can not only reconstruct the lost data, but also ensure that all the other drives remain safe.&#xD;&#xA;&#xD;&#xA;LOCKSS networks have redundant copies spread around a network with voting-and-polling to check fixity. This provides parity on a network level, but it's much faster to recover from a hard drive crash locally than over a network.&#xD;&#xA;&#xD;&#xA;For more general advice, Backblaze, a cloud-based backup service, recently published their 180 TB server configurations with [lots of advice](http://blog.backblaze.com/2013/02/20/180tb-of-good-vibrations-storage-pod-3-0/). Also, put all your drives through a burn-in test, since it appears that drives have [higher infant mortality rate (page 4)](https://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/disk_failures.pdf) It's easier to replace an empty drive under warranty than to wait for the warranty to ship so you can rebuild an array." />
  <row Id="95" PostHistoryTypeId="2" PostId="45" RevisionGUID="fb4b3dcc-40de-4f3e-8b90-e29f5add137b" CreationDate="2013-02-27T11:51:02.103" UserId="94" Text="De-selection or disposal of unwanted data is a difficult challenge. Your question assumes that data becomes less valuable the longer it remains stored without access by a user. However, this is not always a good measure of its value. Interest in data by users can remain low or none existent for a considerable period of time, before current events or some other triggers create new interest in the data, leading to a spike in access. This is indeed quite common in library and archival collections.&#xD;&#xA;&#xD;&#xA;Manual de-selection can be very time consuming and costly, but automated de-selection may not do the job of &quot;sensible forgetting&quot; well, potentially putting important data at risk. Unfortunately there are no easy answers." />
  <row Id="96" PostHistoryTypeId="2" PostId="46" RevisionGUID="43ca1521-af74-432b-8364-1321cc02784b" CreationDate="2013-02-27T12:06:02.647" UserId="94" Text="Media lifetime is a red herring. All media types suffer from two key problems:&#xD;&#xA;&#xD;&#xA; 1. The media rots and gradually becomes unreadable&#xD;&#xA; 2. The media technology (the devices needed to read the media) becomes obsolete&#xD;&#xA;&#xD;&#xA;Write your data to some media and leave it on a shelf, and it will eventually become damaged and/or impossible to read. When this happens, you won't know. This is not a viable approach to long term digital preservation.&#xD;&#xA;&#xD;&#xA;To ensure the longevity of your data you need to repeatedly check that it remains undamaged by verifying its integrity (this is easy to do by creating checksums for your data and then periodically recalculating your checksums in order to identify any data loss). If data loss is detected, repair is necessary with the aid of a backup copy (or two).&#xD;&#xA;&#xD;&#xA;This means that the critical media characteristic is accessibility, not longevity.&#xD;&#xA;&#xD;&#xA;For example, a manufacturer might claim that their specialist gold CD-ROMS are guaranteed to last 100 years. However, in order to confirm that their &quot;guarantee&quot; is in fact correct, you need to periodically verify the integrity of the data. Sitting in front of a disc drive, repeatedly inserting CDs so that you can do an integrity check every month is not a lot of fun if you have a lot of data.&#xD;&#xA;&#xD;&#xA;High capacity and easily accessible storage media (albeit storage media with a limited lifetime) such as hard disk and magnetic tape remains the long term storage media of choice.&#xD;&#xA;&#xD;&#xA;Keep the data accessible. Keep at least 3 copies. Keep checking the integrity of the data. Fix the data from a backup when necessary. Migrate to new media when it reaches end of life." />
  <row Id="97" PostHistoryTypeId="2" PostId="47" RevisionGUID="387be3d8-b10b-4b35-9321-7175aff4c3ea" CreationDate="2013-02-27T13:03:46.150" UserId="97" Text="If you use the built-in NTFS encryption mode, then once you lose access to the SAM database in Windows (which holds the user account details), you no longer have access to the encryption/decryption key necessary to decrypt the files. This is because the keys are stored in the SAM database.&#xD;&#xA;&#xD;&#xA;In reformatting, you lost access to the SAM database by normal means.&#xD;&#xA;&#xD;&#xA;**So, the files are for all intents and purposes impossible to access *on that media.***&#xD;&#xA;&#xD;&#xA;If you have unencrypted backups, that's your best bet. Otherwise, I'm afraid you are pretty much out of luck.&#xD;&#xA;&#xD;&#xA;If you are *really* lucky, it *might* be possible for a data recovery firm to access the data as it was prior to the reformatting, but I really wouldn't expect that. Their focus tends to be physically damaged media, not logically overwritten media." />
  <row Id="98" PostHistoryTypeId="2" PostId="48" RevisionGUID="b106e442-5404-4fdb-8a31-a9d67600e38a" CreationDate="2013-02-27T13:15:28.030" UserId="94" Text="[Quick View Plus][1] is a handy tool that lets you view a large number of different file formats and is pretty affordable. Here's [an archivist's take on using it][2]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/QuickView&#xD;&#xA;  [2]: http://digital-archiving.blogspot.co.uk/2013/02/in-praise-of-quick-view-plus.html" />
  <row Id="99" PostHistoryTypeId="2" PostId="49" RevisionGUID="e62cc377-058c-4ebf-bdb4-26f5e20f24a5" CreationDate="2013-02-27T13:34:15.987" UserId="97" Text="I would argue that if you want *simple to read*, then uncompressed BMP is probably a good choice. It is an extremely simple format (basically a header specifying color depth, width, height and some identifying information, followed by the raw pixel data), and it supports up to eight bits per pixel per channel in three channels (red, green and blue) which is the same as PNG and JPEG. Non-24-bit BMPs also have a color palette stored in the image file, but I do believe that is omitted in a 24-bit BMP.&#xD;&#xA;&#xD;&#xA;The lack of compression means the files are very large (a 15 Mpx photo in 24-bit color will be nearly 50 MB in size), but also makes them *easy to interpret* if only you have cursory knowledge of the format. About all you need to know is that there is [a relatively small header](http://en.wikipedia.org/wiki/BMP_file_format#Bitmap_file_header) followed by RGB byte triplets each specifying one pixel (or an appropriate number of bytes per pixel specifying indexes into the palette), and you can probably write a decoder by simple trial and error in a few hours. If that knowledge has been lost and cannot be obtained, then you likely have plenty more trouble even accessing the data as files.&#xD;&#xA;&#xD;&#xA;Another upside is that minor data loss (other than in the header) leads to absolutely minimal degradation: only the exact pixels actually affected by the data loss are corrupted. And with some mathematical trickery and human trial-and-error, the header can probably be reconstructed from an otherwise complete (but possibly corrupted) image file.&#xD;&#xA;&#xD;&#xA;The major downside of BMP is that the files are huge. For a full-color image of any substantial pixel size, file size can be approximated as `3 * H * W`, where `H` and `W` are the height and width in pixels, respectively. As we know, image size expressed in megapixels is simply `H * W`, so for a given image size of `Mp megapixels` this simply reduces to `3 * Mp` bytes of image data." />
  <row Id="100" PostHistoryTypeId="5" PostId="42" RevisionGUID="a5a2bf7f-e955-4a41-b3cc-e729f8d5fe8d" CreationDate="2013-02-27T14:02:27.480" UserId="97" Comment="Improved body and title" Text="I encrypted some files using the encryption facility provided by Windows. The files were on my hard disk. Now, I have formatted the system and reinstalled Windows. Now the files have become inaccessible. Is there a way the files can be accessed?" />
  <row Id="101" PostHistoryTypeId="4" PostId="42" RevisionGUID="a5a2bf7f-e955-4a41-b3cc-e729f8d5fe8d" CreationDate="2013-02-27T14:02:27.480" UserId="97" Comment="Improved body and title" Text="How to access Windows encrypted files after reformat?" />
  <row Id="102" PostHistoryTypeId="24" PostId="42" RevisionGUID="a5a2bf7f-e955-4a41-b3cc-e729f8d5fe8d" CreationDate="2013-02-27T14:02:27.480" Comment="Proposed by 97 approved by 83 edit id of 4" />
  <row Id="103" PostHistoryTypeId="2" PostId="50" RevisionGUID="d0053cf5-990a-44c1-8eb1-8179f8a801a7" CreationDate="2013-02-27T14:22:17.353" UserId="100" Text="It would be helpful to know about some of your &quot;specs&quot; - what you're scanning at, which OCR software you're using, and whether you're running high or low resolution images through that software. &#xD;&#xA;&#xD;&#xA;One thing we try to keep in mind with our digitized genealogical content is that having the content accessible online, even with an OCR error rate, is still better than it sitting on a shelf in a single location. We've also had fantastic success, both in uptake and in accuracy of transcriptions, by asking volunteers to help transcribe our content through Flickr (http://www.flickr.com/photos/statelibrarync/sets/72157627124710723/). Genealogists are hugely motivated! While a crowdsourcing project may be down the road for you, keep that in mind as an option.&#xD;&#xA;&#xD;&#xA;This sounds like a fantastic opportunity, and I hope you can move forward with digitization and also preservation of this content.&#xD;&#xA;&#xD;&#xA;Lisa Gregory&#xD;&#xA;State Library of North Carolina." />
  <row Id="104" PostHistoryTypeId="2" PostId="51" RevisionGUID="0df1d057-ad7f-459d-8386-eb1bd76b7e5d" CreationDate="2013-02-27T14:28:57.883" UserId="98" Text="When you want to preserve something for a long-term period, you should always consider the following topics:&#xD;&#xA;&#xD;&#xA;1 - you don't know what you'll do with it in the future: if you have a precise idea in mind of all the future uses, you can choose how to store the images according to that uses. But very frequently you (or somebody else) will want to make different uses of it, and then will blame who &quot;preserved&quot; it for not taking enough care.&#xD;&#xA;&#xD;&#xA;2 - so, if it's within your possibilities, put as much effort as you can in doing it. If you have enough media storage, use as much resolution as possible. 300 dpi might be good today, but also might be to few in the future. One day someone though that VHS was more than enough to store video, and today bluray is good, but it's not *that* much, is it?&#xD;&#xA;&#xD;&#xA;3 - if some file format is widespread enough today, probably it'll be easier to find something about it in the future. Everybody uses JPG, so go the same way. With as much quality as possible (perhaps a 0.98 quality).&#xD;&#xA;&#xD;&#xA;4 - If you were dealing with high-quality photos, you would have to worry about color calibration. Every equipment has it's way to represent color, and so your photos have one color, that will be transformed slightly by the scanner, and your computer monitor will also show some different colors when you view them. So, yes, if you're not using that high-end calibrated scanner, you could do some color adjustment, or save some color reference like the color profile...&#xD;&#xA;&#xD;&#xA;5 - ... but I think you're talking about family photos, not spending that much to scan them, etc. So you could scan some color reference to help someone in the future, why not? Buy some scanner calibration sheet (for example, [this one](http://www.rpimaging.com/catalog/product/view/id/490), scan it, and store a copy of it with each folder you're organizing your photos. That way, in the future, someone who will be using your pictures will have an idea on how *your scanner* represented the magenta, cyan, yellow..." />
  <row Id="105" PostHistoryTypeId="2" PostId="52" RevisionGUID="cda1315e-6b7a-447c-bbd6-429b86509db5" CreationDate="2013-02-27T14:29:51.427" UserId="97" Text="When saving text in a Unicode encoding, how should one choose whether or not to include a byte-order mark to indicate the text being Unicode (and what encoding thereof)?&#xD;&#xA;&#xD;&#xA;Are there are inherent advantages from a digital preservation point of view to simply adding a UTF-8 BOM to a pure 7-bit-US-ASCII file, over not doing so? What are the potential drawbacks? Keeping in mind that the binary format of 7-bit US ASCII and the corresponding UTF-8 data is exactly the same." />
  <row Id="106" PostHistoryTypeId="1" PostId="52" RevisionGUID="cda1315e-6b7a-447c-bbd6-429b86509db5" CreationDate="2013-02-27T14:29:51.427" UserId="97" Text="Should Unicode/UTF text files include a BOM?" />
  <row Id="107" PostHistoryTypeId="3" PostId="52" RevisionGUID="cda1315e-6b7a-447c-bbd6-429b86509db5" CreationDate="2013-02-27T14:29:51.427" UserId="97" Text="&lt;text&gt;&lt;unicode&gt;" />
  <row Id="108" PostHistoryTypeId="5" PostId="49" RevisionGUID="6f019ebd-d40d-4130-a07a-4a25d99d3a3c" CreationDate="2013-02-27T14:35:31.077" UserId="97" Comment="added 23 characters in body" Text="I would argue that if you want *simple to read*, then uncompressed BMP is probably a good choice. It is an extremely simple format (basically a header specifying color depth, width, height and some identifying information, followed by the raw pixel data), and it supports up to eight bits per pixel per channel in three channels (red, green and blue) which is the same as PNG and JPEG. Non-24-bit BMPs also have a color palette stored in the image file, but I do believe that is omitted in a 24-bit BMP.&#xD;&#xA;&#xD;&#xA;The lack of compression means the files are very large (a 15 Mpx photo in 24-bit color will be nearly 50 MB in size), but also makes them *easy to interpret* if only you have cursory knowledge of the format. About all you need to know is that there is [a relatively small header](http://en.wikipedia.org/wiki/BMP_file_format#Bitmap_file_header) followed by RGB byte triplets each specifying one pixel (or an appropriate number of bytes per pixel specifying indexes into the palette), and you can probably write a decoder by simple trial and error in a few hours. If that knowledge has been lost and cannot be obtained, then you likely have plenty more trouble even accessing the data as files.&#xD;&#xA;&#xD;&#xA;Another upside is that minor data loss (other than in the header) leads to absolutely minimal degradation: only the exact pixels actually affected by the data loss are corrupted. And with some mathematical trickery and human trial-and-error, the header can probably be reconstructed from an otherwise complete (but possibly corrupted) image file.&#xD;&#xA;&#xD;&#xA;The major downside of BMP is that the files are huge. For a full-color image of any substantial pixel size, the resulting file size in bytes can be approximated as `3 * H * W`, where `H` and `W` are the height and width in pixels, respectively. As we know, image size expressed in megapixels is simply `H * W`, so for a given image size of `Mp megapixels` this simply reduces to `3 * Mp` bytes of image data." />
  <row Id="109" PostHistoryTypeId="6" PostId="52" RevisionGUID="b145f6bf-f684-45a3-b2e2-f0a4e45f2083" CreationDate="2013-02-27T14:37:41.557" UserId="97" Comment="Add 'file-formats' tag" Text="&lt;file-formats&gt;&lt;text&gt;&lt;unicode&gt;" />
  <row Id="110" PostHistoryTypeId="5" PostId="49" RevisionGUID="a520283e-0d4e-4e90-9310-d59414ee22be" CreationDate="2013-02-27T14:43:45.917" UserId="97" Comment="added 4 characters in body" Text="I would argue that if you want *simple to read*, then uncompressed BMP is probably a good choice. It is an extremely simple format (basically a header specifying color depth, width, height and some identifying information, followed by the raw pixel data), and it supports up to eight bits per pixel per channel in three channels (red, green and blue) which is the same as PNG and JPEG. Non-24-bit BMPs also have a color palette stored in the image file, but I do believe that is omitted in a 24-bit BMP.&#xD;&#xA;&#xD;&#xA;The lack of compression means the files are very large (a 15 Mpx photo in 24-bit color will be nearly 50 MB in size), but also makes them *easy to interpret* if only you have cursory knowledge of the format. About all you need to know is that there is [a relatively small header](http://en.wikipedia.org/wiki/BMP_file_format#Bitmap_file_header) followed by RGB byte triplets each specifying one pixel (or an appropriate number of bytes per pixel specifying indexes into the palette), and you can probably write a decoder by simple trial and error in a few hours. If that knowledge has been lost and cannot be obtained, then you likely have plenty more trouble even accessing the data as files.&#xD;&#xA;&#xD;&#xA;Another upside is that minor data loss (other than in the header) leads to absolutely minimal degradation: only the exact pixels actually affected by the data loss are corrupted. And with some mathematical trickery and human trial-and-error, the header can probably be reconstructed from an otherwise complete (but possibly corrupted) image file.&#xD;&#xA;&#xD;&#xA;The major downside of BMP is that the files are huge. For a full-color image of any substantial pixel size, the resulting file size in bytes can be approximated as `3 * H * W`, where `H` and `W` are the height and width in pixels, respectively. As we know, image size expressed in megapixels is simply `H * W`, so for a given image size of `Mp megapixels` this simply reduces to `3 * Mp` megabytes of image data." />
  <row Id="111" PostHistoryTypeId="4" PostId="52" RevisionGUID="9608c718-f3b8-4f60-8b87-8e9249fd1138" CreationDate="2013-02-27T14:44:20.877" UserId="97" Comment="edited title" Text="Should general-purpose Unicode/UTF text files include a BOM?" />
  <row Id="112" PostHistoryTypeId="2" PostId="53" RevisionGUID="e261ceb4-875f-400a-adc2-88cf84b634b1" CreationDate="2013-02-27T14:56:02.090" UserId="26" Text="I wouldn't see any necessity for adding a BOM to existing 7-bit ASCII files.  Technically, as you mention, 7-bit ASCII files can *already* be treated as UTF-8 without any modification (this was one of the key motivations behind Unicode).  So, if an application claims to be compatible with Unicode, it should be able to read these files by definition.&#xD;&#xA;&#xD;&#xA;Of course, if any of these files contains &quot;extended&quot; ASCII characters (&gt;0x7F), it should be converted to UTF-8, with BOM." />
  <row Id="113" PostHistoryTypeId="2" PostId="54" RevisionGUID="d0eb36dc-abe9-4baf-b689-df0b45e11841" CreationDate="2013-02-27T14:58:35.350" UserId="-1" Text="" />
  <row Id="114" PostHistoryTypeId="2" PostId="55" RevisionGUID="f01fb5a0-36f8-4188-8cf7-72457e324fd4" CreationDate="2013-02-27T14:58:35.350" UserId="-1" Text="" />
  <row Id="115" PostHistoryTypeId="2" PostId="56" RevisionGUID="c56673ae-b5e8-430e-8d25-e4bfef08ef7a" CreationDate="2013-02-27T15:04:36.597" UserId="36" Text="What measures, technologies, or techniques are applicable to guarding information against damage/loss during organisation?&#xD;&#xA;&#xD;&#xA;Often when I'm archiving data, before the archiving process I also need to de-duplicate and organise the data. I'm aware that this is a risky time for the data and that human, software or mechanical error might lead to some data being corrupt or lost, without me even knowing about it. &#xD;&#xA;&#xD;&#xA;So there are two questions:&#xD;&#xA;&#xD;&#xA;1) How can I keep an eye on the data so that I can audit where it was versus where it is now, and see if any data has been deleted or changed during organisation?&#xD;&#xA;&#xD;&#xA;2) How can I make it harder to make human error while organising the data? &#xD;&#xA;&#xD;&#xA;" />
  <row Id="116" PostHistoryTypeId="1" PostId="56" RevisionGUID="c56673ae-b5e8-430e-8d25-e4bfef08ef7a" CreationDate="2013-02-27T15:04:36.597" UserId="36" Text="Guarding data against corruption or loss during organisation" />
  <row Id="117" PostHistoryTypeId="3" PostId="56" RevisionGUID="c56673ae-b5e8-430e-8d25-e4bfef08ef7a" CreationDate="2013-02-27T15:04:36.597" UserId="36" Text="&lt;organisation&gt;&lt;deduplication&gt;" />
  <row Id="118" PostHistoryTypeId="5" PostId="3" RevisionGUID="d5501ef8-36ce-460a-bbe3-023ed452e741" CreationDate="2013-02-27T15:13:29.717" UserId="23" Comment="code blocks" Text="OK, let's get this thing going ... I've downloaded JHOVE2 and attempted to build it, using Maven through Eclipse with the target &quot;compile&quot;. I get the following error:&#xD;&#xA;&#xD;&#xA;&gt; [ERROR] Failed to execute goal on project jhove2: Could not resolve&#xD;&#xA;&gt; dependencies for project org.jhove2:jhove2:jar:2.0.0: Failed to&#xD;&#xA;&gt; collect dependencies for [commons-logging:commons-logging-api:jar:1.1&#xD;&#xA;&gt; (compile), commons-logging:commons-logging:jar:1.1.1 (compile),&#xD;&#xA;&gt; org.springframework:spring-context:jar:2.5.6 (compile),&#xD;&#xA;&gt; org.springframework:spring-test:jar:2.5.6 (test),&#xD;&#xA;&gt; log4j:log4j:jar:1.2.14 (compile), junit:junit:jar:4.4 (test),&#xD;&#xA;&gt; jdom:jdom:jar:1.0 (compile), soap:soap:jar:2.3.1 (compile),&#xD;&#xA;&gt; xerces:xercesImpl:jar:2.9.1 (compile),&#xD;&#xA;&gt; xml-resolver:xml-resolver:jar:1.2 (compile), net.sf:jargs:jar:1.0&#xD;&#xA;&gt; (compile), org.mvel:mvel2:jar:2.0.18 (compile),&#xD;&#xA;&gt; org.geotools:gt-main:jar:2.6.5 (compile),&#xD;&#xA;&gt; org.geotools:gt-shapefile:jar:2.6.5 (compile),&#xD;&#xA;&gt; com.sleepycat:je:jar:4.0.103 (compile)]: Failed to read artifact&#xD;&#xA;&gt; descriptor for net.sf:jargs:jar:1.0: Could not transfer artifact&#xD;&#xA;&gt; net.sf:jargs:pom:1.0 from/to JBOSS&#xD;&#xA;&gt; (http://repository.jboss.org/maven2/): Access denied to&#xD;&#xA;&gt; http://repository.jboss.org/maven2/net/sf/jargs/1.0/jargs-1.0.pom.&#xD;&#xA;&gt; Error code 403, Forbidden -&gt; [Help 1]&#xD;&#xA;&#xD;&#xA;This may be because JHOVE2 hasn't been updated in a few years, so the dependencies in its pom.xml perhaps are no longer valid. On the other hand, every time I touch Maven something bad happens, so I may just be doing something dumb. Are there any changes to pom.xml or other tricks which can get the build working?" />
  <row Id="119" PostHistoryTypeId="24" PostId="3" RevisionGUID="d5501ef8-36ce-460a-bbe3-023ed452e741" CreationDate="2013-02-27T15:13:29.717" Comment="Proposed by 23 approved by -1 edit id of 1" />
  <row Id="120" PostHistoryTypeId="5" PostId="3" RevisionGUID="d418fb61-c323-4d66-a356-c6746380c21f" CreationDate="2013-02-27T15:13:29.717" UserId="101" Comment="code blocks" Text="I've downloaded JHOVE2 and attempted to build it, using Maven through Eclipse with the target &quot;compile&quot;. I get the following error:&#xD;&#xA;&#xD;&#xA;&gt; [ERROR] Failed to execute goal on project jhove2: Could not resolve&#xD;&#xA;&gt; dependencies for project org.jhove2:jhove2:jar:2.0.0: Failed to&#xD;&#xA;&gt; collect dependencies for [commons-logging:commons-logging-api:jar:1.1&#xD;&#xA;&gt; (compile), commons-logging:commons-logging:jar:1.1.1 (compile),&#xD;&#xA;&gt; org.springframework:spring-context:jar:2.5.6 (compile),&#xD;&#xA;&gt; org.springframework:spring-test:jar:2.5.6 (test),&#xD;&#xA;&gt; log4j:log4j:jar:1.2.14 (compile), junit:junit:jar:4.4 (test),&#xD;&#xA;&gt; jdom:jdom:jar:1.0 (compile), soap:soap:jar:2.3.1 (compile),&#xD;&#xA;&gt; xerces:xercesImpl:jar:2.9.1 (compile),&#xD;&#xA;&gt; xml-resolver:xml-resolver:jar:1.2 (compile), net.sf:jargs:jar:1.0&#xD;&#xA;&gt; (compile), org.mvel:mvel2:jar:2.0.18 (compile),&#xD;&#xA;&gt; org.geotools:gt-main:jar:2.6.5 (compile),&#xD;&#xA;&gt; org.geotools:gt-shapefile:jar:2.6.5 (compile),&#xD;&#xA;&gt; com.sleepycat:je:jar:4.0.103 (compile)]: Failed to read artifact&#xD;&#xA;&gt; descriptor for net.sf:jargs:jar:1.0: Could not transfer artifact&#xD;&#xA;&gt; net.sf:jargs:pom:1.0 from/to JBOSS&#xD;&#xA;&gt; (http://repository.jboss.org/maven2/): Access denied to&#xD;&#xA;&gt; http://repository.jboss.org/maven2/net/sf/jargs/1.0/jargs-1.0.pom.&#xD;&#xA;&gt; Error code 403, Forbidden -&gt; [Help 1]&#xD;&#xA;&#xD;&#xA;This may be because JHOVE2 hasn't been updated in a few years, so the dependencies in its pom.xml perhaps are no longer valid. On the other hand, every time I touch Maven something bad happens, so I may just be doing something dumb. Are there any changes to pom.xml or other tricks which can get the build working?" />
  <row Id="121" PostHistoryTypeId="5" PostId="55" RevisionGUID="f431c29d-0106-43d3-a715-17b13dcfb050" CreationDate="2013-02-27T15:13:50.653" UserId="83" Comment="added 92 characters in body" Text="Windows is a family of client, server, and mobile operating systems, developed by Microsoft." />
  <row Id="122" PostHistoryTypeId="24" PostId="55" RevisionGUID="f431c29d-0106-43d3-a715-17b13dcfb050" CreationDate="2013-02-27T15:13:50.653" Comment="Proposed by 83 approved by 101 edit id of 5" />
  <row Id="123" PostHistoryTypeId="5" PostId="38" RevisionGUID="4566ed39-c10a-41a1-9b5b-19c024052b83" CreationDate="2013-02-27T15:13:55.287" UserId="91" Comment="added 108 characters in body" Text="The gradual decay of files caused by storage media failures that can remain unnoticed until they accumulate." />
  <row Id="124" PostHistoryTypeId="24" PostId="38" RevisionGUID="4566ed39-c10a-41a1-9b5b-19c024052b83" CreationDate="2013-02-27T15:13:55.287" Comment="Proposed by 91 approved by 101 edit id of 3" />
  <row Id="125" PostHistoryTypeId="10" PostId="42" RevisionGUID="c48cf2d1-fbe8-408b-b88f-a8b3e2b976a2" CreationDate="2013-02-27T15:16:49.000" UserId="101" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:101,&quot;DisplayName&quot;:&quot;Robert Cartaino&quot;}]}" />
  <row Id="126" PostHistoryTypeId="2" PostId="57" RevisionGUID="e4a5fb4e-bcea-4bc2-9215-2fd130f22ee6" CreationDate="2013-02-27T15:57:02.407" UserId="98" Text=".ATP is (or might be) the Sony mini-disc ATRAC format. take a look [here](http://www.ehow.com/facts_5531617_file-extension-atp.html) and [here](http://file.downloadatoz.com/atp-file-extension/)&#xD;&#xA;&#xD;&#xA;.DSP seems to be audio-related, too. &#xD;&#xA;&#xD;&#xA;.QT2 , according to [this site](http://filext.com/file-extension/QT2), is a file associated with some document viewer, but I couldn't open the original publisher..." />
  <row Id="127" PostHistoryTypeId="2" PostId="58" RevisionGUID="c9594b75-1f4c-4dd0-a567-e877cd08894d" CreationDate="2013-02-27T15:57:46.790" UserId="17" Text="For disks created by pre-OS X versions of Mac OS, the best solution I've found is to (1) create disk images and (2) access their contents with an emulator.&#xD;&#xA;&#xD;&#xA;Your first priority should be to get the data off of the floppies, since they're at risk of degradation. Modern OSes can't read the System 6 filesystem even if they have a 3.5&quot; floppy drive, so what you need is a bit-by-bit disk image of any floppies you want to save. &#xD;&#xA;&#xD;&#xA;You might be able to create these with the SE itself, if it still works and if it has an internal hard drive and some form of network access, although I can't recommend any particular utilities. Alternatively, you can use a USB floppy drive to create images on a modern system using a utility like [dd][1] or [FTK imager][2]. A poster on the thread linked above recommended [this drive][3], although I can't vouch for it myself.&#xD;&#xA;&#xD;&#xA;One issue to be aware of is that not all 3.5&quot; inch floppies are 1.44 MB. In the early years of 3.5&quot; floppies there were [a number of different physical formats][4] (400kb, 800kb, single- and double-density, etc.), and not all drives were compatible with all types. A lot of the &quot;toaster&quot;-style Macs could only read certain varieties -- I remember this giving me headaches even at the time. If you think you might have disks in these formats, you should make sure that they are supported by whichever drive you end up using.&#xD;&#xA;&#xD;&#xA;Once you've created the disk images, you can open and mount them in an [emulator][5]. I can't recommend a particular emulator for an SE, but I use [SheepShaver][6] to access disks created by a circa-1997 PowerTower (one of the short-lived Macintosh clones running System 7.) &#xD;&#xA;&#xD;&#xA;Setup will depend on the particular emulator you're using, but you should be aware that  pre-X versions of Mac OS, including [System 6][7], are available for free from Apple's website. If you still have the installation disks for the software your parents used, you can take images of those and use them to install the necessary applications in the emulation environment without having to delve into the legal gray area of abandonware.&#xD;&#xA;&#xD;&#xA;Migrating your data to a contemporary format will depend on the particular applications and might be difficult. At the cost of losing some of the formatting, you might be able to export the data into an open format (ASCII text for word processing documents, CSV for spreadsheets) that you could then open on your current system. Ideally the application itself will be able to do this, but you might have to dig around to find a file conversion utility that runs on System 6.&#xD;&#xA;&#xD;&#xA;Once you've done all this, you can manage the disk images like you would any other files in your personal organization/backup system. If you plan to preserve this collection long-term (or to eventually store it in a formal repository), you should record any provenance/identifying information so that you know the contents and extent of what you have. &#xD;&#xA;&#xD;&#xA;This doesn't need to be complicated -- human-readable filenames and a spreadsheet are fine -- but you should be able to identify which images came from which (physical) disks and what each image contains, even if you end up discarding the physical copies. (I.e., image &quot;Financial_2.img&quot; came from an 800kb Maxell floppy with a handwritten label &quot;Financial Records Disk 2 of 6&quot; and contains financial data for the year 1988 in ClarisWorks Spreadsheet format.)&#xD;&#xA;&#xD;&#xA;In addition, you might want to keep a record of how to configure and set up the emulator, what the contemporary OS and hardware requirements are to run the emulator, what software (running on the emulator) is necessary to access the files, et cetera. That way, if you pass the collection on to someone else, or if you don't use it for several years and forget exactly what you did, you'll have the Representation Information (in OAIS terms) necessary to keep your data accessible.&#xD;&#xA;&#xD;&#xA;As I mentioned, I used this process to back up and preserve the contents of a System 7 desktop. Accessing the files is as easy as booting SheepShaver and mounting the disk image, although I haven't tackled the process of exporting them to preservation formats yet.&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Dd_%28Unix%29&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/FTK&#xD;&#xA;  [3]: http://www.newegg.com/Product/Product.aspx?Item=N82E16821105004&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/3.5%22_Floppy_Disk#3+1.E2.81.842-inch_floppy_disk_.28.22Microfloppy.22.29&#xD;&#xA;  [5]: http://en.wikipedia.org/wiki/List_of_computer_system_emulators#Apple_Macintosh_with_680x0_CPU&#xD;&#xA;  [6]: http://en.wikipedia.org/wiki/SheepShaver&#xD;&#xA;  [7]: http://download.info.apple.com/Apple_Support_Area/Apple_Software_Updates/English-North_American/Macintosh/System/Older_System/System_6.0.x/&#xD;&#xA;  [8]: http://download.info.apple.com/Apple_Support_Area/Apple_Software_Updates/English-North_American/Macintosh/System/Older_System/System_6.0.x/" />
  <row Id="128" PostHistoryTypeId="5" PostId="56" RevisionGUID="f632573a-d6c6-4bfa-96bd-cbcd835bc814" CreationDate="2013-02-27T16:18:56.080" UserId="36" Comment="added 244 characters in body" Text="What measures, technologies, or techniques are applicable to guarding information against damage/loss during organisation?&#xD;&#xA;&#xD;&#xA;Often when I'm archiving data, before the archiving process I also need to de-duplicate and organise the data. I'm aware that this is a risky time for the data and that human, software or mechanical error might lead to some data being corrupt or lost, without me even knowing about it. &#xD;&#xA;&#xD;&#xA;So there are two questions:&#xD;&#xA;&#xD;&#xA;1) How can I keep an eye on the data so that I can audit where it was versus where it is now, and see if any data has been deleted or changed during organisation?&#xD;&#xA;&#xD;&#xA;* An obvious strategy is to have some sort of checksumming going on&#xD;&#xA;&#xD;&#xA;2) How can I make it harder to make human error while organising the data? &#xD;&#xA;&#xD;&#xA;* Example: you might have a filesystem that simply refuses to let you delete a file if it is unique. Or something like [ZFS][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/ZFS" />
  <row Id="129" PostHistoryTypeId="6" PostId="56" RevisionGUID="f632573a-d6c6-4bfa-96bd-cbcd835bc814" CreationDate="2013-02-27T16:18:56.080" UserId="36" Comment="added 244 characters in body" Text="&lt;organisation&gt;&lt;deduplication&gt;&lt;corruption&gt;" />
  <row Id="130" PostHistoryTypeId="2" PostId="59" RevisionGUID="a570eb99-e763-4871-8c15-57521d56a94f" CreationDate="2013-02-27T16:37:58.653" UserId="105" Text="UTF-8 files should never have a byte-order mark (BOM). The BOM is only used in the UTF-16LE and UTF-16BE encodings to differentiate between the big-endian and little-endian forms. Also, don't forget that there may be higher-level requirements on the contents of files, and adding a character at the beginning may make them invalid. Javascript files in particular will break if they contain a BOM, because it's not a valid part of the Javascript grammar." />
  <row Id="131" PostHistoryTypeId="2" PostId="60" RevisionGUID="f2dc8072-bd4f-4b30-b302-52e775ec9fd1" CreationDate="2013-02-27T16:38:38.610" UserId="17" Text="As you mention in your post, taking regular checksums should be the first step for ensuring continued integrity.&#xD;&#xA;&#xD;&#xA;There are a number of tools that can help you manage files, check for duplicates and calculate checksums, including [CINCH][1], the [Duke Data Accessioner][2] and [Karen's Directory Printer][3]. If you're concerned about getting rid of duplicate files without accidentally deleting non-duplicates, one strategy might be to output a file list with checksums, use it to detect and delete duplicates, and compare the resulting list with the original to confirm that no unique checksums were lost. You could save both outputs as evidence of the process. I don't know if any tools do this automatically but it shouldn't be difficult to script.&#xD;&#xA;&#xD;&#xA;  [1]: http://cinch.nclive.org/Cinch/&#xD;&#xA;  [2]: http://library.duke.edu/uarchives/about/tools/data-accessioner.html&#xD;&#xA;  [3]: http://www.karenware.com/powertools/ptdirprn.asp&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/Google_Refine" />
  <row Id="132" PostHistoryTypeId="5" PostId="53" RevisionGUID="7f43a7aa-3399-44c3-9ff6-4de70f33817d" CreationDate="2013-02-27T16:48:12.037" UserId="26" Comment="added 5 characters in body" Text="I wouldn't see any necessity for adding a BOM to existing 7-bit ASCII files.  Technically, as you mention, 7-bit ASCII files can *already* be treated as UTF-8 without any modification (this was one of the key motivations behind Unicode).  So, if an application claims to be compatible with Unicode, it should be able to read these files by definition.&#xD;&#xA;&#xD;&#xA;**Edit**: Brain-fart... as the other answer states, you shouldn't *ever* need a BOM with UTF-8 encoding. (because it's UTF-8!)" />
  <row Id="133" PostHistoryTypeId="2" PostId="61" RevisionGUID="f31d68b4-e4d0-4d5f-8f9c-c955450d932c" CreationDate="2013-02-27T17:25:32.220" UserId="74" Text="How (and where) to store CD discs to assure that the data stored there would be readable as long as possible? &#xD;&#xA;&#xD;&#xA;Are the room temperature and humidity optimal, or it would be better to store them in colder place? Cellars are usually much colder, but they have often high humidity.&#xD;&#xA;&#xD;&#xA;Additionally, how they should be stored? Is wrapping them in paper a good idea? Or better are textile materials such as cotton?&#xD;&#xA;" />
  <row Id="134" PostHistoryTypeId="1" PostId="61" RevisionGUID="f31d68b4-e4d0-4d5f-8f9c-c955450d932c" CreationDate="2013-02-27T17:25:32.220" UserId="74" Text="How to store CD discs to preserve the data longer?" />
  <row Id="135" PostHistoryTypeId="3" PostId="61" RevisionGUID="f31d68b4-e4d0-4d5f-8f9c-c955450d932c" CreationDate="2013-02-27T17:25:32.220" UserId="74" Text="&lt;storage&gt;&lt;cd&gt;" />
  <row Id="136" PostHistoryTypeId="2" PostId="62" RevisionGUID="27f77388-84d3-40b2-807a-e8128ca25327" CreationDate="2013-02-27T17:33:52.167" UserId="74" Text="Most of contemporary tools and libraries doesn't work well with BOMs for UTF-8 files. I had often to remove the BOM using tools like Notepad++. I've never encountered UTF-8 file with BOM in which the byte order would be inverted, anyway.&#xD;&#xA;&#xD;&#xA;But when it comes to preservation, in 50 years the BOM itself wouldn't be issue, but rather determining the format. BOM is some kind of marking that the file CAN be UTF-8, so paradoxically, it could help future digiarcheologis to determine the character encoding format. Removing the BOM is trivial. &#xD;&#xA;&#xD;&#xA;When storing the data, I would add the README file with ASCII description, what type of character encoding is used and if it is with BOM or not. More important then using BOM or not is to inform the future generations that it is UTF-8 at all, to save them guessing and testing various matches." />
  <row Id="138" PostHistoryTypeId="2" PostId="64" RevisionGUID="acd8e4c4-af33-4db1-a13e-bb8ee3100fd6" CreationDate="2013-02-27T17:41:15.780" UserId="74" Text="PDF is almost de facto standard when it comes to exchanging document. One of the best things is that always, on each machine, the page numbers stay the same, so it can be easily cited in academic publications etc.&#xD;&#xA;&#xD;&#xA;But de facto standard is also opening PDFs with Acrobat Reader. So the single company is making it all functioning fluently. &#xD;&#xA;&#xD;&#xA;However, thinking in longer perspective, say 50 years, is it a good idea to store documents as PDFs? Is the PDF format documented good enough to ensure that after 50 years it will be relatively easy to write software that will read such documents, taking into account that PDF may be then completely deprecated and  no longer supported?" />
  <row Id="139" PostHistoryTypeId="1" PostId="64" RevisionGUID="acd8e4c4-af33-4db1-a13e-bb8ee3100fd6" CreationDate="2013-02-27T17:41:15.780" UserId="74" Text="Is PDF format appropriate for preserving documents with long perspective?" />
  <row Id="140" PostHistoryTypeId="3" PostId="64" RevisionGUID="acd8e4c4-af33-4db1-a13e-bb8ee3100fd6" CreationDate="2013-02-27T17:41:15.780" UserId="74" Text="&lt;publishing&gt;&lt;pdf&gt;" />
  <row Id="141" PostHistoryTypeId="2" PostId="65" RevisionGUID="b73e5b2e-f1db-4a4c-998b-74719c429231" CreationDate="2013-02-27T17:46:58.130" UserId="109" Text="I'd like to be able to capture and validate checksums for large-scale collections of files, typically nested within a complex directory hierarchy. Does every single file need a checksum? Are there ways to leverage the existing directory structure to, say, validate only a node in the file tree and not necessarily every file within?&#xD;&#xA;&#xD;&#xA;I realize this is in part a computer science question but I'm hoping those with digital preservation expertise might have encountered a similar challenge." />
  <row Id="142" PostHistoryTypeId="1" PostId="65" RevisionGUID="b73e5b2e-f1db-4a4c-998b-74719c429231" CreationDate="2013-02-27T17:46:58.130" UserId="109" Text="Most efficient way to generate and validate file checksums" />
  <row Id="143" PostHistoryTypeId="3" PostId="65" RevisionGUID="b73e5b2e-f1db-4a4c-998b-74719c429231" CreationDate="2013-02-27T17:46:58.130" UserId="109" Text="&lt;validation&gt;&lt;file-management&gt;" />
  <row Id="145" PostHistoryTypeId="2" PostId="66" RevisionGUID="eaa67f69-57c1-4b45-9f6e-85560ea4ca22" CreationDate="2013-02-27T17:55:23.927" UserId="74" Text="Usenet newsgroups are cashed and duplicated by numerous servers, and the redundancy is the very important factor for preserving the data.&#xD;&#xA;&#xD;&#xA;When it comes to sharing some trivia and local information, many individuals are using blogs and CMS'es. The example could be, for example, bulletin of small sport community, with information about the competitions, winners etc. &#xD;&#xA;&#xD;&#xA;Is it a good idea to create usenet newsgroup, or use existing group, for sharing such information and preserving them for future generations due to heavy duplication? Are really all usenet messages archived, starting from the beginning? Are the organizations storing them planning to continue that in future? The time of usenet seems to be over, replaced by specialized fora and Q&amp;As from the other side, and the social portals from the other." />
  <row Id="146" PostHistoryTypeId="1" PostId="66" RevisionGUID="eaa67f69-57c1-4b45-9f6e-85560ea4ca22" CreationDate="2013-02-27T17:55:23.927" UserId="74" Text="Are the usenet newsgroups good way to preserve sniplets of publicly available information?" />
  <row Id="147" PostHistoryTypeId="3" PostId="66" RevisionGUID="eaa67f69-57c1-4b45-9f6e-85560ea4ca22" CreationDate="2013-02-27T17:55:23.927" UserId="74" Text="&lt;publishing&gt;&lt;archive&gt;&lt;usenet&gt;" />
  <row Id="152" PostHistoryTypeId="5" PostId="49" RevisionGUID="71528863-2324-4759-9d1a-9ded5d58b68d" CreationDate="2013-02-27T18:46:01.600" UserId="97" Comment="added 669 characters in body" Text="I would argue that if you want *simple to read*, then uncompressed BMP without a lot of bells and whistles is probably a good choice. It is an extremely simple format (basically a header specifying color depth, width, height and some identifying information, followed by the raw pixel data), and it supports up to eight bits per pixel per channel in three channels (red, green and blue) which is the same as PNG and JPEG. From what I understand, any BMP using less than 256 distinct colors also stores the color palette used, but for photos, it makes sense to use 24 bits per pixel to capture as much color fidelity as possible. The remainder of this answer deals primarily with 24-bpp BMPs, but the general principles should hold at any bit depth.&#xD;&#xA;&#xD;&#xA;The lack of compression means the files are very large (a 15 Mpx photo in 24-bit color will be nearly 50 MB in size), but also makes them *easy to interpret* if only you have cursory knowledge of the format. About all you need to know is that there is [a relatively small header](http://en.wikipedia.org/wiki/BMP_file_format#Bitmap_file_header) followed by RGB byte triplets each specifying one pixel (or an appropriate number of bytes per pixel specifying indexes into the palette), and you can probably write a decoder by simple trial and error in a few hours. Given a set of uncompressed BMP files and preferably *some* clue what they are supposed to look like, implementing a BMP viewer from scratch is a fairly trivial task.&#xD;&#xA;&#xD;&#xA;Another upside of uncompressed BMP is that minor data loss (other than in the header) leads to absolutely minimal degradation: only the exact pixels actually affected by the data loss are corrupted. And with some mathematical trickery and human trial-and-error, the header can probably be reconstructed from an otherwise complete image file, even if the image data itself is degraded or corrupted, as long as the number of bytes is correct.&#xD;&#xA;&#xD;&#xA;The major downside of BMP is that the files are huge. For a full-color image of any substantial pixel size, the resulting file size in bytes can be approximated as `3 * H * W`, where `H` and `W` are the height and width in pixels, respectively. As we know, image size expressed in megapixels is simply `H * W`, so for a given image size of `Mp megapixels` this simply reduces to `3 * Mp` megabytes of image data.&#xD;&#xA;&#xD;&#xA;BMPs can also store ICC color profiles, enabling the viewer to accurately render the colors as intended. That would be more complex to implement, and I'm not sure how widespread support for ICC-carrying BMPs is even today, but also would allow more faithful visualization of the data than is possible with simply RGB triplets." />
  <row Id="154" PostHistoryTypeId="4" PostId="8" RevisionGUID="df287431-ef5a-4f53-8705-537ddd952089" CreationDate="2013-02-27T18:48:16.073" UserId="94" Comment="Clarifies that this question is about extracting data from digital media, and *not* digitising (scanning) non-digital media." Text="What are some advantages and disadvantages of outsourcing extraction/transfer of digital content from digital media?" />
  <row Id="155" PostHistoryTypeId="24" PostId="8" RevisionGUID="df287431-ef5a-4f53-8705-537ddd952089" CreationDate="2013-02-27T18:48:16.073" Comment="Proposed by 94 approved by 32 edit id of 6" />
  <row Id="156" PostHistoryTypeId="5" PostId="49" RevisionGUID="74aaa703-a229-4166-9748-f8c183f2e794" CreationDate="2013-02-27T18:53:27.187" UserId="97" Comment="added 669 characters in body" Text="I would argue that if you want *simple to read*, then uncompressed BMP without a lot of bells and whistles is probably a good choice. It is an extremely simple format (basically a header specifying color depth, width, height and some identifying information, followed by the raw pixel data), and it supports up to eight bits per pixel per channel in three channels (red, green and blue) which is the same as PNG and JPEG. From what I understand, any BMP using less than 256 distinct colors also stores the color palette used, but for photos, it makes sense to use 24 bits per pixel to capture as much color fidelity as possible. The remainder of this answer deals primarily with 24-bpp BMPs, but the general principles should hold at any bit depth.&#xD;&#xA;&#xD;&#xA;The lack of compression means the files are very large (a 15 Mpx photo in 24-bit color will be nearly 50 MB in size), but also makes them *easy to interpret* if only you have cursory knowledge of the format. About all you need to know is that there is [a relatively small header](http://en.wikipedia.org/wiki/BMP_file_format#Bitmap_file_header) followed by RGB byte triplets each specifying one pixel (or an appropriate number of bytes per pixel specifying indexes into the palette), and you can probably write a decoder by simple trial and error in a few hours. Given a set of uncompressed BMP files and preferably *some* clue what they are supposed to look like, implementing a BMP viewer from scratch is a fairly trivial task.&#xD;&#xA;&#xD;&#xA;Another upside of uncompressed BMP is that minor data loss (other than in the header) leads to absolutely minimal degradation: only the exact pixels actually affected by the data loss are corrupted. And with some mathematical trickery and human trial-and-error, the header can probably be reconstructed from an otherwise complete image file, even if the image data itself is degraded or corrupted, as long as the number of bytes is correct.&#xD;&#xA;&#xD;&#xA;The major downside of BMP is that the files are huge. For a full-color image of any substantial pixel size, the resulting file size in bytes can be approximated as `3 * H * W`, where `H` and `W` are the height and width in pixels, respectively. As we know, image size expressed in megapixels is simply `H * W`, so for a given image size of `Mp megapixels` this simply reduces to `3 * Mp` megabytes of image data.&#xD;&#xA;&#xD;&#xA;BMPs can also store ICC color profiles, enabling the viewer to accurately render the colors as intended. That would be more complex to implement, and I'm not sure how widespread support for ICC-carrying BMPs is even today, but also would allow more faithful visualization of the data than is possible with simply RGB triplets.&#xD;&#xA;&#xD;&#xA;Pretty much any other format which meets the requirement of being easy to read and long-lived in the face of advancing technology, compression algorithms and so on, is going to basically boil down to the same thing (a container plus raw, uncompressed bitmap data representing pixel color value triplets). Uncompressed TIFF for example falls into that category. So unless you have specific needs which cannot be met using BMP you would gain little by choosing such a more advanced format, and risk making it more difficult to understand for someone who has to reverse-engineer it. Better then to stick to the KISS principle: *keep it simple, stupid*. For preservation purposes, I'd take a stack of BMPs over a single multi-page uncompressed TIFF any day." />
  <row Id="158" PostHistoryTypeId="2" PostId="70" RevisionGUID="b8d191dd-b0a8-4b5d-9f9a-4bfffb6dbdf3" CreationDate="2013-02-27T19:32:35.280" UserId="32" Text="The Archivematica project made format policy decisions based on a review of significant characteristics and decided upon the best preservation and access formats based on testing open source tool outcomes, availability of open source conversion tools and ubiquity of format rendering software. &#xD;&#xA;&#xD;&#xA;In short, for raster images, we chose uncompressed TIFF for all but JPEG2000 and PNG, which will be retained in their original formats for preservation and JPEG for access. For vector images, we chose SVG 1.1 for preservation and PDF for access. Finally, for raw camera files we chose to keep preservation copies in their original format though we may consider DNG in the future. For access of raw camera files, we again selected JPEG.&#xD;&#xA;&#xD;&#xA;You can find detailed discussions of our decision-making and testing here:&#xD;&#xA;Raster images: https://www.archivematica.org/wiki/Raster_images&#xD;&#xA;Raw camera files: https://www.archivematica.org/wiki/Raw_camera_files&#xD;&#xA;Vector images: https://www.archivematica.org/wiki/Vector_images" />
  <row Id="159" PostHistoryTypeId="2" PostId="71" RevisionGUID="12493e6d-781b-4522-9a56-9fc4a62c6816" CreationDate="2013-02-27T19:46:34.153" UserId="97" Text="**I would not consider Usenet a viable archival strategy.** Not now nor in the past. Yes, it's true that storage of content is distributed and by design duplicated, but the control over such parameters as storage time is entirely in the hands of third parties who may have no interest in your particular content, and they are by no means *obliged* to provide you (or anyone else, really) with access to their archives.&#xD;&#xA;&#xD;&#xA;Not all of Usenet is archived. There are many hierarchies which are only available in certain regions or to users of certain ISPs (or Usenet server providers), and there are hierarchies which are generally excluded from archiving (`alt.binaries.*` comes to mind). Also, you'd probably be hard pressed to find a publicly available archive that goes back a significant amount of time. DejaNews, which got bought by Google, had a quite good archive, and I do believe that Google is allowing access to all of it, but last I heard it wasn't really searchable in a practical way.&#xD;&#xA;&#xD;&#xA;NNTP is largely a best-effort protocol. Not all news posts are guaranteed to be on every server (though usually they do end up that way eventually), and they are not guaranteed to be delivered to each server in the order of their creation. Different posts may be delivered through entirely different servers. This is all a product of NNTP's initial design, where it had to run over such arcane transport protocols as UUCP as well as TCP/IP (and many others; &quot;everyone&quot; running TCP/IP is a relatively new invention, and now we are moving into a mixed protocol world again as we head into a combined IPv4/IPv6 Internet).&#xD;&#xA;&#xD;&#xA;I don't know exactly how the process would go for getting a new newsgroup into the major archives. That would probably depend on their configuration, which is a factor you don't control. You could contact the administrators and ask for your group to be included if it doesn't show up in their archives, but that's about it as far as control over what is archived goes. The posts to be archived would still have to make it to them, however and obviously, and they would almost certainly only archive content from the point in time of inclusion of the newsgroup into their archive. I don't see one Usenet archive(r/provider) going out of their way to track down copies of previous posts.&#xD;&#xA;&#xD;&#xA;Access for regular users is limited. While there are both free and fee-based Usenet servers available to the general public, and software is available for the taking, very few people these days are likely to install and configure a NNTP client. So while technically most anyone who has Internet access will be able to access Usenet if they put in the effort, few people put in the effort." />
  <row Id="160" PostHistoryTypeId="2" PostId="72" RevisionGUID="471da046-e884-414e-96fd-658b6b4d789c" CreationDate="2013-02-27T21:02:30.223" UserId="113" Text="The *ideal* format for preservation at this time is TIFF with 24bit color and no compression. JPEG using the highest quality setting (minimal lossy compression) will serve many personal preservation needs and some professional, well enough. Adobe DNG for preservation coupled with JPEG is worth considering, and is a method used for high-quality digital photography, but could also be used for scanned photos.&#xD;&#xA;&#xD;&#xA;The &quot;extra&quot; questions about resolution and color adjustments are probably best answered separately. Nonetheless, they are important, and so briefly:&#xD;&#xA; &#xD;&#xA;400 pixels per inch (ppi) scanning resolution is sufficient for most printed photographs. 600 ppi might be necessary to capture the grain in certain situation. Approximately 3000 ppi is typically sufficient to capture the full detail of a photograph from 35mm film. Only use the native *optical* resolutions of the scanning device. Color management using ICC color profiles and calibration techniques will help to achieve the best possible results.&#xD;&#xA;&#xD;&#xA;The hardware used to do the scanning plays a huge role in the quality of the results.&#xD;&#xA;&#xD;&#xA;Digging into the Details:&#xD;&#xA;&#xD;&#xA;The *ideal* format for preserving analog digital photos is one that:&#xD;&#xA;&#xD;&#xA; 1. Does not use lossy compression&#xD;&#xA; 2. Has an open definition&#xD;&#xA; 3. Is not proprietary nor use proprietary components such as compression&#xD;&#xA; 4. Supports sufficient bit depth to represent the full dynamic range of color found in the original&#xD;&#xA; 5. Is widely adopted by the preservation community&#xD;&#xA; 6. Is approved by an international standards organization, or is at least a de-facto standard&#xD;&#xA; 7. Should support embedded metadata standards&#xD;&#xA; 8. Supports ICC color profiles&#xD;&#xA;&#xD;&#xA;[TIFF][1] largely complies with the above. More specifically, TIFF with 24 bits of RGB color or 8 bits grayscale for black and white images, without compression. It has proprietary origins but is open and probably more widely adopted than any other format making it a de-facto standard. Like most formats, there are many varieties of TIFF. TIFF without compression is most appropriate. TIFF is rarely used with LZW lossless compression within the preservation community, because [LZW][2] has multiple commercially held patents associated with it, making it undesirable.&#xD;&#xA;&#xD;&#xA;JPEG does not comply because is uses lossy compression. Every time a lossy compression is resaved, the visual artifacts of the lossy compression are compounded and the quality of the image degrades. Some software applications, such as iPhoto, mitigate the effects by keeping the original so the option always exists to revert to it. Technically, the JPEG format supports lossless compression as well, but this feature is so rarely implemented in software that it cannot be depended on in the preservation context. &#xD;&#xA;&#xD;&#xA;JPEG2000 complies if used with only lossless compression (a.k.a. reversible), though it is relatively a very complicated format. It is used extensively by some large preservation repositories, but is not a good choice for the average user. &#xD;&#xA;&#xD;&#xA;RAW formats tend to be proprietary and therefore not as reliable in the long term.  RAW formats store the greatest amount of visual data because it represents what the hardware is actually capable of, but are not supported widely by viewing applications. [Adobe Digital Negative][3] (DNG) is, however, a relatively good option in this realm. Pair Adobe DNG with JPEG and you have a rich master and an easy to use copy, in that order.&#xD;&#xA;&#xD;&#xA;Finally, this topic is one that has been examined thoroughly on an ongoing basis for a couple of decades in the digital preservation realm and there is a ton of information available on the web if you care to explore it further. I've tried to give a concise answer (ha!) without compromising preservation ideals. The external links of provided barely scratch the surface.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.digitalpreservation.gov/formats/fdd/fdd000022.shtml&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/Digital_Negative" />
  <row Id="161" PostHistoryTypeId="2" PostId="73" RevisionGUID="20f29edf-efab-4c3b-ac33-0ba715423492" CreationDate="2013-02-27T21:13:25.077" UserId="64" Text="What steps can I take to help ensure my digital media will be preserved for the future?  I've heard that active management is a key to success, but what does this mean?  My media's life can be affected by at least these issues:&#xD;&#xA;&#xD;&#xA;1. Durability of media&#xD;&#xA;2. How the media is used, handled, and stored&#xD;&#xA;3. How likely will the media become obsolete&#xD;&#xA;&#xD;&#xA;Given the above, what steps can I take to mitigate these risks?" />
  <row Id="162" PostHistoryTypeId="1" PostId="73" RevisionGUID="20f29edf-efab-4c3b-ac33-0ba715423492" CreationDate="2013-02-27T21:13:25.077" UserId="64" Text="What steps can I take to help ensure my digital media will be preserved for the future?" />
  <row Id="163" PostHistoryTypeId="3" PostId="73" RevisionGUID="20f29edf-efab-4c3b-ac33-0ba715423492" CreationDate="2013-02-27T21:13:25.077" UserId="64" Text="&lt;durabilty&gt;&lt;obsolecence&gt;&lt;handling&gt;&lt;storing&gt;&lt;using&gt;" />
  <row Id="164" PostHistoryTypeId="2" PostId="74" RevisionGUID="91480542-9b97-450f-b6aa-1c25ff73b80d" CreationDate="2013-02-27T21:13:25.077" UserId="64" Text="The [Digital Preservation U.S. government website](http://digitalpreservation.gov/personalarchiving/documents/media_durability.pdf) has these suggestions of steps you can actively take:&#xD;&#xA;&#xD;&#xA;• Have at least two separate copies of your content on separate &#xD;&#xA;media, more copies are better.&#xD;&#xA;&#xD;&#xA;• Use different kinds of media (DVDs, CDs, portable hard drives, thumb drives or Internet storage); use reputable vendors and products.&#xD;&#xA;&#xD;&#xA;• Store media copies in different locations that are as physically far apart as practical.&#xD;&#xA;&#xD;&#xA;• Label media properly and keep in secure locations (such as with important papers).&#xD;&#xA;&#xD;&#xA;• Create new archival media copies at least every five years to avoid data loss." />
  <row Id="166" PostHistoryTypeId="2" PostId="75" RevisionGUID="57d5741c-63a0-4ef9-a27c-37bb92449b85" CreationDate="2013-02-27T21:28:43.910" UserId="26" Text="In addition to the things already mentioned, you should take advantage of the recent emergence of **cloud storage**.&#xD;&#xA;&#xD;&#xA;I'm fairly certain that cloud storage will become more and more prevalent. So, as you continue to make local backups on physical media, you can also upload your data to various cloud servers for an additional level of redundancy." />
  <row Id="167" PostHistoryTypeId="5" PostId="72" RevisionGUID="c748143c-9a00-4e56-a13c-3a6318629e1a" CreationDate="2013-02-27T22:28:46.593" UserId="113" Comment="added 626 characters in body" Text="The *ideal* format for preservation at this time is TIFF with 24bit color and no compression. PNG with lossless compression is a reasonable alternative. JPEG using the highest quality setting (minimal lossy compression) will serve many personal preservation needs and some professional, well enough. Adobe DNG for preservation coupled with JPEG is worth considering, and is a method used for high-quality digital photography, but could also be used for scanned photos.&#xD;&#xA;&#xD;&#xA;The &quot;extra&quot; questions about resolution and color adjustments are probably best answered separately. Nonetheless, they are important, and so briefly:&#xD;&#xA; &#xD;&#xA;400 pixels per inch (ppi) scanning resolution is sufficient for most printed photographs. 600 ppi might be necessary to capture the grain in certain situation. Approximately 3000 ppi is typically sufficient to capture the full detail of a photograph from 35mm film. Only use the native *optical* resolutions of the scanning device. Color management using ICC color profiles and calibration techniques will help to achieve the best possible results.&#xD;&#xA;&#xD;&#xA;The hardware used to do the scanning plays a huge role in the quality of the results.&#xD;&#xA;&#xD;&#xA;Digging into the Details:&#xD;&#xA;&#xD;&#xA;The *ideal* format for preserving analog digital photos is one that:&#xD;&#xA;&#xD;&#xA; 1. Does not use lossy compression&#xD;&#xA; 2. Has an open definition&#xD;&#xA; 3. Is not proprietary nor use proprietary components such as compression&#xD;&#xA; 4. Supports sufficient bit depth to represent the full dynamic range of color found in the original&#xD;&#xA; 5. Is widely adopted by the preservation community&#xD;&#xA; 6. Is approved by an international standards organization, or is at least a de-facto standard&#xD;&#xA; 7. Should support embedded metadata standards&#xD;&#xA; 8. Supports ICC color profiles&#xD;&#xA;&#xD;&#xA;[TIFF][1] largely complies with the above. More specifically, TIFF with 24 bits of RGB color or 8 bits grayscale for black and white images, without compression. It has proprietary origins but is open and probably more widely adopted than any other format making it a de-facto standard. Like most formats, there are many varieties of TIFF. TIFF without compression is most appropriate. TIFF is rarely used with LZW lossless compression within the preservation community, because [LZW][2] has multiple commercially held patents associated with it, making it undesirable.&#xD;&#xA;&#xD;&#xA;[PNG][3] is a viable preservation format that supports lossless compression that is free of patents (unlike LZW). If TIFF had not come first, PNG might be used more widely for preservation than it is.&#xD;&#xA;&#xD;&#xA;JPEG does not comply because is uses lossy compression. Every time a lossy compression is resaved, the visual artifacts of the lossy compression are compounded and the quality of the image degrades. Some software applications, such as iPhoto, mitigate the effects by keeping the original so the option always exists to revert to it. Technically, the JPEG format supports lossless compression as well, but this feature is so rarely implemented in software that it cannot be depended on in the preservation context. &#xD;&#xA;&#xD;&#xA;JPEG2000 complies if used with only lossless compression (a.k.a. reversible), though it is relatively a very complicated format. It is used extensively by some large preservation repositories, but is not a good choice for the average user. &#xD;&#xA;&#xD;&#xA;RAW formats tend to be proprietary and therefore not as reliable in the long term.  RAW formats store the greatest amount of visual data because it represents what the hardware is actually capable of, but are not supported widely by viewing applications. [Adobe Digital Negative][4] (DNG) is, however, a relatively good option in this realm. Pair Adobe DNG with JPEG and you have a rich master and an easy to use copy, in that order.&#xD;&#xA;&#xD;&#xA;[BMP][5] is a proprietary format controlled by Microsoft that is deeply integrated into the Windows operating system. It was not designed for interoperability. Therefore, it has not been used widely for preservation. &#xD;&#xA;&#xD;&#xA;Finally, this topic is one that has been examined thoroughly on an ongoing basis for a couple of decades in the digital preservation realm and there is a ton of information available on the web if you care to explore it further. I've tried to give a concise answer (ha!) without compromising preservation ideals. The external links of provided barely scratch the surface.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.digitalpreservation.gov/formats/fdd/fdd000022.shtml&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch&#xD;&#xA;  [3]: http://www.digitalpreservation.gov/formats/fdd/fdd000153.shtml&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/Digital_Negative&#xD;&#xA;  [5]: http://www.digitalpreservation.gov/formats/fdd/fdd000189.shtml" />
  <row Id="168" PostHistoryTypeId="2" PostId="76" RevisionGUID="913df2c4-b8a3-445c-9b9a-55a316d0c1b0" CreationDate="2013-02-27T22:30:59.533" UserId="30" Text="After a certain period of time, all data loses value. For example, the IRS only requires a few years of tax information be retained, beyond which they cannot audit by law.&#xD;&#xA;&#xD;&#xA;At what point should &quot;preservation&quot; be changed into &quot;conservation&quot; - ie, when should data no longer be considered vital and allowed to die (or be actively deleted)?&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&lt;sup&gt;&lt;sub&gt;related to, but different from, [How to handle sensibly lossful digital long term preservation?](http://digitalpreservation.stackexchange.com/q/39/30)&lt;/sub&gt;&lt;/sup&gt;" />
  <row Id="169" PostHistoryTypeId="1" PostId="76" RevisionGUID="913df2c4-b8a3-445c-9b9a-55a316d0c1b0" CreationDate="2013-02-27T22:30:59.533" UserId="30" Text="When should digital preservation be stopped?" />
  <row Id="170" PostHistoryTypeId="3" PostId="76" RevisionGUID="913df2c4-b8a3-445c-9b9a-55a316d0c1b0" CreationDate="2013-02-27T22:30:59.533" UserId="30" Text="&lt;storage&gt;&lt;lifetime&gt;" />
  <row Id="171" PostHistoryTypeId="6" PostId="39" RevisionGUID="c1592b11-e123-405c-804e-6ca86426756c" CreationDate="2013-02-27T22:33:17.940" UserId="30" Comment="edited tags" Text="&lt;storage&gt;&lt;lifetime&gt;" />
  <row Id="172" PostHistoryTypeId="5" PostId="39" RevisionGUID="ed8933f9-6b43-4818-9940-8ba8d9f4fe06" CreationDate="2013-02-27T23:15:36.903" UserId="30" Comment="spelling fix; tags" Text="Is there an application, or a module/plug-in in a long time preservation framework, which makes a storage system systematically lose information you *don't* need any longer while preserving the data you *will* need again in the future?&#xD;&#xA;&#xD;&#xA;I think of a system that could distinguish accesses to data which indicate that it was used (e.g. reading in a file for a longer time that suggests that a user really reads it, or re-opening the same file without using search in prior, or with a certain frequency per month, year, ...) from accesses which are not likely to suggest the data to be of any worth any more (e.g. virus scanners or search systems inspecting those files or users flipping through files found from search results for a short time only, then going on to the next hit suggesting they didn't find the information they were looking for in them) and finally forgetting / overwriting uninteresting data in case that storage becomes short while keeping the worthwhile data.&#xD;&#xA;&#xD;&#xA;I observe this for a couple of times whenever I migrated to new desktop computer hardware, I kept a backup but usually I only need very few things out of it later on, either because old software cannot be used in a sensible way any longer (what to do with a DOS screen saver on Windows?) or, for personal data, because of interests simply shifting. (For example, I am no longer interested in things I cared for as a teenager.) The same can be observed in libraries: It may happen that, if you randomly pick a book from a shelf, its pages stick together, indicating no one has read it for a couple of centuries.&#xD;&#xA;&#xD;&#xA;Are there (problably software) approaches that address the problem of sensible forgetting in digital age?" />
  <row Id="173" PostHistoryTypeId="24" PostId="39" RevisionGUID="ed8933f9-6b43-4818-9940-8ba8d9f4fe06" CreationDate="2013-02-27T23:15:36.903" Comment="Proposed by 30 approved by 10 edit id of 7" />
  <row Id="174" PostHistoryTypeId="2" PostId="77" RevisionGUID="35b4077e-8bfe-4e86-91cb-2214663aae48" CreationDate="2013-02-27T23:37:48.863" UserId="78" Text="I am not sure exactly what the media that you will be working with will lend itself too. I am also not quite sure what you mean by 'evaluate'. &#xD;&#xA;&#xD;&#xA;That said, a number of digitization labs post their guidelines and the scanners that they use for digitization. These documents might give you an idea of what questions you should be asking based on your materials. &#xD;&#xA;&#xD;&#xA;UNT has an excellent page up with examples of the scanned results [here][1]. &#xD;&#xA;&#xD;&#xA;The CDRH in Lincoln also has a good guide (including best practices links and readings) [here][2].&#xD;&#xA;&#xD;&#xA;I hope this helps!&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.library.unt.edu/digital-projects-unit/scanners-and-scanning-systems&#xD;&#xA;  [2]: http://cdrh.unl.edu/articles/scanning_benchmarks.php" />
  <row Id="175" PostHistoryTypeId="2" PostId="78" RevisionGUID="c9c94e68-20e6-4d9a-9600-e91c7bc2b65a" CreationDate="2013-02-27T23:54:49.487" UserId="70" Text="Since the storage capacities and data volumes have been growing exponentially, you can easily see for yourself that the cost of storing data indefinitely is limited.&#xD;&#xA;&#xD;&#xA;I.e. you need $1000 for storing 1 year, $1500 for storing 2 years, $1750 for 3 years and so on - and $2000 for storing data indefinitely.&#xD;&#xA;&#xD;&#xA;So, my personal opinion observerd on a number of projects is that there is no need to delete (or even support deletion in the system design) because the cost decreases together or faster than the value does." />
  <row Id="176" PostHistoryTypeId="2" PostId="79" RevisionGUID="f93576f0-5c5e-450e-8df7-e2cca41a4ba5" CreationDate="2013-02-27T23:57:47.613" UserId="54" Text="The internal battery in Game Boy cartridges eventually depletes, which erases its temporary contents (the game progress saved in RAM). How can I backup and restore this game save (e.g., the original *Pokémon*) between battery replacements or as a precaution against the effects of time?" />
  <row Id="177" PostHistoryTypeId="1" PostId="79" RevisionGUID="f93576f0-5c5e-450e-8df7-e2cca41a4ba5" CreationDate="2013-02-27T23:57:47.613" UserId="54" Text="How do I backup and restore a Game Boy cartridge save file?" />
  <row Id="178" PostHistoryTypeId="3" PostId="79" RevisionGUID="f93576f0-5c5e-450e-8df7-e2cca41a4ba5" CreationDate="2013-02-27T23:57:47.613" UserId="54" Text="&lt;electronics&gt;&lt;ram&gt;&lt;video-games&gt;" />
  <row Id="179" PostHistoryTypeId="2" PostId="80" RevisionGUID="eff85711-178b-4a82-8538-37878e3bae5f" CreationDate="2013-02-27T23:58:54.853" UserId="70" Text="There are generally 2 approaches:&#xD;&#xA;&#xD;&#xA;1. &quot;exponential memory&quot;, which assumes we know how to aggregate two pieces of data into one, maybe as simple as throwing one out, or averaging 2 data points&#xD;&#xA;&#xD;&#xA;- for one year, we store 100 points of data, for next year we throw out 50, then 25 and so on&#xD;&#xA;&#xD;&#xA;2. &quot;store everything&quot;, because the data volumes grow exponentially and storage becomes cheaper exponentially, so the total cost is limited even when storing infinite history.&#xD;&#xA;&#xD;&#xA;Approach 1 needs cherry-picking of nontrivial data (such as books), which can make it even more costly than approach 2. So, I vote to save everything with the highest precision possible." />
  <row Id="180" PostHistoryTypeId="2" PostId="81" RevisionGUID="e276fcf9-68e3-4d6f-ba24-f07ea14d6bbd" CreationDate="2013-02-28T00:12:03.567" UserId="70" Text="For example, I have quite large collection of books, audio, photos etc.&#xD;&#xA;&#xD;&#xA;Setting aside the question of preserving the data itself, it is equally important to make it easily searchable and catalog-able, without having to know how to parse the file contents (eg EXIF tags)&#xD;&#xA;&#xD;&#xA;So I tried a number of software packages that handle libraries, but they all seem to get obsolete real fast.&#xD;&#xA;&#xD;&#xA;Right now I am using a combination of custom-written wiki software based on CouchDB (because it's easy to replicate) together with JSON-formatted descriptors in folders. It is not pretty but it works.&#xD;&#xA;&#xD;&#xA;**Are there any &quot;best practices&quot; for storing metadata for file collections?**" />
  <row Id="181" PostHistoryTypeId="1" PostId="81" RevisionGUID="e276fcf9-68e3-4d6f-ba24-f07ea14d6bbd" CreationDate="2013-02-28T00:12:03.567" UserId="70" Text="How to future-proof digital metadata" />
  <row Id="182" PostHistoryTypeId="3" PostId="81" RevisionGUID="e276fcf9-68e3-4d6f-ba24-f07ea14d6bbd" CreationDate="2013-02-28T00:12:03.567" UserId="70" Text="&lt;file-management&gt;&lt;metadata&gt;&lt;library-management&gt;&lt;organization&gt;" />
  <row Id="183" PostHistoryTypeId="2" PostId="82" RevisionGUID="6d0174dc-bd83-4ebb-a84a-a5e264135123" CreationDate="2013-02-28T00:27:07.303" UserId="70" Text="Overall, I had a number of devastating data losses during organization, so here are empirically chosen rules:&#xD;&#xA;&#xD;&#xA;1. Never &quot;move&quot;. Always copy. If you need to re-organize something, create a fresh repository (folder, database, whatever) and gradually copy the data over to the new database. The hardware costs nothing compared to time lost on restoration.&#xD;&#xA;2. Create a separate list of &quot;processed&quot; things. And compare it to the &quot;original&quot; list to see if you're done. Preferably by a tool.&#xD;&#xA;3. If the new data arrives during re-org, add it BOTH to new and old repositories.&#xD;&#xA;4. Keep the original copy for at least 1 year after re-org.&#xD;&#xA;5. Have a person different from the one who did the re-org to have a fresh glance over the data (This is surprisingly effective)&#xD;&#xA;6. De-duplication should also be done by a tool, not manually. And unless the files are bit-by-bit identical, it is usually better to keep both copies (e.g. photo both in TIFF and in JPEG format), selecting one copy as the primary one. For example, when deduplicating books, you might mistake different editions as one, and lose the differences. Even more important is to keep all translations if you have many.&#xD;&#xA;" />
  <row Id="184" PostHistoryTypeId="2" PostId="83" RevisionGUID="37305374-fbb0-4a12-9131-36a3b9296b48" CreationDate="2013-02-28T00:28:52.793" UserId="54" Text="Perhaps blasphemous to some, but if I break a book's binding to feed it to a duplex scanner, am I legally required (in the US) to keep the physical pages post-scanning in order to justify having the digital version?" />
  <row Id="185" PostHistoryTypeId="1" PostId="83" RevisionGUID="37305374-fbb0-4a12-9131-36a3b9296b48" CreationDate="2013-02-28T00:28:52.793" UserId="54" Text="If I break a book's binding to scan its pages, am I legally required to keep the book's physical remains to use the digital copy?" />
  <row Id="186" PostHistoryTypeId="3" PostId="83" RevisionGUID="37305374-fbb0-4a12-9131-36a3b9296b48" CreationDate="2013-02-28T00:28:52.793" UserId="54" Text="&lt;digitize&gt;&lt;legal&gt;&lt;scanning&gt;" />
  <row Id="187" PostHistoryTypeId="2" PostId="84" RevisionGUID="39b2d390-2d72-4fbf-b767-f22229544ead" CreationDate="2013-02-28T00:32:16.553" UserId="70" Text="I am not a lawyer, but I think in most jurisdictions you can get a notary to assert the fact that you indeed owned the original books, and then destroyed (and not sold, so no transfer of rights) them.&#xD;&#xA;&#xD;&#xA;Then you can keep this notice instead of a whole stack of books in some safe place. Not entirely effortless, but certainly easier than storing all the paper." />
  <row Id="188" PostHistoryTypeId="6" PostId="83" RevisionGUID="0c506c15-3cb2-47ae-816c-5f46324bd0d5" CreationDate="2013-02-28T00:34:50.390" UserId="54" Comment="edited tags" Text="&lt;digitize&gt;&lt;legal&gt;&lt;scanning&gt;&lt;law&gt;" />
  <row Id="189" PostHistoryTypeId="2" PostId="85" RevisionGUID="fca748b3-f29c-42ec-bfee-a7aba9d7e845" CreationDate="2013-02-28T00:47:19.650" UserId="70" Text="It is hard to predict which formats will stick, but there are general rules to consider:&#xD;&#xA;&#xD;&#xA;1. The format should be simple (i.e. do not require setting up complex streaming servers etc). For example, MJPEG is simpler than MPEG which is simpler than full-blown DVD container with menus etc.&#xD;&#xA;2. The format should not require cumbersome licensing or obtaining patent rights to decoding/playing.&#xD;&#xA;3. Source code for the decoders should be available, otherwise we risk decoders dying together with companies that maintain them&#xD;&#xA;4. Since different codecs introduce different kinds of artifacts, give plenty of headroom when selecting bit rate. If possible, use PSNR or some other metrics to check re-encoded video for artifacts. If the video can be encoded to lossless formats like huffyuv with enough compression (e.g. screencasts or cartoons), use them.&#xD;&#xA;5. Keep the originals too.&#xD;&#xA;6. If possible, keep 2 formats - one with inter-frame compression (e.g. H264) and one with intra-frame only (e.g. MJPEG). Use the latter for creating new re-encodes when needed.&#xD;&#xA;" />
  <row Id="190" PostHistoryTypeId="2" PostId="86" RevisionGUID="6f7bd5a3-af22-4be6-8e4f-c5226c8e800a" CreationDate="2013-02-28T00:50:48.810" UserId="26" Text="There is an Arduino-based tutorial for reading the ROM and RAM from the GameBoy cartridge:&#xD;&#xA;http://www.insidegadgets.com/2011/03/28/gbcartread-arduino-based-gameboy-cart-reader-%E2%80%93-part-2-read-the-ram/&#xD;&#xA;&#xD;&#xA;And for writing back to RAM:&#xD;&#xA;http://www.insidegadgets.com/2011/04/07/gbcartread-arduino-based-gameboy-cart-reader-%e2%80%93-part-3-write-to-ram&#xD;&#xA;&#xD;&#xA;Or you can switch to playing the games in an emulator." />
  <row Id="191" PostHistoryTypeId="2" PostId="87" RevisionGUID="79f136de-d625-4c6e-8432-d95cfd34c3f2" CreationDate="2013-02-28T00:57:14.217" UserId="70" Text="There is a known problem with the Wayback Machine regarding domain name ownership - oftentimes, the domain changes hands, and the new owner either puts a spam blog on it, or puts a robots.txt which forbids crawling, and then WM promptly deletes the entire website history.&#xD;&#xA;&#xD;&#xA;So, first, are there archival engines which do not delete the history in those cases?&#xD;&#xA;&#xD;&#xA;and, second, is it legal to crawl and store personal copies of websites to protect against this particular problem? or to build such an engine?&#xD;&#xA;&#xD;&#xA;(Please comment if this question better be split to 2-3 different ones)" />
  <row Id="192" PostHistoryTypeId="1" PostId="87" RevisionGUID="79f136de-d625-4c6e-8432-d95cfd34c3f2" CreationDate="2013-02-28T00:57:14.217" UserId="70" Text="Preserving website content" />
  <row Id="193" PostHistoryTypeId="3" PostId="87" RevisionGUID="79f136de-d625-4c6e-8432-d95cfd34c3f2" CreationDate="2013-02-28T00:57:14.217" UserId="70" Text="&lt;archive&gt;&lt;legal&gt;&lt;crawling&gt;&lt;www&gt;&lt;digital-born&gt;" />
  <row Id="195" PostHistoryTypeId="2" PostId="88" RevisionGUID="7e461e57-7399-4079-b952-447cb455887d" CreationDate="2013-02-28T01:29:26.213" UserId="76" Text="In particular, I'm thinking about [Archive-it][1], [HTTrack][2], [Heritrix][3], and [WGet][4]. I would be particularly interested in which tools make sense in small, medium and large cultural heritage organizations (Libraries, Archives, Museums, Historical Societies) and in consideration of use of standards, preservation risks, ease of use, differences in approaches to significant properties and sustainability. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.archive-it.org/&#xD;&#xA;  [2]: http://www.httrack.com/&#xD;&#xA;  [3]: https://webarchive.jira.com/wiki/display/Heritrix/Heritrix&#xD;&#xA;  [4]: http://www.gnu.org/software/wget/" />
  <row Id="196" PostHistoryTypeId="1" PostId="88" RevisionGUID="7e461e57-7399-4079-b952-447cb455887d" CreationDate="2013-02-28T01:29:26.213" UserId="76" Text="When does it make sense to use different web archiving tools?" />
  <row Id="197" PostHistoryTypeId="3" PostId="88" RevisionGUID="7e461e57-7399-4079-b952-447cb455887d" CreationDate="2013-02-28T01:29:26.213" UserId="76" Text="&lt;software&gt;&lt;web-archiving&gt;&lt;open-source&gt;&lt;ingest&gt;" />
  <row Id="198" PostHistoryTypeId="6" PostId="87" RevisionGUID="1da77a4c-7b85-4458-af7b-5c0286233e3e" CreationDate="2013-02-28T01:33:54.180" UserId="76" Comment="edited tags" Text="&lt;legal&gt;&lt;crawling&gt;&lt;www&gt;&lt;digital-born&gt;&lt;web-archiving&gt;" />
  <row Id="199" PostHistoryTypeId="2" PostId="89" RevisionGUID="a306196a-032e-4a04-834e-aea30a63dc95" CreationDate="2013-02-28T01:43:10.717" UserId="76" Text="The notion of [Archival appraisal][1] is relevant here. Organizations need to make decisions about the long term value of materials. Some materials have permanent value, some are set to specific retention schedules. In any case, it is a matter of an organization identifying why something is important and what uses it can serve in the future and then deciding how to manage content toward those ends. &#xD;&#xA;&#xD;&#xA;Side note, all date does **not** lose it's value. There are a lot of things that weren't thought of as valuable at all but were kept around as a result of benign neglect (for example [Martha Ballard's Diary][2]). There are also things like antiquarian books, or early drafts of scientific papers, that have enormous historical value. The point here is that value is always dependent on some intended use. So the question is, how is the data useful to you and your organization and who else might it be useful to in the future. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Archival_appraisal&#xD;&#xA;  [2]: http://dohistory.org/diary/about.html" />
  <row Id="200" PostHistoryTypeId="2" PostId="90" RevisionGUID="226996cd-2cf2-4c72-ac5b-d2ee8963cfa5" CreationDate="2013-02-28T03:40:53.893" UserId="76" Text="There are some emerging practices in archives that might be helpful:&#xD;&#xA;&#xD;&#xA; 1. Using [write blockers][1] so you don't damage data on accident&#xD;&#xA; 2. It's probably a good idea to generate a manifest (even just a full file listing) that  you hang onto. Something like [Bagger][2] would let you do that and generate [fixity information][3] (ie checksums). &#xD;&#xA;&#xD;&#xA;The idea would be to capture a copy of what you want to preserve in it's most pristine condition, document that, and then make decisions on what you actually want to keep. &#xD;&#xA;&#xD;&#xA;I think the most concise info I've seen on this is the technical section of the free OCLC report [You've Got to Walk Before You Can Run: First Steps for Managing Born-Digital Content Received on Physical Media (PDF)][4] &#xD;&#xA;&#xD;&#xA;More broadly, the [Open Archival Information System Reference model][5] suggests thinking about submission information packages (what you start with or are given), archival information packages (what you keep for the long haul) and dissemination information packages (what you are going to make available in a given context). What's useful about this framework in my mind is thinking of that organization process as getting the submission together and creating the archival copy. So you want to do the things mentioned at the top to make sure you get the submission right before you start thinking about turning it into the archival package you plan on keeping. So, documenting what you got and what you kept is an important part of that. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.forensicswiki.org/wiki/Write_Blockers&#xD;&#xA;  [2]: http://sourceforge.net/projects/loc-xferutils/files/loc-bagger/2.1.2/&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/File_Fixity&#xD;&#xA;  [4]: http://www.oclc.org/content/dam/research/publications/library/2012/2012-06.pdf&#xD;&#xA;  [5]: http://en.wikipedia.org/wiki/Open_Archival_Information_System" />
  <row Id="201" PostHistoryTypeId="2" PostId="91" RevisionGUID="2551fa99-a7ac-456f-81c9-c0932ab5d6bd" CreationDate="2013-02-28T03:47:02.747" UserId="76" Text="Given the probabilities of hash value collisions are MD5 hashes sufficient for ensuring file fixity? Or should SHA1 or SHA2 be used? Or, should folks be catching all three. I would be interested in both issues to consider for tamper resistance and for simply knowing if what I have is what I think I have. " />
  <row Id="202" PostHistoryTypeId="1" PostId="91" RevisionGUID="2551fa99-a7ac-456f-81c9-c0932ab5d6bd" CreationDate="2013-02-28T03:47:02.747" UserId="76" Text="Should Digital Preservationists be Worried about cartographic hash collisions?" />
  <row Id="203" PostHistoryTypeId="3" PostId="91" RevisionGUID="2551fa99-a7ac-456f-81c9-c0932ab5d6bd" CreationDate="2013-02-28T03:47:02.747" UserId="76" Text="&lt;file-fixity&gt;&lt;cryptographic-hash&gt;&lt;checksum&gt;&lt;authenticity&gt;" />
  <row Id="204" PostHistoryTypeId="2" PostId="92" RevisionGUID="a6094c53-f732-49cb-9b3f-80562f9a96b1" CreationDate="2013-02-28T03:56:20.953" UserId="76" Text="Libraries, Archives and Museums seem to have generally gone with creating stacks of XML files with basic metadata in them. See for example [MODS][1], [EAD][2], or [Dublin Core XML][3]. As far as I'm concerned, what really matters is that there is textual information on hand. So if that's in XML files: Great. If it's in EXIF tags, or ID3 tags: that's great too. Frankly, that embeded EXIF or ID3 tag information has some particularly nice benifits to it, in that it sticks with the file. At the end of the day, you can open a photo or a mp3 in a text editor and see a bunch of nonsense and those tags too so the information sticks with the files. (This post has an [example of seeing ID3 tags in a text editor][4]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.loc.gov/standards/mods/&#xD;&#xA;  [2]: http://www.loc.gov/ead/index.html&#xD;&#xA;  [3]: http://dublincore.org/schemas/xmls/&#xD;&#xA;  [4]: http://blogs.loc.gov/digitalpreservation/2012/11/glitching-files-for-understanding-avoiding-screen-essentialism-in-three-easy-steps/" />
  <row Id="205" PostHistoryTypeId="5" PostId="87" RevisionGUID="7074539c-2637-4549-a4de-fce849f6d593" CreationDate="2013-02-28T04:00:06.280" UserId="70" Comment="split to 2 questions" Text="There is a known problem with the Wayback Machine regarding domain name ownership - oftentimes, the domain changes hands, and the new owner either puts a spam blog on it, or puts a robots.txt which forbids crawling, and then WM promptly deletes the entire website history.&#xD;&#xA;&#xD;&#xA;Is it legal to crawl and store personal copies of websites to protect against this particular problem? or to build such an engine?" />
  <row Id="206" PostHistoryTypeId="2" PostId="93" RevisionGUID="21f8e9fb-f09e-476b-93a3-ba88fcdc5f79" CreationDate="2013-02-28T04:03:57.870" UserId="70" Text="There is a known problem with the Wayback Machine regarding domain name ownership - oftentimes, the domain changes hands, and the new owner either puts a spam blog on it, or puts a robots.txt which forbids crawling, and then WM promptly deletes the entire website history.&#xD;&#xA;&#xD;&#xA;So, are there any options to combat this problem? Archive crawlers which don't retroactively delete content, for example." />
  <row Id="207" PostHistoryTypeId="1" PostId="93" RevisionGUID="21f8e9fb-f09e-476b-93a3-ba88fcdc5f79" CreationDate="2013-02-28T04:03:57.870" UserId="70" Text="Coping with spam in web archivers" />
  <row Id="208" PostHistoryTypeId="3" PostId="93" RevisionGUID="21f8e9fb-f09e-476b-93a3-ba88fcdc5f79" CreationDate="2013-02-28T04:03:57.870" UserId="70" Text="&lt;web-archiving&gt;&lt;crawling&gt;&lt;digital-born&gt;&lt;data-curation&gt;" />
  <row Id="209" PostHistoryTypeId="2" PostId="94" RevisionGUID="58d9a26d-b1dc-421b-adfd-1fb387b53ccc" CreationDate="2013-02-28T04:15:22.857" UserId="70" Text="- If you only care about accidental data corruption: Any of those will do.&#xD;&#xA;&#xD;&#xA;- If there is a possibility of targeted attack: MD5 is broken, SHA1 weakened, SHA2 is doing very well. Note that SHA1 and SHA2 are completely different algorithms.&#xD;&#xA;&#xD;&#xA;- Also, if you're going to store LOTS of data (e.g. 2^70 files) then you need a hash algorithm which has at least 2X+20 bits due to birthday paradox.&#xD;&#xA;&#xD;&#xA;Overall, I would use 256-bit SHA2 for next 100+ years. &#xD;&#xA;&#xD;&#xA;For tamper resistance, using any two hash algorithms (even if one is broken) increases hack difficulty immensely. So calculating MD5 in addition to sha256 (because it is already widely used) should be more than enough." />
  <row Id="210" PostHistoryTypeId="2" PostId="95" RevisionGUID="6863a429-1879-40d4-870b-040059ec97f0" CreationDate="2013-02-28T04:19:23.913" UserId="29" Text="MD5 is [broken](http://www.mathstat.dal.ca/~selinger/md5collision/) and SHA-1 will be crackable within [five years](http://www.schneier.com/blog/archives/2012/10/when_will_we_se.html) (I especially like the speculation of using criminal organizations harnessing a botnet to crack SHA-1 cheaply.) Because archives help establish the historical records and have often be attacked/manipulated to alter that record, I don't doubt that digital archives will be the target of attacks to remove or alter that record as well.&#xD;&#xA;&#xD;&#xA;Checksums will continue to be cracked over time, so digital preservation systems will need a graceful method to calculate new checksums for stored objects and store them in their database alongside past checksums. Retaining hashes from a variety of algorithms will also complicate the task of crafting a collision file." />
  <row Id="211" PostHistoryTypeId="2" PostId="96" RevisionGUID="034d4c2f-e55e-4de1-a676-a4bafc67e1e0" CreationDate="2013-02-28T04:20:58.047" UserId="70" Text="Unfortunately, I had lots of difficulties using all of them.&#xD;&#xA;&#xD;&#xA;There could be 2 distinct goals:&#xD;&#xA;&#xD;&#xA;- preserve few sites as well as possible&#xD;&#xA;- preserve a lot of sites with &quot;best effort&quot; quality.&#xD;&#xA;&#xD;&#xA;For the first option, you'd better have an array of these tools, plus a couple of paid ones, and try all of them to select the one that provides the best quality on this particular website.&#xD;&#xA;&#xD;&#xA;For the second goal, I'd go with wget because it's easiest to script. Maybe also HTTrack, but wget is simpler and probably easier to build a crawling cluster from.&#xD;&#xA;" />
  <row Id="212" PostHistoryTypeId="5" PostId="93" RevisionGUID="d4c2eb8a-3d3f-45fd-96e0-7e843103bbd6" CreationDate="2013-02-28T04:28:39.370" UserId="70" Comment="clarified dupes" Text="There is a known problem with the Wayback Machine regarding domain name ownership - oftentimes, the domain changes hands, and the new owner either puts a spam blog on it, or puts a robots.txt which forbids crawling, and then WM promptly deletes the entire website history.&#xD;&#xA;&#xD;&#xA;So, are there any options to combat this problem? Archive crawlers which don't retroactively delete content, for example.&#xD;&#xA;&#xD;&#xA;**EDIT:** This question is an offspring from http://digitalpreservation.stackexchange.com/questions/87/preserving-website-content. This one is about &quot;how to deal with spam&quot;, and the other one is &quot;what websites we could/should preserve&quot;." />
  <row Id="213" PostHistoryTypeId="2" PostId="97" RevisionGUID="a320f752-1ec0-4ec7-9bb3-ba148fbcf04b" CreationDate="2013-02-28T04:51:17.630" UserId="29" Text="Modern file systems are basically databases of metadata with pointers to bitstreams. This is great for accessibility since the database can be queried quickly, but not great for preservation since the database can become disassociated from the bitstreams. In more complex preservation systems like [Safety Deposit Box/Preservica](http://www.digital-preservation.com/wp-content/uploads/SwissFederalArchives.pdf) (this is a white paper about their work with the Swiss Federal Archives) the system can store one copy of the metadata in a database for access and a second copy in a package with the digital object on the storage medium." />
  <row Id="214" PostHistoryTypeId="2" PostId="98" RevisionGUID="beb82326-ddce-41e9-82a5-3a71675b5649" CreationDate="2013-02-28T04:52:31.777" UserId="118" Text="I would suggest that a database would be the best place to store the metadata about the documents. This could be commercial off the shelf library/records software or home grown software. I imagine that trying to shoehorn metadata into a wiki would not be ideal.&#xD;&#xA;&#xD;&#xA;You should not rely on the metadata embedded into the documents (eg EXIF, document properties), because it is likely to be a subset of all the metadata about that document. You have already flagged the problems associated with trying to parse the embedded metadata for retreival purposes. Also, different document types have different capacity for storing tags - you would have to try to ensure that it was easy to identify all incarnations of the same data (DC.Title vs vs title vs subject, for example)&#xD;&#xA;&#xD;&#xA;No matter what database solution you use for your metadata you will need to determine your preservation strategy for that database (you mention the problem of the risk of obsolescence).&#xD;&#xA;&#xD;&#xA;Your strategy would probably be migrating to a newer platform when the time is right. Dumping the information to an XML file (or two) could be an intermediate step in the migration process. It would all depend on the volume of data that needs to be migrated.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="215" PostHistoryTypeId="4" PostId="91" RevisionGUID="c260ad46-f473-4331-bf20-9e573c1e6230" CreationDate="2013-02-28T05:13:38.910" UserId="70" Comment="fixed cartographic to cryptographic in title" Text="Should Digital Preservationists be Worried about cryptographic hash collisions?" />
  <row Id="216" PostHistoryTypeId="24" PostId="91" RevisionGUID="c260ad46-f473-4331-bf20-9e573c1e6230" CreationDate="2013-02-28T05:13:38.910" Comment="Proposed by 70 approved by 76 edit id of 8" />
  <row Id="219" PostHistoryTypeId="6" PostId="65" RevisionGUID="65ff78bb-df0d-46d6-bb39-04b821085207" CreationDate="2013-02-28T08:44:18.703" UserId="91" Comment="edited tags" Text="&lt;file-management&gt;&lt;validation&gt;&lt;bagit&gt;&lt;archive-format&gt;" />
  <row Id="221" PostHistoryTypeId="5" PostId="71" RevisionGUID="ff45cc22-077a-4dff-9d53-d0af403ee08c" CreationDate="2013-02-28T08:58:15.033" UserId="97" Comment="Define NNTP" Text="**I would not consider Usenet a viable archival strategy.** Not now nor in the past. Yes, it's true that storage of content is distributed and by design duplicated, but the control over such parameters as storage time is entirely in the hands of third parties who may have no interest in your particular content, and they are by no means *obliged* to provide you (or anyone else, really) with access to their archives.&#xD;&#xA;&#xD;&#xA;Not all of Usenet is archived. There are many hierarchies which are only available in certain regions or to users of certain ISPs (or Usenet server providers), and there are hierarchies which are generally excluded from archiving (`alt.binaries.*` comes to mind). Also, you'd probably be hard pressed to find a publicly available archive that goes back a significant amount of time. DejaNews, which got bought by Google, had a quite good archive, and I do believe that Google is allowing access to all of it, but last I heard it wasn't really searchable in a practical way.&#xD;&#xA;&#xD;&#xA;NNTP (Network News Transfer Protocol, the protocol that Usenet clients and servers use to talk to each other) is largely a best-effort protocol. Not all news posts are guaranteed to be on every server (though usually they do end up that way eventually), and they are not guaranteed to be delivered to each server in the order of their creation. Different posts may be delivered through entirely different servers. This is all a product of NNTP's initial design, where it had to run over such arcane transport protocols as UUCP as well as TCP/IP (and many others; &quot;everyone&quot; running TCP/IP is a relatively new invention, and now we are moving into a mixed protocol world again as we head into a combined IPv4/IPv6 Internet).&#xD;&#xA;&#xD;&#xA;I don't know exactly how the process would go for getting a new newsgroup into the major archives. That would probably depend on their configuration, which is a factor you don't control. You could contact the administrators and ask for your group to be included if it doesn't show up in their archives, but that's about it as far as control over what is archived goes. The posts to be archived would still have to make it to them, however and obviously, and they would almost certainly only archive content from the point in time of inclusion of the newsgroup into their archive. I don't see one Usenet archive(r/provider) going out of their way to track down copies of previous posts.&#xD;&#xA;&#xD;&#xA;Access for regular users is limited. While there are both free and fee-based Usenet servers available to the general public, and software is available for the taking, very few people these days are likely to install and configure a NNTP client. So while technically most anyone who has Internet access will be able to access Usenet if they put in the effort, few people put in the effort." />
  <row Id="223" PostHistoryTypeId="2" PostId="100" RevisionGUID="e6ed2ecb-af14-4870-8deb-4e3063ddd90a" CreationDate="2013-02-28T09:09:51.347" UserId="36" Text="Tools exist for adding redundancy in archiving, for example [DVDisaster][1], which fills your DVD's leftover space with recovery information, allowing more physical degradation than usual of the disk.&#xD;&#xA;&#xD;&#xA;Is this strategy worth the bother when archiving? One obvious alternative is to refresh your backup media more often, and use redundancy of actual physical copies, rather than redundancy per-disk.&#xD;&#xA;&#xD;&#xA;Another downside is the possibility that DVDisaster software becomes unmaintained and unusable.&#xD;&#xA;&#xD;&#xA;Supposing this per-medium redundancy strategy isn't a waste of time, what alternatives exist to DVDisaster?&#xD;&#xA;&#xD;&#xA;  [1]: http://dvdisaster.net/en/index.html" />
  <row Id="224" PostHistoryTypeId="1" PostId="100" RevisionGUID="e6ed2ecb-af14-4870-8deb-4e3063ddd90a" CreationDate="2013-02-28T09:09:51.347" UserId="36" Text="When does per-medium redundancy in archiving become over the top?" />
  <row Id="225" PostHistoryTypeId="3" PostId="100" RevisionGUID="e6ed2ecb-af14-4870-8deb-4e3063ddd90a" CreationDate="2013-02-28T09:09:51.347" UserId="36" Text="&lt;archive&gt;&lt;archive-format&gt;&lt;redundancy&gt;" />
  <row Id="226" PostHistoryTypeId="6" PostId="73" RevisionGUID="b1631e51-bdac-419b-9610-eb5b9c32336a" CreationDate="2013-02-28T10:59:23.117" UserId="54" Comment="edited tags" Text="&lt;future-proofing&gt;&lt;durabilty&gt;&lt;obsolescence&gt;" />
  <row Id="227" PostHistoryTypeId="2" PostId="101" RevisionGUID="20062f2e-53fa-488f-92fb-60609c906fc1" CreationDate="2013-02-28T11:08:25.970" UserId="119" Text="An important feature of any web archiving tool is the ability to specify that one would like to pull down embedded resources (&quot;page requisites&quot; in the parlance of wget) that are hosted on a domain other than the site that is the target of the crawl. Such cases are found not only as examples of people &quot;hot-linking&quot; other people's images, but is encountered heavily on sites with cloud based content delivery networks, such as Tumblr. Including such assets in a crawl is absolutely necessary when one's aim is to achieve a complete mirror of a site.&#xD;&#xA;&#xD;&#xA;**Heritrix** has an option for including such assets in the crawl scope: [https://webarchive.jira.com/wiki/display/Heritrix/unexpected+offsite+content][1]&#xD;&#xA;&#xD;&#xA;As does **Httrack**, using the --near flag:&lt;br&gt;&#xD;&#xA;[http://www.httrack.com/html/fcguide.html][2]&lt;br&gt;&#xD;&#xA;But of course, Httrack does not offer WARC output.&#xD;&#xA;&#xD;&#xA;**Wget** has the -H flag, allowing one to &quot;span hosts&quot; (in other words hit sites with domain names other than the starting url), it lacks the ability to specify that the crawler should span hosts only for page requisites, and so tries to download the entire web if one combines an infinitely recursive crawl with -H. There are some hacky ways of getting around this, but they aren't pretty or reliable. The great thing about Wget though is that it allows the user to output WARC &lt;i&gt;and&lt;/i&gt; a directory tree of the crawled site, thus not locking one in to WARC completely.&#xD;&#xA;&#xD;&#xA;Are there any tools of the trade that allow one to conduct the comprehensive type of crawl mentioned above, but also have the affordance of outputting WARC, *and* a directory tree?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://webarchive.jira.com/wiki/display/Heritrix/unexpected+offsite+content&#xD;&#xA;  [2]: http://www.httrack.com/html/fcguide.html" />
  <row Id="228" PostHistoryTypeId="1" PostId="101" RevisionGUID="20062f2e-53fa-488f-92fb-60609c906fc1" CreationDate="2013-02-28T11:08:25.970" UserId="119" Text="web archiving tools that produce WARC + directory tree" />
  <row Id="229" PostHistoryTypeId="3" PostId="101" RevisionGUID="20062f2e-53fa-488f-92fb-60609c906fc1" CreationDate="2013-02-28T11:08:25.970" UserId="119" Text="&lt;web-archiving&gt;&lt;wget&gt;&lt;heretrix&gt;&lt;warc&gt;&lt;httrack&gt;" />
  <row Id="230" PostHistoryTypeId="2" PostId="102" RevisionGUID="370ee7c6-135b-4458-9594-d3fb37b3d10c" CreationDate="2013-02-28T11:50:50.333" UserId="31" Text="I'd say you have two options. You could crawl using Heritrix and generate the WARC files, but then pop the contents of those files out into a directory tree. You could do this via one of these two warctozip scripts [1], [2] (disclaimer: I wrote the latter one and it's just a rough prototype and should probably be merged with the former).&#xD;&#xA;&#xD;&#xA;The other option is to use an existing tool to enumerate all the URLs and then pass that list to wget. This is what my [flashfreeze tool][3] does, using a PhantomJS script to determine all the dependencies of a specific page, and then using wget to grab them as a WARC file.&#xD;&#xA;&#xD;&#xA;[1]: https://github.com/alard/warctozip/blob/master/warctozip.py&#xD;&#xA;[2]: https://github.com/ukwa/warc/blob/master/warc/warctozip.py&#xD;&#xA;[3]: https://github.com/ukwa/flashfreeze&#xD;&#xA;" />
  <row Id="231" PostHistoryTypeId="4" PostId="16" RevisionGUID="bdeeae68-46c4-42d5-9cbb-c3ef236913b9" CreationDate="2013-02-28T11:50:59.020" UserId="91" Comment="added “digital” to the question because this is not about your usual analog VHS audio track" Text="How can I recover digital audio that was encoded on VHS cassettes around 1987?" />
  <row Id="232" PostHistoryTypeId="6" PostId="16" RevisionGUID="bdeeae68-46c4-42d5-9cbb-c3ef236913b9" CreationDate="2013-02-28T11:50:59.020" UserId="91" Comment="added “digital” to the question because this is not about your usual analog VHS audio track" Text="&lt;out-of-date-format&gt;&lt;data-recovery&gt;&lt;vhs&gt;&lt;pcm&gt;&lt;audio&gt;" />
  <row Id="233" PostHistoryTypeId="24" PostId="16" RevisionGUID="bdeeae68-46c4-42d5-9cbb-c3ef236913b9" CreationDate="2013-02-28T11:50:59.020" Comment="Proposed by 91 approved by 64 edit id of 9" />
  <row Id="234" PostHistoryTypeId="2" PostId="103" RevisionGUID="f2240ba7-f618-4f56-9f88-ee19f23166a5" CreationDate="2013-02-28T12:08:40.663" UserId="31" Text="Whether you as a individual have the right to take a copy of a website, and for what purposes, and whether you can redistribute it, will be down to you local copyright laws. I expect fair use/fail dealing exceptions mean you can copy but not distribute, but YMMV.&#xD;&#xA;&#xD;&#xA;Another option is to be aware that there is more than just one web archive, each with different policies, so you could check for other holdings or nominate the site for archival elsewhere." />
  <row Id="235" PostHistoryTypeId="2" PostId="104" RevisionGUID="8b243dc4-867f-4fb2-b1de-63aa22b59aa5" CreationDate="2013-02-28T12:20:22.170" UserId="31" Text="The best alternative I'm aware of are the parity archive [parchive](http://en.m.wikipedia.org/wiki/Parchive) formats, which let you do the same thing but in a media independent fashion and using open source tools.&#xD;&#xA;&#xD;&#xA;However, your time might be better spent simply burning a second DVD or putting a copy on another medium. Data disks like CDROMs and DVDs already use low-level error detection and correction codes, making bit loss relatively unlikely (I suspect) compared to whole disk failure (cracks, breaks, scratches, ink failure, etc.)" />
  <row Id="239" PostHistoryTypeId="2" PostId="106" RevisionGUID="918c2912-1307-42be-bc6d-1138b406e31e" CreationDate="2013-02-28T14:28:11.420" UserId="94" Text="As you suggest, checksums are definitely going to play a useful role in meeting this challenge. [This Stack Q&amp;A][1] has some initial suggestions on approach and tools, although it may need a slightly different approach for your specific needs here.&#xD;&#xA;&#xD;&#xA;If the data changes as part of the organisation process, checksums suddenly become a lot less useful. [This is a very simple but effective tool for matching source and destination filenames][2], and may be of some help here.&#xD;&#xA;&#xD;&#xA;There are many possible approaches for dealing with many different flavours of de-duplication. [These are some descriptions of duplication challenges and some experiences of solving them][3], which might be of use.&#xD;&#xA;&#xD;&#xA;As things stand I think there is still a need for a more comprehensive organisation or curatorial tool to tackle this challenge more effectively. It needs to enable all the potentially catastrophic changes (such as delete, and rename) while allowing undo of these changes, and capturing a change log or detailed provenance metadata. From my [experiences of working with digital preservation practitioners from Libraries and Archives][4], this is a pretty common use case.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://digitalpreservation.stackexchange.com/questions/65/most-efficient-way-to-generate-and-validate-file-checksums&#xD;&#xA;  [2]: http://openplanetsfoundation.org/blogs/2012-12-05-resource-audit-and-comparison-tool-react&#xD;&#xA;  [3]: http://wiki.opf-labs.org/display/REQ/Digital+Preservation+and+Data+Curation+Requirements+and+Solutions#DigitalPreservationandDataCurationRequirementsandSolutions-Duplicationissues&#xD;&#xA;  [4]: http://wiki.opf-labs.org/display/REQ/Digital+Preservation+and+Data+Curation+Requirements+and+Solutions" />
  <row Id="240" PostHistoryTypeId="5" PostId="56" RevisionGUID="c6d6945a-0407-4f72-99cb-95fd939d06b5" CreationDate="2013-02-28T14:57:35.973" UserId="54" Comment="list format, retag per [tag standards](http://meta.stackoverflow.com/q/23869/183032) until British English synonyms are added" Text="What measures, technologies, or techniques are applicable to guarding information against damage/loss during organisation?&#xD;&#xA;&#xD;&#xA;Often when I'm archiving data, before the archiving process I also need to de-duplicate and organise the data. I'm aware that this is a risky time for the data and that human, software or mechanical error might lead to some data being corrupt or lost, without me even knowing about it. &#xD;&#xA;&#xD;&#xA;So there are two questions:&#xD;&#xA;&#xD;&#xA; 1. How can I keep an eye on the data so that I can audit where it was versus where it is now, and see if any data has been deleted or changed during organisation?&#xD;&#xA;&#xD;&#xA;* An obvious strategy is to have some sort of checksumming going on&#xD;&#xA;&#xD;&#xA; 2.  How can I reduce human error while organising the data? &#xD;&#xA;&#xD;&#xA;* Example: you might have a filesystem that simply refuses to let you delete a file if it is unique. Or something like [ZFS][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/ZFS" />
  <row Id="241" PostHistoryTypeId="6" PostId="56" RevisionGUID="c6d6945a-0407-4f72-99cb-95fd939d06b5" CreationDate="2013-02-28T14:57:35.973" UserId="54" Comment="list format, retag per [tag standards](http://meta.stackoverflow.com/q/23869/183032) until British English synonyms are added" Text="&lt;deduplication&gt;&lt;corruption&gt;&lt;organization&gt;" />
  <row Id="242" PostHistoryTypeId="24" PostId="56" RevisionGUID="c6d6945a-0407-4f72-99cb-95fd939d06b5" CreationDate="2013-02-28T14:57:35.973" Comment="Proposed by 54 approved by 101 edit id of 10" />
  <row Id="243" PostHistoryTypeId="4" PostId="56" RevisionGUID="49ea0ebe-d753-4123-ae2e-bfb1edfc540c" CreationDate="2013-02-28T15:08:28.227" UserId="36" Comment="edited title" Text="Guarding data against corruption or loss during file organisation/curation" />
  <row Id="247" PostHistoryTypeId="2" PostId="107" RevisionGUID="058a39b4-d98a-49cf-b78b-d53d8c683469" CreationDate="2013-02-28T15:57:36.677" UserId="42" Text="I think that your best bet is to do a web search for an independent audio consultant. It would have to be someone who has been in business since 1987 and has worked with that technology. Such a person would either still have the appropriate equipment or would know where to get it." />
  <row Id="248" PostHistoryTypeId="2" PostId="108" RevisionGUID="d1d3bf81-127c-48e2-a037-5dbce68e3e69" CreationDate="2013-02-28T16:08:02.317" UserId="32" Text="In the medium-sized archives where I used to work, we used web httrack for a couple reasons:&#xD;&#xA;&#xD;&#xA;1- it was free, and since web harvesting for preservation was not a regular part of our acquisition policy it didn't make sense to pay for any service&#xD;&#xA;&#xD;&#xA;2- it was very simple to run in the browser and the parameters/settings were well-documented online&#xD;&#xA;&#xD;&#xA;We did run into problems using it, though. We could only crawl the site we were capturing every 2 days because the process took so long (I assume because of the size of the site and the limitations of our processors, which at that stage were limited to a test machine). When the crawl finished, sometimes the CSS had changed during the course of the processing, so we had to go back and manually update it for some parts of the site -- a very tedious process." />
  <row Id="249" PostHistoryTypeId="2" PostId="109" RevisionGUID="e14df557-9fe1-4256-80c7-2cc7987e3ee9" CreationDate="2013-02-28T16:13:39.067" UserId="32" Text="Organizations struggle with the question of &quot;where do I start?&quot; when it comes to setting up a workstation for acquiring content from digital media. Assuming something like a [FRED][1] is out of a small- to medium-sized institution's price range, what hardware would make up the basic workstation. Keep in mind that most organizations will have to deal with various sized floppies, zip drives, hard drives at a minimum.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.digitalintelligence.com/products/fred/" />
  <row Id="250" PostHistoryTypeId="1" PostId="109" RevisionGUID="e14df557-9fe1-4256-80c7-2cc7987e3ee9" CreationDate="2013-02-28T16:13:39.067" UserId="32" Text="What hardware would make up a good starter set for digital media acquisitions?" />
  <row Id="251" PostHistoryTypeId="3" PostId="109" RevisionGUID="e14df557-9fe1-4256-80c7-2cc7987e3ee9" CreationDate="2013-02-28T16:13:39.067" UserId="32" Text="&lt;media&gt;&lt;acquisition&gt;&lt;hardware&gt;" />
  <row Id="252" PostHistoryTypeId="4" PostId="87" RevisionGUID="beb72f7a-55de-481d-b832-a85d2f87d3fc" CreationDate="2013-02-28T16:16:26.217" UserId="70" Comment="edited title" Text="When is it legal/ethical to preserve website content?" />
  <row Id="253" PostHistoryTypeId="2" PostId="110" RevisionGUID="19f26b2d-2539-4d80-9de0-910033ef2549" CreationDate="2013-02-28T16:47:18.713" UserId="70" Text="There are currently a number of de-facto standard formats, such as KML, GPX, TRK, LTB and so on. However, none of them are clearly better over one another. Which one will last?&#xD;&#xA;&#xD;&#xA;(Currently we are considering using GPX with custom extensions)" />
  <row Id="254" PostHistoryTypeId="1" PostId="110" RevisionGUID="19f26b2d-2539-4d80-9de0-910033ef2549" CreationDate="2013-02-28T16:47:18.713" UserId="70" Text="What format should be used when archiving GPS track data?" />
  <row Id="255" PostHistoryTypeId="3" PostId="110" RevisionGUID="19f26b2d-2539-4d80-9de0-910033ef2549" CreationDate="2013-02-28T16:47:18.713" UserId="70" Text="&lt;file-formats&gt;&lt;future-proofing&gt;&lt;gis&gt;&lt;gps&gt;" />
  <row Id="256" PostHistoryTypeId="2" PostId="111" RevisionGUID="069712c4-b3b9-4729-83d9-ac14d4df15ca" CreationDate="2013-02-28T16:57:04.277" UserId="70" Text="Since the Library of Congress is archiving tweets, and the practice of archiving IRC chatroom logs is common, well, why not.&#xD;&#xA;&#xD;&#xA;The storage is not a problem - any DB or even CSV file should do.&#xD;&#xA;&#xD;&#xA;But I've not seen a good tools to make these DBs nicely searchable and accessible for non-tech people (everybody out there knows SQL? I think no).&#xD;&#xA;&#xD;&#xA;So, **what software could be used for providing useful access to such archives?** Would be great if it could also provide anonymized stats for researchers (i.e. replacing nicknames with numbers etc)" />
  <row Id="257" PostHistoryTypeId="1" PostId="111" RevisionGUID="069712c4-b3b9-4729-83d9-ac14d4df15ca" CreationDate="2013-02-28T16:57:04.277" UserId="70" Text="Tool suites for archiving instant messaging history" />
  <row Id="258" PostHistoryTypeId="3" PostId="111" RevisionGUID="069712c4-b3b9-4729-83d9-ac14d4df15ca" CreationDate="2013-02-28T16:57:04.277" UserId="70" Text="&lt;software&gt;&lt;digital-born&gt;&lt;accessibility&gt;&lt;presentation&gt;" />
  <row Id="259" PostHistoryTypeId="2" PostId="112" RevisionGUID="043ce024-84e4-42e6-a077-024e8243cb50" CreationDate="2013-02-28T17:06:28.353" UserId="127" Text="I have some old Amstrad CF2 disks that once belonged to my grandfather with all his research on his family history and several war diaries that he was writing with a view to publishing one day.&#xD;&#xA;&#xD;&#xA;The Amstrad machine that he used to write the files on has long since found its way onto a scrap heap some where.&#xD;&#xA;&#xD;&#xA;I was wondering if there is any method by which I can extract the data contained on the disks and transfer, convert and store on either a windows or linux based machine. Both of which I have access to. " />
  <row Id="260" PostHistoryTypeId="1" PostId="112" RevisionGUID="043ce024-84e4-42e6-a077-024e8243cb50" CreationDate="2013-02-28T17:06:28.353" UserId="127" Text="How can I extract data from an Amstrad CF2 disk to either a windows or linux based PC" />
  <row Id="261" PostHistoryTypeId="3" PostId="112" RevisionGUID="043ce024-84e4-42e6-a077-024e8243cb50" CreationDate="2013-02-28T17:06:28.353" UserId="127" Text="&lt;out-of-date-format&gt;&lt;data-recovery&gt;&lt;vintage-computing&gt;&lt;genealogy&gt;" />
  <row Id="262" PostHistoryTypeId="2" PostId="113" RevisionGUID="ebc29c99-7caa-45df-b605-7c310a03876c" CreationDate="2013-02-28T17:09:05.003" UserId="70" Text="Everybody here is talking about storage failures in one way or another.&#xD;&#xA;&#xD;&#xA;What these discussions miss is **solid, objective data** on media failure rates.&#xD;&#xA;&#xD;&#xA;I'll start with the [Failure Trends in a Large Disk Drive Population][1] article from Google, and [Disk failures in the real world][2] from CMU. See also the less scientific [Is Your SSD More Reliable Than A Hard Drive?][3] article for Tom's hardware.&#xD;&#xA;&#xD;&#xA;Please contribute! Links to studies on tape and DVD/Bluray media are especially welcome.&#xD;&#xA;&#xD;&#xA;See also: http://digitalpreservation.stackexchange.com/questions/5/comparative-lifetimes-of-digital-media&#xD;&#xA;&#xD;&#xA;  [1]: http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/disk_failures.pdf&#xD;&#xA;  [2]: http://static.usenix.org/events/fast07/tech/schroeder/schroeder.pdf&#xD;&#xA;  [3]: http://www.tomshardware.com/reviews/ssd-reliability-failure-rate,2923.html" />
  <row Id="263" PostHistoryTypeId="1" PostId="113" RevisionGUID="ebc29c99-7caa-45df-b605-7c310a03876c" CreationDate="2013-02-28T17:09:05.003" UserId="70" Text="Studies on media failure rates" />
  <row Id="264" PostHistoryTypeId="3" PostId="113" RevisionGUID="ebc29c99-7caa-45df-b605-7c310a03876c" CreationDate="2013-02-28T17:09:05.003" UserId="70" Text="&lt;storage&gt;&lt;lifetime&gt;&lt;storage-media&gt;&lt;reliability&gt;&lt;mttf&gt;" />
  <row Id="265" PostHistoryTypeId="10" PostId="73" RevisionGUID="52cc5b58-ecfd-46e9-ba9f-83b4cb7699b2" CreationDate="2013-02-28T17:15:56.607" UserId="-1" CloseReasonId="1" Text="{&quot;OriginalQuestionIds&quot;:[4],&quot;Voters&quot;:[{&quot;Id&quot;:30,&quot;DisplayName&quot;:&quot;warren&quot;},{&quot;Id&quot;:118,&quot;DisplayName&quot;:&quot;John Lovejoy&quot;},{&quot;Id&quot;:54,&quot;DisplayName&quot;:&quot;sean&quot;},{&quot;Id&quot;:43,&quot;DisplayName&quot;:&quot;jcmeloni&quot;},{&quot;Id&quot;:70,&quot;DisplayName&quot;:&quot;wizzard0&quot;}]}" />
  <row Id="266" PostHistoryTypeId="2" PostId="114" RevisionGUID="68d0be09-065d-4d5e-bbce-cbb424c0ad44" CreationDate="2013-02-28T17:59:16.510" UserId="14" Text="There's a distinction between preservation and retention, which Trevor alludes to but should be made more explicit. Tax records are an example of retention; they're kept for a finite time for a specific need, and after that they have no value or may even be problematic to keep because of privacy issues. Records of this type normally aren't made public. Things like customer records and maintenance logs fall into the category of retention. Information which is appropriate for public disclosure falls into the &quot;preservation&quot; category, and in general there's no reason to delete it unless you're facing a resource crunch." />
  <row Id="267" PostHistoryTypeId="2" PostId="115" RevisionGUID="a448f15d-9529-4a04-b31b-a860dfe2f867" CreationDate="2013-02-28T18:46:12.353" UserId="26" Text="One way or another, you will need access to an Amstrad 3&quot; floppy drive. You may be able to find one on Ebay, or ask friends or colleagues if they have a similar old Amstrad system.&#xD;&#xA;&#xD;&#xA;If you do obtain an Amstrad floppy drive, you will need to build an adapter to bridge it to a PC floppy connector on your motherboard. Instructions are here: http://www.fvempel.nl/3pc.html&#xD;&#xA;&#xD;&#xA;(there was also a [Slashdot][1] discussion about this)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://ask.slashdot.org/story/08/05/08/1523257/retrieving-data-from-old-amstrad-floppies" />
  <row Id="268" PostHistoryTypeId="2" PostId="116" RevisionGUID="be8c8cb8-7611-4950-86c6-91ac6a487eaf" CreationDate="2013-02-28T19:04:29.197" UserId="131" Text="Is there still an economic argument for preserving born-digital video at less than 4:4:4?&#xD;&#xA;&#xD;&#xA;Jerome McDonough 2004 argues for 4:4:4 (chroma subsampling) or nothing, although he points out that this is sometimes dangerously expensive.&#xD;&#xA;&#xD;&#xA;Since disk storage costs have gone down (and up) since then, what's the rationale at this point, for keeping anything less than a &quot;pristine&quot; set of workfiles? &#xD;&#xA;&#xD;&#xA;For clarity, I'm referring to what George Blood 2011 describes as &quot;Digital Source, media independent 'file based'&quot;, and not things that are recovered from tape or such." />
  <row Id="269" PostHistoryTypeId="1" PostId="116" RevisionGUID="be8c8cb8-7611-4950-86c6-91ac6a487eaf" CreationDate="2013-02-28T19:04:29.197" UserId="131" Text="Is there still an economic argument for preserving born-digital video at less than 4:4:4?" />
  <row Id="270" PostHistoryTypeId="3" PostId="116" RevisionGUID="be8c8cb8-7611-4950-86c6-91ac6a487eaf" CreationDate="2013-02-28T19:04:29.197" UserId="131" Text="&lt;file-formats&gt;&lt;born-digital&gt;&lt;video&gt;&lt;economics&gt;&lt;satisficing&gt;" />
  <row Id="271" PostHistoryTypeId="2" PostId="117" RevisionGUID="a9597842-604a-4c12-9174-8646ef5d5e24" CreationDate="2013-02-28T19:09:55.390" UserId="17" Text="I'm from the US, so I'm not familiar with Amstrads in particular, but most of what I wrote [here][1] is applicable to any 1980s floppy-based personal computer.&#xD;&#xA;&#xD;&#xA;Once you've acquired and connected the floppy drive, you can use a disk imaging program (make sure you use one that specializes in 3&quot; Amstrad disks) to make disk images that you can mount on an [emulator][2]. That way you can view the disk's contents even if they can't easily be converted to a modern format. &#xD;&#xA;&#xD;&#xA;If you can't find a drive, it looks like there are a few vendors in the UK who specialize in extracting data from old Amstrad disks -- I saw a few in the first page of Google results for &quot;Amstrad CF2.&quot; You might also look for Amstrad hobbyist forums and see if any of them can vouch for a particular vendor. &#xD;&#xA;&#xD;&#xA;If you go to a vendor, I recommend asking if they would be willing to give you bit-by-bit disk images in addition to converting the files to a modern format. That way you can be sure that you aren't losing any significant properties or metadata during the conversion.&#xD;&#xA;&#xD;&#xA;  [1]: http://digitalpreservation.stackexchange.com/questions/19/extracting-data-from-and-preserving-3-5in-floppy-disks-used-in-a-macintosh-se-1&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/List_of_computer_system_emulators#Amstrad_CPC" />
  <row Id="272" PostHistoryTypeId="5" PostId="16" RevisionGUID="5859d80f-be8e-4896-9339-92147d3e5482" CreationDate="2013-02-28T19:35:07.587" UserId="64" Comment="added 53 characters in body" Text="I have some live music recorded in 1971 on reel-to-reel tapes that I transferred to a digital format around 1987.  I no longer have access to the reel-to-reel tapes.  At the time, there was no direct PCM recording option on VHS machines (it came out soon thereafter).  The recording studio had a large box that took analog input and converted it to digital and stored the digital signal on the video section of VHS cassettes as a video signal that encoded the audio.&#xD;&#xA;&#xD;&#xA;I do not recall the encoder but I have the VHS tapes and would like to recover the audio.&#xD;&#xA;&#xD;&#xA;How could I go about recovering the audio that is encoded on these VHS tapes?  What might have been the machine encoding scheme used and if identified, how could this be decoded today?" />
  <row Id="273" PostHistoryTypeId="2" PostId="118" RevisionGUID="09972b09-4ff4-4e09-9313-70255198a585" CreationDate="2013-02-28T19:36:43.640" UserId="70" Text="I may be wrong, but aren't modern lossless frame compression algorithms (such as JPEG2000) providing better compression ratios than chroma subsampling, without any loss of quality?&#xD;&#xA;&#xD;&#xA;Sure you can compress chroma plane with less bitrate, that will still preserve a lot more data than &quot;dumb&quot; subsampling, and possibly boil down to smaller compressed size than subsampling, too.&#xD;&#xA;&#xD;&#xA;The compression speed might be an issue, of course.&#xD;&#xA;" />
  <row Id="274" PostHistoryTypeId="2" PostId="119" RevisionGUID="a124e13a-f23c-4156-8ad5-44cb2ec51909" CreationDate="2013-02-28T19:56:29.900" UserId="31" Text="This system already exists - it's called the Internet! The popular, widely used stuff gets copied, shared and re-shared. It won't get lost, but then it will also be difficult to tell if it is authentic.&#xD;&#xA;&#xD;&#xA;The main problem, however, is that only keeping the popular or frequently used things may be too simplistic as a way of judging the value of content. Is a resource most people use rarely more or less valuable than a resource some people use daily? &#xD;&#xA;&#xD;&#xA;Perhaps the popular, mainstream stuff will largely look after itself. Maybe we should worry more about making sure the smaller, quieter voices are not lost in the noise." />
  <row Id="275" PostHistoryTypeId="2" PostId="120" RevisionGUID="f3891b8b-bf7e-42d4-8e0b-cbc21ca92113" CreationDate="2013-02-28T20:03:24.307" UserId="31" Text="It may well be worth looking at [WAIL](http://matkelly.com/wail/), which attempts to bundle up Heritix and Wayback to make then easier to use. Making the 'big' tools like Heritrix more usable by more people is a great idea." />
  <row Id="280" PostHistoryTypeId="5" PostId="103" RevisionGUID="d635c34f-2c1c-45d4-a317-9b1a5490e546" CreationDate="2013-02-28T22:01:36.810" UserId="31" Comment="Added update to reflect addition of ethics to the title." Text="Whether you as a individual have the right to take a copy of a website, and for what purposes, and whether you can redistribute it, will be down to you local copyright laws. I expect fair use/fail dealing exceptions mean you can copy but not distribute, but YMMV.&#xD;&#xA;&#xD;&#xA;Another option is to be aware that there is more than just one web archive, each with different policies, so you could check for other holdings or nominate the site for archival elsewhere.&#xD;&#xA;&#xD;&#xA;**EDIT**&#xD;&#xA;&#xD;&#xA;I notice you included the ethics issue when you updated the title. This a very good point, and one that's not really been explored in detail as far as I know. Certainly, personally speaking, I'm not sure large national web archives should be archiving Facebook as a matter of course (for example), any more than we should be archiving everyone's email, or digitising everybody's letters. Such personal information should be 'opt in' only, IMO." />
  <row Id="281" PostHistoryTypeId="5" PostId="103" RevisionGUID="feac073b-0a9a-4a7b-8e44-5ceac9ce3931" CreationDate="2013-02-28T22:13:02.893" UserId="31" Comment="Added link to Search Engines and Ethics." Text="Whether you as a individual have the right to take a copy of a website, and for what purposes, and whether you can redistribute it, will be down to you local copyright laws. I expect fair use/fail dealing exceptions mean you can copy but not distribute, but YMMV.&#xD;&#xA;&#xD;&#xA;Another option is to be aware that there is more than just one web archive, each with different policies, so you could check for other holdings or nominate the site for archival elsewhere.&#xD;&#xA;&#xD;&#xA;**EDIT**&#xD;&#xA;&#xD;&#xA;I notice you included the ethics issue when you updated the title. This a very good point, and one that's not really been explored in detail as far as I know (although this work on [Search Engines and Ethics][1] might be a good starting point). Certainly, personally speaking, I'm not sure large national web archives should be archiving Facebook as a matter of course (for example), any more than we should be archiving everyone's email, or digitising everybody's letters. Such personal information should be 'opt in' only, IMO.&#xD;&#xA;&#xD;&#xA;[1]: http://plato.stanford.edu/entries/ethics-search/" />
  <row Id="282" PostHistoryTypeId="4" PostId="76" RevisionGUID="af454285-bb68-455f-9797-8fcc16565559" CreationDate="2013-02-28T22:26:38.617" UserId="97" Comment="Better title" Text="When should preserved data be allowed to die?" />
  <row Id="283" PostHistoryTypeId="24" PostId="76" RevisionGUID="af454285-bb68-455f-9797-8fcc16565559" CreationDate="2013-02-28T22:26:38.617" Comment="Proposed by 97 approved by 101 edit id of 12" />
  <row Id="284" PostHistoryTypeId="5" PostId="89" RevisionGUID="1086665e-eb76-49c0-ad3b-02f477c8a480" CreationDate="2013-02-28T22:26:41.143" UserId="97" Comment="Fixed grammar" Text="The notion of [Archival appraisal](http://en.wikipedia.org/wiki/Archival_appraisal) is relevant here. Organizations need to make decisions about the long term value of materials. Some materials have permanent value, some are set to specific retention schedules. In any case, it is a matter of an organization identifying why something is important and what uses it can serve in the future and then deciding how to manage content toward those ends.&#xD;&#xA;&#xD;&#xA;As a side note, all data does **not** lose its value. There are a lot of things that weren't thought of as valuable at all but were kept around as a result of benign neglect (for example [Martha Ballard's Diary](http://dohistory.org/diary/about.html)). There are also things like antiquarian books, or early drafts of scientific papers, that have enormous historical value. The point here is that value is always dependent on some intended use. So the question is, how is the data useful to you and your organization and who else might it be useful to in the future." />
  <row Id="285" PostHistoryTypeId="24" PostId="89" RevisionGUID="1086665e-eb76-49c0-ad3b-02f477c8a480" CreationDate="2013-02-28T22:26:41.143" Comment="Proposed by 97 approved by 101 edit id of 11" />
  <row Id="286" PostHistoryTypeId="2" PostId="123" RevisionGUID="b6e10e42-e3da-4cd3-8f3a-c841e1ba49c2" CreationDate="2013-02-28T23:08:33.983" UserId="54" Text="[*Instant Artist*][1] is an abandonware layout and publishing platform from the early 90s by Pixellite Group and Autodesk.&#xD;&#xA;&#xD;&#xA;Its file formats include `.sgn` (signs), `.ban` (banners), `.gc` (greeting cards), `.bc` (business cards), `.lth` (letterheads), etc. They don't appear to open in contemporary software. I'd rather not emulate if I can avoid it. What are these files' possible file format migration paths, or are they proprietary and unable to be converted?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Sierra_Print_Artist" />
  <row Id="287" PostHistoryTypeId="1" PostId="123" RevisionGUID="b6e10e42-e3da-4cd3-8f3a-c841e1ba49c2" CreationDate="2013-02-28T23:08:33.983" UserId="54" Text="File format migration paths for Instant Artist files" />
  <row Id="288" PostHistoryTypeId="3" PostId="123" RevisionGUID="b6e10e42-e3da-4cd3-8f3a-c841e1ba49c2" CreationDate="2013-02-28T23:08:33.983" UserId="54" Text="&lt;file-format-migration&gt;&lt;data-conversion&gt;" />
  <row Id="289" PostHistoryTypeId="10" PostId="113" RevisionGUID="8d556afa-c394-4a7c-9eeb-0e6bddd19d6c" CreationDate="2013-03-01T00:35:56.387" UserId="101" CloseReasonId="4" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:101,&quot;DisplayName&quot;:&quot;Robert Cartaino&quot;}]}" />
  <row Id="290" PostHistoryTypeId="10" PostId="5" RevisionGUID="f8917ed0-6f68-41ff-ad6c-5a1acf6f19f3" CreationDate="2013-03-01T00:38:05.700" UserId="101" CloseReasonId="4" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:101,&quot;DisplayName&quot;:&quot;Robert Cartaino&quot;}]}" />
  <row Id="292" PostHistoryTypeId="5" PostId="31" RevisionGUID="4b27b934-c62b-48b5-9243-aeb75ba14580" CreationDate="2013-03-01T01:37:10.437" UserId="62" Comment="deleted usenet advice." Text="Perhaps the best way at present is to arrange to have it snagged by the [Wayback Machine](http://archive.org/web/web.php) (or another archive) from your homepage at your current institution. That way you'll have a more or less permanent URL, as long as the Internet Archive lasts.&#xD;&#xA;&#xD;&#xA;&lt;del&gt;If it can be formatted as straight text, one or another usenet group would likely be thrilled to see it (and automatically archive it in numerous usenet archives throughout the world). I've been doing this with little programs and ideas, mainly so I can find my own work again after my computer inevitably dies. :)&lt;/del&gt; According to [this answer](http://digitalpreservation.stackexchange.com/a/71/62), I'm full of crap here.&#xD;&#xA;&#xD;&#xA;I was about to say Google Docs, but I see it's now turned into Google Drive, and that strikes me as a little unstable." />
  <row Id="293" PostHistoryTypeId="2" PostId="125" RevisionGUID="e380e5b3-e71d-4c31-8d48-bcdd77a84f4f" CreationDate="2013-03-01T01:55:34.777" UserId="102" Text="These days, the Internet is becoming more and more dominated by Ajax, and they're a pain for archivers to preserve.&#xD;&#xA;&#xD;&#xA;E.g. answer pages on quora.com and the &quot;expand more&quot; button on Facebook. There are scripts that one can write that can click the &quot;expand more&quot; button multiple times, but when there are 1000 &quot;expand more&quot; buttons, it gets to the point where Firefox/Chrome simply can't handle all the RAM anymore." />
  <row Id="294" PostHistoryTypeId="1" PostId="125" RevisionGUID="e380e5b3-e71d-4c31-8d48-bcdd77a84f4f" CreationDate="2013-03-01T01:55:34.777" UserId="102" Text="How can I mass-archive Ajax-heavy websites that archive.org can't preserve?" />
  <row Id="295" PostHistoryTypeId="3" PostId="125" RevisionGUID="e380e5b3-e71d-4c31-8d48-bcdd77a84f4f" CreationDate="2013-03-01T01:55:34.777" UserId="102" Text="&lt;web-archiving&gt;" />
  <row Id="296" PostHistoryTypeId="2" PostId="126" RevisionGUID="3623a405-1309-4e84-9550-c82c76082280" CreationDate="2013-03-01T01:57:31.200" UserId="102" Text="It used to be do-able with DownThemAll (back before Facebook went all crazy on Ajax). Now it's impossible." />
  <row Id="297" PostHistoryTypeId="1" PostId="126" RevisionGUID="3623a405-1309-4e84-9550-c82c76082280" CreationDate="2013-03-01T01:57:31.200" UserId="102" Text="How can I archive all the Facebook discussions on a Facebook group page?" />
  <row Id="298" PostHistoryTypeId="3" PostId="126" RevisionGUID="3623a405-1309-4e84-9550-c82c76082280" CreationDate="2013-03-01T01:57:31.200" UserId="102" Text="&lt;facebook&gt;" />
  <row Id="299" PostHistoryTypeId="2" PostId="127" RevisionGUID="68f440a2-ac8d-4132-bb43-0500b4e6765e" CreationDate="2013-03-01T01:59:15.647" UserId="102" Text="I use ClipCache right now and it has a &gt;1 GB file, but it does have some flaws (Snipping Tool and Excel captures take up an unreasonably huge amount of space) so I am looking into alternatives." />
  <row Id="300" PostHistoryTypeId="1" PostId="127" RevisionGUID="68f440a2-ac8d-4132-bb43-0500b4e6765e" CreationDate="2013-03-01T01:59:15.647" UserId="102" Text="What are some good clipboard-archiving utilities for archiving everything I copy in Windows?" />
  <row Id="301" PostHistoryTypeId="3" PostId="127" RevisionGUID="68f440a2-ac8d-4132-bb43-0500b4e6765e" CreationDate="2013-03-01T01:59:15.647" UserId="102" Text="&lt;clipboard&gt;" />
  <row Id="302" PostHistoryTypeId="2" PostId="128" RevisionGUID="e4ab9dfb-e17b-4708-8227-80451071a573" CreationDate="2013-03-01T02:01:37.307" UserId="102" Text="So I'm thinking of screen-capturing everything I have as video and storing it all on external hard drives (or even online backup). But it would take a lot of MBs.." />
  <row Id="303" PostHistoryTypeId="1" PostId="128" RevisionGUID="e4ab9dfb-e17b-4708-8227-80451071a573" CreationDate="2013-03-01T02:01:37.307" UserId="102" Text="How many MBs would it take for me to store everything that my computer captures as video?" />
  <row Id="304" PostHistoryTypeId="3" PostId="128" RevisionGUID="e4ab9dfb-e17b-4708-8227-80451071a573" CreationDate="2013-03-01T02:01:37.307" UserId="102" Text="&lt;screencaptures&gt;" />
  <row Id="305" PostHistoryTypeId="2" PostId="129" RevisionGUID="e0296d4f-aa8d-45d0-86d7-3866667b3046" CreationDate="2013-03-01T02:06:23.537" UserId="102" Text="I have like 100 notebooks of content by now, and they're a pain to lug around. Is there a way for me to mass digitize them without having to do anything manually at all?" />
  <row Id="306" PostHistoryTypeId="1" PostId="129" RevisionGUID="e0296d4f-aa8d-45d0-86d7-3866667b3046" CreationDate="2013-03-01T02:06:23.537" UserId="102" Text="How can I digitize as many of my old notebooks with as little extra effort as possible?" />
  <row Id="307" PostHistoryTypeId="3" PostId="129" RevisionGUID="e0296d4f-aa8d-45d0-86d7-3866667b3046" CreationDate="2013-03-01T02:06:23.537" UserId="102" Text="&lt;notebooks&gt;" />
  <row Id="308" PostHistoryTypeId="2" PostId="130" RevisionGUID="f3d408f6-7226-42de-aa95-0c92cd6c916e" CreationDate="2013-03-01T02:10:02.920" UserId="102" Text="Ideally I'd like to back everything up to a cloud. But my Internet connection probably isn't fast enough to back everything up so I'd still like a secure place to store them.&#xD;&#xA;&#xD;&#xA;I would also like to guard against all low-probability events, such as table collapses, floods, earthquakes, window-shattering meteors, and me accidentally stepping on my old hard drives. " />
  <row Id="309" PostHistoryTypeId="1" PostId="130" RevisionGUID="f3d408f6-7226-42de-aa95-0c92cd6c916e" CreationDate="2013-03-01T02:10:02.920" UserId="102" Text="What is the safest place to store an external hard drive?" />
  <row Id="310" PostHistoryTypeId="3" PostId="130" RevisionGUID="f3d408f6-7226-42de-aa95-0c92cd6c916e" CreationDate="2013-03-01T02:10:02.920" UserId="102" Text="&lt;hardware&gt;" />
  <row Id="311" PostHistoryTypeId="2" PostId="131" RevisionGUID="32b93fae-0793-4e19-a640-616315f7329f" CreationDate="2013-03-01T02:12:46.617" UserId="102" Text="Perhaps because louder speakers tend to induce larger vibrations that could have an affect on the hard disk head?&#xD;&#xA;&#xD;&#xA;Alternatively, can putting an external speaker next to an external hard drive possibly reduce the lifespan of the hard drive?" />
  <row Id="312" PostHistoryTypeId="1" PostId="131" RevisionGUID="32b93fae-0793-4e19-a640-616315f7329f" CreationDate="2013-03-01T02:12:46.617" UserId="102" Text="If my computer plays music that's really loud, could that make my computer's hard drives accumulate bad sectors faster, or maybe fail faster?" />
  <row Id="313" PostHistoryTypeId="3" PostId="131" RevisionGUID="32b93fae-0793-4e19-a640-616315f7329f" CreationDate="2013-03-01T02:12:46.617" UserId="102" Text="&lt;hard-drives&gt;" />
  <row Id="314" PostHistoryTypeId="2" PostId="132" RevisionGUID="04635b20-fae7-4356-b065-5a7f1410b360" CreationDate="2013-03-01T02:15:09.927" UserId="102" Text="Highly-polluted environments (especially ones with nanoparticles) could have more dust contamination, which could increase the amount of tiny dust that accumulates inside hard drives." />
  <row Id="315" PostHistoryTypeId="1" PostId="132" RevisionGUID="04635b20-fae7-4356-b065-5a7f1410b360" CreationDate="2013-03-01T02:15:09.927" UserId="102" Text="Do electronic devices, hard drives, and laptops fail faster in highly-polluted environments?" />
  <row Id="316" PostHistoryTypeId="3" PostId="132" RevisionGUID="04635b20-fae7-4356-b065-5a7f1410b360" CreationDate="2013-03-01T02:15:09.927" UserId="102" Text="&lt;hard-drives&gt;" />
  <row Id="317" PostHistoryTypeId="2" PostId="133" RevisionGUID="3cbcc458-d548-4b28-8aa9-ad0c7dfcb2a2" CreationDate="2013-03-01T02:17:09.970" UserId="102" Text="Many external links to Wikipedia are now dead links. I know not to trust Yahoo News, for example, since its links become dead links quite rapidly. On the other hand, I have a bit more trust for sources like the New York Times." />
  <row Id="318" PostHistoryTypeId="1" PostId="133" RevisionGUID="3cbcc458-d548-4b28-8aa9-ad0c7dfcb2a2" CreationDate="2013-03-01T02:17:09.970" UserId="102" Text="What news websites should I choose to link to when I want the links to work years from now?" />
  <row Id="319" PostHistoryTypeId="3" PostId="133" RevisionGUID="3cbcc458-d548-4b28-8aa9-ad0c7dfcb2a2" CreationDate="2013-03-01T02:17:09.970" UserId="102" Text="&lt;links&gt;" />
  <row Id="320" PostHistoryTypeId="2" PostId="134" RevisionGUID="0a795868-38ce-4f19-977e-a52257a4df0f" CreationDate="2013-03-01T02:23:15.423" UserId="126" Text="[ArchiveFacebook](https://addons.mozilla.org/en-US/firefox/addon/archivefacebook/) is a Firefox plugin that was developed by the [Old Dominion University Web Science and Digital Libraries Research Group](http://ws-dl.blogspot.com/). While I think it doesn't archive groups yet, the sense I get is that it could be adapted to do so." />
  <row Id="321" PostHistoryTypeId="2" PostId="135" RevisionGUID="9fdc8f46-6d2a-4b44-b7df-98ac8d9b65a9" CreationDate="2013-03-01T02:26:29.463" UserId="102" Text="Cost is definitely a factor, and I would be willing to sacrifice some durability for cost (if a slight increase in durability results in a significant increase in cost). " />
  <row Id="322" PostHistoryTypeId="1" PostId="135" RevisionGUID="9fdc8f46-6d2a-4b44-b7df-98ac8d9b65a9" CreationDate="2013-03-01T02:26:29.463" UserId="102" Text="What should I look for when buying a durable external hard drive?" />
  <row Id="323" PostHistoryTypeId="3" PostId="135" RevisionGUID="9fdc8f46-6d2a-4b44-b7df-98ac8d9b65a9" CreationDate="2013-03-01T02:26:29.463" UserId="102" Text="&lt;hard-drives&gt;" />
  <row Id="324" PostHistoryTypeId="5" PostId="129" RevisionGUID="448ba90c-1389-461e-8164-943421a1a2f5" CreationDate="2013-03-01T02:30:26.887" UserId="102" Comment="added 176 characters in body" Text="I have a very large number of notebooks by now, and they're a pain to lug around (especially since I have to move around a lot). Is there a way for me to mass digitize them without having to do anything manually at all?&#xD;&#xA;&#xD;&#xA;There are book-scanning services like http://www.blueleaf-book-scanning.com/, but they don't seem to accept notebooks yet." />
  <row Id="325" PostHistoryTypeId="2" PostId="136" RevisionGUID="0c7924e7-71e3-41cb-97bc-1b23678906a5" CreationDate="2013-03-01T02:39:02.200" UserId="102" Text="Say, an Ajax-heavy website like my Facebook news feed or my Quora news feed updates itself every 5 minutes or so, and I want to be able to automatically store &quot;snapshots&quot; of the refreshes every 5 minutes or so. How would I go about doing it?" />
  <row Id="326" PostHistoryTypeId="1" PostId="136" RevisionGUID="0c7924e7-71e3-41cb-97bc-1b23678906a5" CreationDate="2013-03-01T02:39:02.200" UserId="102" Text="What is the easiest way to archive a rapidly-updating website every 5 minutes?" />
  <row Id="327" PostHistoryTypeId="3" PostId="136" RevisionGUID="0c7924e7-71e3-41cb-97bc-1b23678906a5" CreationDate="2013-03-01T02:39:02.200" UserId="102" Text="&lt;web-archiving&gt;" />
  <row Id="328" PostHistoryTypeId="2" PostId="137" RevisionGUID="0a507b5b-121d-4765-ba68-52a11b266f59" CreationDate="2013-03-01T02:42:02.663" UserId="102" Text="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/disk_failures.pdf says that there is no correlation between disk usage and failure rate. But I'm not sure if this applies for home usage.&#xD;&#xA;&#xD;&#xA;I also wonder - will my external hard drives be just as good if I haven't used them for 10 years? All my external hard drives from 2007 are still going strong, for example. " />
  <row Id="329" PostHistoryTypeId="1" PostId="137" RevisionGUID="0a507b5b-121d-4765-ba68-52a11b266f59" CreationDate="2013-03-01T02:42:02.663" UserId="102" Text="Will using an external hard drive more often mean that it will fail faster on average?" />
  <row Id="330" PostHistoryTypeId="3" PostId="137" RevisionGUID="0a507b5b-121d-4765-ba68-52a11b266f59" CreationDate="2013-03-01T02:42:02.663" UserId="102" Text="&lt;hard-drives&gt;" />
  <row Id="331" PostHistoryTypeId="4" PostId="135" RevisionGUID="4f3e14df-733e-4533-907c-edb0b200e681" CreationDate="2013-03-01T02:42:56.797" UserId="102" Comment="edited title" Text="What should I look for when finding an external HD that maximizes its expected lifetime as a stable carrier medium?" />
  <row Id="333" PostHistoryTypeId="2" PostId="138" RevisionGUID="e5e626b3-45ef-4670-8ce2-bd5433668979" CreationDate="2013-03-01T03:09:46.823" UserId="102" Text="I know that Google Reader often archives all of the items in a RSS feed, which allows me to access the RSS feeds of blogs that no longer exist, for example. But how can I mass-export the content of those feeds onto disk?" />
  <row Id="334" PostHistoryTypeId="1" PostId="138" RevisionGUID="e5e626b3-45ef-4670-8ce2-bd5433668979" CreationDate="2013-03-01T03:09:46.823" UserId="102" Text="How can I archive all the data in a Google Reader RSS feed?" />
  <row Id="335" PostHistoryTypeId="3" PostId="138" RevisionGUID="e5e626b3-45ef-4670-8ce2-bd5433668979" CreationDate="2013-03-01T03:09:46.823" UserId="102" Text="&lt;web-archiving&gt;" />
  <row Id="336" PostHistoryTypeId="2" PostId="139" RevisionGUID="244622d3-8db1-4310-875d-7dcd2a678232" CreationDate="2013-03-01T03:40:27.557" UserId="126" Text="I would also add to Nicholas' detailed answer that the type of disks you have can largely determine your imaging setup. While Mac high-density (1.4MB) disks will be easily readable and at least imageable if you have a USB floppy drive and software like dd or FTK Imager, you will not be able to image 400K (single-sided double density) or 800K (double-sided double density) disks using this setup. The reason behind this is the encoding scheme used on the disk.&#xD;&#xA;&#xD;&#xA;High-density floppies use [MFM](http://en.wikipedia.org/wiki/Modified_Frequency_Modulation) encoding, which is supported by essentially all contemporary floppy controller boards (including those found in USB floppies, and those found on some recent motherboards). Double density Mac floppies, however, use [GCR](http://en.wikipedia.org/wiki/Group_Code_Recording) encoding, which is **unsupported by most recent controllers.**&#xD;&#xA;&#xD;&#xA;To image GCR-encoded floppies using a more recent computer, you will need a board such as a [KryoFlux](http://www.kryoflux.com/) or a [Catweasel](http://en.wikipedia.org/wiki/Individual_Computers_Catweasel). I suggest looking at [this post](http://libraries.stackexchange.com/questions/1261/is-there-a-hardware-controller-option-for-acquiring-data-images-off-floppy-disks/1262#1262) on libraries.stackexchange for potential guidance on which equipment to acquire (in the interest of disclosure, I have an answer on that post).&#xD;&#xA;&#xD;&#xA;The other complicating factor in this is the file system used on the disks. If you're imaging 400K SSDD Mac disks, these disks will probably contain an [MFS](http://en.wikipedia.org/wiki/Macintosh_File_System) file system, which very few contemporary tools will read. [HFS](http://en.wikipedia.org/wiki/Hierarchical_File_System) and HFS+/HFS Extended file systems are somewhat easier to read using recent tools - FTK Imager, for instance, will support reading and extracting those images." />
  <row Id="337" PostHistoryTypeId="2" PostId="140" RevisionGUID="b83c0b86-dc58-4cf8-b9e5-4122eb14dcd8" CreationDate="2013-03-01T04:07:04.350" UserId="126" Text="In my opinion, part of the &quot;where do I start?&quot; question is whether the organization intends to work through a backlog or focus on active collecting. I realize at this point that it's likely to be a bit of both for any institution that is truly beginning on acquiring assets off media.&#xD;&#xA;&#xD;&#xA;At an absolute minimum, I would recommend a USB write blocker such as the [WiebeTech inline writeblocker](http://www.wiebetech.com/products/USB-WriteBlocker.php) (199 USD) or the [Tableau T8-R2](http://www.tableau.com/index.php?pageid=products&amp;model=T8-R2) (~299 USD). While it's possible that acquisitions may come in as bare hard drives or drives pulled from existing machines, these can often be easily attached to USB enclosures. Most USB write blockers will work with the majority of USB mass storage devices, which allows for the most flexibility - for example, you can attach USB flash drives, a USB flash card reader, or USB Zip drives to a write blocker. &#xD;&#xA;&#xD;&#xA;I'd also recommend having a reasonably good amount (8 TB+, depending on how much you expect to acquire) of online, local staging storage. At my place of employment, we've used a [Drobo B800fs](http://www.drobo.com/products/business/b800fs/index.php) for this, and with 8 X 2 TB drives, we have approximately 12 TB of usable staging storage. &#xD;&#xA;&#xD;&#xA;For acquisition alone, you don't really need a high-powered machine. In fact, our acquisition machines are relatively have relatively low specs (AMD Phenom IIx4 970 3.5 GHz processor, 4 GB RAM). If you expect to do a reasonable amount of processing of acquired assets, you'll definitely want a higher-powered workstation, however, but in our case it was cheaper not to buy a FRED and instead to buy a high-end Dell workstation and add 48 GB of RAM. The specs for such a machine largely depend on the type of processing you'll be doing, though.&#xD;&#xA;&#xD;&#xA;I'd also suggest consider acquiring a high-powered laptop to make field-based acquisitions go much easier." />
  <row Id="338" PostHistoryTypeId="5" PostId="140" RevisionGUID="70a43101-9a48-4e34-a47e-4587e998d8fa" CreationDate="2013-03-01T04:25:47.347" UserId="126" Comment="enhanced answer" Text="In my opinion, part of the &quot;where do I start?&quot; question is whether the organization intends to work through a backlog or focus on active collecting. I realize at this point that it's likely to be a bit of both for any institution that is truly beginning on acquiring assets off media.&#xD;&#xA;&#xD;&#xA;At an absolute minimum, I would recommend a USB write blocker such as the [WiebeTech inline writeblocker](http://www.wiebetech.com/products/USB-WriteBlocker.php) (199 USD) or the [Tableau T8-R2](http://www.tableau.com/index.php?pageid=products&amp;model=T8-R2) (~299 USD). While it's possible that acquisitions may come in as bare hard drives or drives pulled from existing machines, these can often be easily attached to USB enclosures. Most USB write blockers will work with the majority of USB mass storage devices, which allows for the most flexibility - for example, you can attach USB flash drives, a USB flash card reader, or USB Zip drives to a write blocker. &#xD;&#xA;&#xD;&#xA;I also strongly recommend investing in some reasonably decent optical drives to acquire CD-ROM/CD-R/DVD images. A tray-loading drive rather than a slot-loading drive is preferable given the existence of smaller size (3&quot; and &quot;business card&quot;) CDs.&#xD;&#xA;&#xD;&#xA;I'd recommend having a reasonably good amount (8 TB+, depending on how much you expect to acquire) of online, local staging storage. At my place of employment, we've used a [Drobo B800fs](http://www.drobo.com/products/business/b800fs/index.php) for this, and with 8 X 2 TB drives, we have approximately 12 TB of usable staging storage. &#xD;&#xA;&#xD;&#xA;For acquisition alone, you don't really need a high-powered machine. In fact, our acquisition machines are relatively have relatively low specs (AMD Phenom IIx4 970 3.5 GHz processor, 4 GB RAM). Alternately, I'd suggest consider acquiring (in addition, if resources allow) a high-powered laptop to make field-based acquisitions go much easier.&#xD;&#xA;&#xD;&#xA;If you expect to do a reasonable amount of processing of acquired assets, you'll definitely want a higher-powered workstation, however, but in our case it was cheaper not to buy a FRED and instead to buy a high-end Dell workstation and add 48 GB of RAM. The specs for such a machine largely depend on the type of processing you'll be doing, though.&#xD;&#xA;&#xD;&#xA;Any drives for obsolete or uncommon formats should be treated as extras, in my opinion. While it might make sense to have a setup with floppy drives, you should consider whether these media are worth the time and effort to set up a local means to read them. This is equally true of formats like Jaz cartridges, Syquest cartridges, etc.&#xD;&#xA;&#xD;&#xA;In terms of useful accessories, I've mentioned them in line - things like cables, adapters, card readers, etc. are all inexpensive and easily sourced from your favorite online computer retailer. " />
  <row Id="339" PostHistoryTypeId="2" PostId="141" RevisionGUID="c3433b79-89b5-43d0-a21c-43d36f4be871" CreationDate="2013-03-01T06:41:21.527" UserId="74" Text="You should generally avoid **commercial** sites. They have a tendency to close their articles for subscribers only, so is with the New York Times. Even if some portal have free archive now, he can change that policy in future. There's nothing more annoying than clicking in the link and having the screen requiring to pay. This can even make your visitors think your website is commercial and leave it for all.&#xD;&#xA;&#xD;&#xA;The best way to refer to newspaper articles is to cite everything crucial, and provide the source as bookmark, with optional link (but the number of physical newspaper being more important, it is more likely to survive than link). " />
  <row Id="340" PostHistoryTypeId="2" PostId="142" RevisionGUID="641de93f-d8f9-4f9d-a7e6-b3c3c3911bb3" CreationDate="2013-03-01T06:49:31.413" UserId="74" Text="You are taking false approach trying to preserve **user interface**, and not **content**. They used to be the same in the begins of internet era, now it is no longer the case.&#xD;&#xA;&#xD;&#xA;First of all, those what you describe is only the content visible to users. Bots are seeing the test content, to make the site googlable. But that content is not the one to be preserved because of heavy data duplication.&#xD;&#xA;&#xD;&#xA;The correct approach is to archive the **data dumps** that are provided by that site (in lack of them, RSS feeds etc.). If such full-AJAX site doesn't provide such dumps, it usually means they are interested only in financial win and not the value of content generated by users, so you should avoid such sites.&#xD;&#xA;&#xD;&#xA;**StackExchange**, for example, is valuing user contribution. Every content is on open licence and the data dumps are published on regular basis." />
  <row Id="341" PostHistoryTypeId="2" PostId="143" RevisionGUID="7a544340-c249-4717-ab08-748cea6ab4f3" CreationDate="2013-03-01T06:59:55.340" UserId="74" Text="There is no economic argument for preserving something **only because it exists**. It depends fully on what is in that video.&#xD;&#xA;&#xD;&#xA;I don't understand what you understand by **Pristine** (AFAIK it's the main city of Kosovo province), but generally, the videos of high value should be stored in the highest quality available. Example of such videos are documentaries, reportages etc. When it comes to lower quality videos, such as thousands of hours of video from the foreign trip, if they were too low quality or too redundant to use for documentary/reportage, storing them in reduced quality is better alternative than deleting them.&#xD;&#xA;&#xD;&#xA;For such materials as private wedding videos, when the people interested in them no longer live, I think it would be enough to use heavy compression and resampling and archiving them with family and temporary tags. The may be of value for future anthropologiests or historicans. &#xD;&#xA;&#xD;&#xA;As general rule, I would never trust someone who is going to describe the whole complex process with one single sentence. Video has no value because it is a video but because of its content. So the economic criteria can be applied only when that value is taken into account." />
  <row Id="342" PostHistoryTypeId="10" PostId="129" RevisionGUID="58c968ad-cadd-4844-8c36-4bd72bd0f345" CreationDate="2013-03-01T07:02:55.263" UserId="-1" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:126,&quot;DisplayName&quot;:&quot;anarchivist&quot;},{&quot;Id&quot;:118,&quot;DisplayName&quot;:&quot;John Lovejoy&quot;},{&quot;Id&quot;:32,&quot;DisplayName&quot;:&quot;Courtney C. Mumma&quot;}]}" />
  <row Id="343" PostHistoryTypeId="2" PostId="144" RevisionGUID="9f3aa294-51ac-45b8-8655-853cc0768be0" CreationDate="2013-03-01T08:10:32.693" UserId="91" Text="Not sure if this is an option in your context, but the only way to make sure URLs will not break and will still deliver the same contents in the years to come is to archive them. There is a non-commercial, free service that does just that: [WebCite](http://www.webcitation.org/). You could then provide two links: one to the original source, and one to the archive copy at WebCite.&#xD;&#xA;&#xD;&#xA;If providing two links or just archive links is not an option, I would second @lechlukasz in suggesting that non-commercial news websites are more likely not to move contents behind a paywall. Perhaps [WikiNews](https://wikinews.org/) is a safe bet for stable news links, being an official WikiMedia project just like Wikipedia." />
  <row Id="344" PostHistoryTypeId="2" PostId="145" RevisionGUID="80ca52e3-6ef8-4736-a5bb-47a4e528602e" CreationDate="2013-03-01T08:20:47.337" UserId="89" Text="There is much work going on in retro digitization, much of it content which is old enaugh not to be subject to copyright any more. The outcome is published to the internet by libraries as well as by Google Books, for example. But what is the copyright situation for the outcome of retro digitization? Is it public domain by default, too, or isn’t it? If not, is the driver of retro digitization projects allowed to explicitly publish the retro digitizations under an open license, or isn’t it?" />
  <row Id="345" PostHistoryTypeId="1" PostId="145" RevisionGUID="80ca52e3-6ef8-4736-a5bb-47a4e528602e" CreationDate="2013-03-01T08:20:47.337" UserId="89" Text="What is the copyright situation for the outcome of retro digitization?" />
  <row Id="346" PostHistoryTypeId="3" PostId="145" RevisionGUID="80ca52e3-6ef8-4736-a5bb-47a4e528602e" CreationDate="2013-03-01T08:20:47.337" UserId="89" Text="&lt;digitize&gt;" />
  <row Id="348" PostHistoryTypeId="6" PostId="145" RevisionGUID="4c59bdb1-cc16-4270-a357-4d6b757e402d" CreationDate="2013-03-01T08:45:16.487" UserId="91" Comment="edited tags" Text="&lt;digitize&gt;&lt;legal&gt;&lt;unclear-location&gt;" />
  <row Id="349" PostHistoryTypeId="5" PostId="66" RevisionGUID="984a1032-c5b6-428b-be1c-3db25489b6df" CreationDate="2013-03-01T08:45:54.223" UserId="97" Comment="General improvements" Text="Usenet newsgroups are cached and duplicated by numerous servers, and redundancy is a very important factor in preservation of data.&#xD;&#xA;&#xD;&#xA;When it comes to sharing some trivia and local information, many individuals are using blogs and CMS'es (content management systems). An example could be the bulletin of a small sports community, with information about the competitions, winners etc. &#xD;&#xA;&#xD;&#xA;Is it a good idea to a create Usenet newsgroup, or use an existing group, for sharing such information and preserving it for future generations due to heavy duplication? Are really all Usenet messages archived, starting from the beginning? Are the organizations storing them planning to continue to do so in future? The time of Usenet seems to be over, as it has been replaced by specialized fora and Q&amp;As from one side, and the social portals from the other." />
  <row Id="350" PostHistoryTypeId="4" PostId="66" RevisionGUID="984a1032-c5b6-428b-be1c-3db25489b6df" CreationDate="2013-03-01T08:45:54.223" UserId="97" Comment="General improvements" Text="Are Usenet newsgroups a good way to preserve sniplets of publicly available information?" />
  <row Id="351" PostHistoryTypeId="24" PostId="66" RevisionGUID="984a1032-c5b6-428b-be1c-3db25489b6df" CreationDate="2013-03-01T08:45:54.223" Comment="Proposed by 97 approved by 74 edit id of 14" />
  <row Id="352" PostHistoryTypeId="2" PostId="146" RevisionGUID="570af187-a14f-4a83-aea6-7c6a6d4c725e" CreationDate="2013-03-01T08:51:43.967" UserId="91" Text="Quora provides an RSS feed. This is what you should archive. It is a friendly XML format. As for Facebook, sorry, I cannot help you there." />
  <row Id="353" PostHistoryTypeId="5" PostId="94" RevisionGUID="1e2cd93c-1ade-46d7-bd8f-2de70fa5fa97" CreationDate="2013-03-01T09:56:20.903" UserId="70" Comment="added 195 characters in body" Text="- If you only care about accidental data corruption: Any of those will do.&#xD;&#xA;&#xD;&#xA;- If there is a possibility of targeted attack: MD5 is broken, SHA1 weakened, SHA2 is doing very well. Note that SHA1 and SHA2 are completely different algorithms.&#xD;&#xA;&#xD;&#xA;- Also, if you're going to store LOTS of data (e.g. 2^70 files) then you need a hash algorithm which has at least 2X+20 bits due to birthday paradox.  &#xD;&#xA;The X here is log2(number_of_files), so, for example, if the repo has 1 000 000 000 files (about 2^30), 2*30+20=80 bits of (good) hash is necessary to provide sufficient collision protection &#xD;&#xA;&#xD;&#xA;Overall, I would use 256-bit SHA2 for next 100+ years. &#xD;&#xA;&#xD;&#xA;For tamper resistance, using any two hash algorithms (even if one is broken) increases hack difficulty immensely. So calculating MD5 in addition to sha256 (because it is already widely used) should be more than enough." />
  <row Id="354" PostHistoryTypeId="10" PostId="136" RevisionGUID="dabf3bab-831e-4bca-809f-9661042ab879" CreationDate="2013-03-01T10:19:03.547" UserId="-1" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:126,&quot;DisplayName&quot;:&quot;anarchivist&quot;},{&quot;Id&quot;:118,&quot;DisplayName&quot;:&quot;John Lovejoy&quot;},{&quot;Id&quot;:32,&quot;DisplayName&quot;:&quot;Courtney C. Mumma&quot;},{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;}]}" />
  <row Id="356" PostHistoryTypeId="2" PostId="147" RevisionGUID="3e71021a-d9b8-491e-83f9-cb6ee28690b4" CreationDate="2013-03-01T12:00:18.180" UserId="31" Text="In general, the best option I am aware of is to use something like [PhantomJS][1] to view the website, and use that to determine all the URLs the rendering process depends upon, which can then be archive by using something like wget. This is what my [flashfreeze prototype][2] does.&#xD;&#xA;&#xD;&#xA;However, in your specific example, you would need to go further and explicitly script PhantomJS it so that it repeatedly simulates/invokes mouse clicks on all the 'expand more' buttons, until it runs out of buttons.&#xD;&#xA;&#xD;&#xA;[1]: http://phantomjs.org/&#xD;&#xA;[2]: https://github.com/ukwa/flashfreeze" />
  <row Id="357" PostHistoryTypeId="2" PostId="148" RevisionGUID="95bc519e-3674-4117-82d6-a7694fe90f8e" CreationDate="2013-03-01T12:01:46.477" UserId="31" Text="Have you tried Google's [Data Liberation For Google Reader](http://www.dataliberation.org/google/reader)?" />
  <row Id="358" PostHistoryTypeId="10" PostId="133" RevisionGUID="f085ddcc-15c3-43aa-94f1-ba04f173b520" CreationDate="2013-03-01T12:16:52.033" UserId="-1" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:118,&quot;DisplayName&quot;:&quot;John Lovejoy&quot;},{&quot;Id&quot;:32,&quot;DisplayName&quot;:&quot;Courtney C. Mumma&quot;},{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;mopennock&quot;}]}" />
  <row Id="359" PostHistoryTypeId="10" PostId="131" RevisionGUID="61337a1d-ad68-4a15-8e11-653fca38a965" CreationDate="2013-03-01T12:23:06.457" UserId="-1" CloseReasonId="3" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:126,&quot;DisplayName&quot;:&quot;anarchivist&quot;},{&quot;Id&quot;:118,&quot;DisplayName&quot;:&quot;John Lovejoy&quot;},{&quot;Id&quot;:32,&quot;DisplayName&quot;:&quot;Courtney C. Mumma&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;mopennock&quot;}]}" />
  <row Id="360" PostHistoryTypeId="2" PostId="149" RevisionGUID="623da740-4165-48d1-973f-958dea56331e" CreationDate="2013-03-01T12:35:07.090" UserId="45" Text="First, you'll have to accept that this process is going to be even more volatile than traditional web archiving and factor that into your decisions regarding quality review, access, etc. One alternate tactic might be contacting the site owner to see if they do provide alternate ways for robots to crawl their content.&#xD;&#xA;&#xD;&#xA;The key part is using a real browser engine – i.e. something which executes JavaScript and follows the expected browser behaviour for the DOM, AJAX, etc. – and budget time for some sort of scripting in situations where content isn't loaded except in response to user action, requires logins to get around quotas for unregistered users (i.e. Quora), etc.&#xD;&#xA;&#xD;&#xA;One approach might be to use a capturing proxy like [liveweb](https://github.com/internetarchive/liveweb) which records a browser's activity into a WARC file, allowing you to record any browser or application which can be configured to use a proxy.&#xD;&#xA;&#xD;&#xA;I would also second Andy's suggestion of considering a webkit-based system like [PhantomJS](http://www.phantomjs.org), perhaps in conjunction with a toolkit like [pjscrape](http://nrabinowitz.github.com/pjscrape/) or [casperjs](http://casperjs.org/). The web QA community has also developed a number of tools around [Selenium](http://seleniumhq.org/) which is harder to get started with than PhantomJS but does have the advantage of supporting other browsers if you find yourself needing to archive something like a site which only works in Internet Explorer." />
  <row Id="361" PostHistoryTypeId="5" PostId="147" RevisionGUID="f0dd65d8-8327-4d16-b909-c04cedeb965c" CreationDate="2013-03-01T12:48:13.033" UserId="31" Comment="Typo" Text="In general, the best option I am aware of is to use something like [PhantomJS][1] to view the website, and use that to determine all the URLs the rendering process depends upon, which can then be archived by using something like wget. This is what my [flashfreeze prototype][2] does.&#xD;&#xA;&#xD;&#xA;However, in your specific example, you would need to go further and explicitly script PhantomJS it so that it repeatedly simulates/invokes mouse clicks on all the 'expand more' buttons, until it runs out of buttons.&#xD;&#xA;&#xD;&#xA;[1]: http://phantomjs.org/&#xD;&#xA;[2]: https://github.com/ukwa/flashfreeze" />
  <row Id="362" PostHistoryTypeId="2" PostId="150" RevisionGUID="36f9b501-0df5-45fa-a834-d96c684828db" CreationDate="2013-03-01T13:07:54.640" UserId="93" Text="The ethical issues around web archiving are interesting and the ethics of web archiving can vary from case to case. To judge whether a particular case is ethical, you might want to consider: &#xD;&#xA;&#xD;&#xA; - Why do I need to take a copy?&#xD;&#xA; - Do I have or need the owner's permission? &#xD;&#xA; - Do I have or should I ask for permission of anyone else who has contributed to the site?&#xD;&#xA; - Is there any reason why the owner would not want me to have a copy?&#xD;&#xA; - What is the subject matter?&#xD;&#xA; - Who will have access to my copy?&#xD;&#xA; - How long do I intend to keep it for?&#xD;&#xA; - Can I keep it securely and if not, who might be affected should it be released back on to the live web?&#xD;&#xA;&#xD;&#xA;There have been a few papers published in recent years:&#xD;&#xA;&#xD;&#xA; - '[Ethical issues in web archive creation and usage][1]' (2008)&#xD;&#xA; - '[Personal Internet Archives and Ethics][2]' (2012). &#xD;&#xA;&#xD;&#xA;The latter is particularly relevant given the context stated in your question.&#xD;&#xA;&#xD;&#xA;  [1]: http://iwaw.europarchive.org/08/IWAW2008-Rauber.pdf&#xD;&#xA;  [2]: http://rea.sagepub.com/content/early/2012/09/07/1747016112459450.full.pdf" />
  <row Id="363" PostHistoryTypeId="2" PostId="151" RevisionGUID="320d4017-057c-49f7-ae70-aac72c8535dd" CreationDate="2013-03-01T13:18:37.197" UserId="119" Text="When imaging 3.5&quot; (both double sided double density, as well as high density) floppy disks for the Macintosh, using a Kryoflux…&#xD;&#xA;&#xD;&#xA;1) What image format should I be dumping if my intent is to use these images within emulators such as Sheepshaver, Basilisk II, or Mini vMac?&#xD;&#xA;&#xD;&#xA;2) Is this image format I will be dumping for _use_ purposes also sufficient for long term preservation? Should I also be dumping a format that preserves lower level information?" />
  <row Id="364" PostHistoryTypeId="1" PostId="151" RevisionGUID="320d4017-057c-49f7-ae70-aac72c8535dd" CreationDate="2013-03-01T13:18:37.197" UserId="119" Text="What raw image format to use for emulation/use" />
  <row Id="365" PostHistoryTypeId="3" PostId="151" RevisionGUID="320d4017-057c-49f7-ae70-aac72c8535dd" CreationDate="2013-03-01T13:18:37.197" UserId="119" Text="&lt;floppy-disk&gt;&lt;disk-image&gt;&lt;kryoflux&gt;" />
  <row Id="366" PostHistoryTypeId="4" PostId="151" RevisionGUID="9020ffb8-9abe-4e1d-9fa9-e7d16df3a0f8" CreationDate="2013-03-01T13:29:09.200" UserId="119" Comment="edited title" Text="What disk image format to use for use vs preservation" />
  <row Id="367" PostHistoryTypeId="5" PostId="58" RevisionGUID="a90941d8-5571-42c2-8c80-e7743183847e" CreationDate="2013-03-01T13:52:02.863" UserId="97" Comment="Mention the disk drive by make and model name" Text="For disks created by pre-OS X versions of Mac OS, the best solution I've found is to (1) create disk images and (2) access their contents with an emulator.&#xD;&#xA;&#xD;&#xA;Your first priority should be to get the data off of the floppies, since they're at risk of degradation. Modern OSes can't read the System 6 filesystem even if they have a 3.5&quot; floppy drive, so what you need is a bit-by-bit disk image of any floppies you want to save. &#xD;&#xA;&#xD;&#xA;You might be able to create these with the SE itself, if it still works and if it has an internal hard drive and some form of network access, although I can't recommend any particular utilities. Alternatively, you can use a USB floppy drive to create images on a modern system using a utility like [dd][1] or [FTK imager][2]. A poster on the thread linked above recommended the [Nippon Labs USB-FLPY-DLOCK][3], although I can't vouch for it myself.&#xD;&#xA;&#xD;&#xA;One issue to be aware of is that not all 3.5&quot; inch floppies are 1.44 MB. In the early years of 3.5&quot; floppies there were [a number of different physical formats][4] (400kb, 800kb, single- and double-density, etc.), and not all drives were compatible with all types. A lot of the &quot;toaster&quot;-style Macs could only read certain varieties -- I remember this giving me headaches even at the time. If you think you might have disks in these formats, you should make sure that they are supported by whichever drive you end up using.&#xD;&#xA;&#xD;&#xA;Once you've created the disk images, you can open and mount them in an [emulator][5]. I can't recommend a particular emulator for an SE, but I use [SheepShaver][6] to access disks created by a circa-1997 PowerTower (one of the short-lived Macintosh clones running System 7.) &#xD;&#xA;&#xD;&#xA;Setup will depend on the particular emulator you're using, but you should be aware that  pre-X versions of Mac OS, including [System 6][7], are available for free from Apple's website. If you still have the installation disks for the software your parents used, you can take images of those and use them to install the necessary applications in the emulation environment without having to delve into the legal gray area of abandonware.&#xD;&#xA;&#xD;&#xA;Migrating your data to a contemporary format will depend on the particular applications and might be difficult. At the cost of losing some of the formatting, you might be able to export the data into an open format (ASCII text for word processing documents, CSV for spreadsheets) that you could then open on your current system. Ideally the application itself will be able to do this, but you might have to dig around to find a file conversion utility that runs on System 6.&#xD;&#xA;&#xD;&#xA;Once you've done all this, you can manage the disk images like you would any other files in your personal organization/backup system. If you plan to preserve this collection long-term (or to eventually store it in a formal repository), you should record any provenance/identifying information so that you know the contents and extent of what you have. &#xD;&#xA;&#xD;&#xA;This doesn't need to be complicated -- human-readable filenames and a spreadsheet are fine -- but you should be able to identify which images came from which (physical) disks and what each image contains, even if you end up discarding the physical copies. (I.e., image &quot;Financial_2.img&quot; came from an 800kb Maxell floppy with a handwritten label &quot;Financial Records Disk 2 of 6&quot; and contains financial data for the year 1988 in ClarisWorks Spreadsheet format.)&#xD;&#xA;&#xD;&#xA;In addition, you might want to keep a record of how to configure and set up the emulator, what the contemporary OS and hardware requirements are to run the emulator, what software (running on the emulator) is necessary to access the files, et cetera. That way, if you pass the collection on to someone else, or if you don't use it for several years and forget exactly what you did, you'll have the Representation Information (in OAIS terms) necessary to keep your data accessible.&#xD;&#xA;&#xD;&#xA;As I mentioned, I used this process to back up and preserve the contents of a System 7 desktop. Accessing the files is as easy as booting SheepShaver and mounting the disk image, although I haven't tackled the process of exporting them to preservation formats yet.&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Dd_%28Unix%29&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/FTK&#xD;&#xA;  [3]: http://www.newegg.com/Product/Product.aspx?Item=N82E16821105004&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/3.5%22_Floppy_Disk#3+1.E2.81.842-inch_floppy_disk_.28.22Microfloppy.22.29&#xD;&#xA;  [5]: http://en.wikipedia.org/wiki/List_of_computer_system_emulators#Apple_Macintosh_with_680x0_CPU&#xD;&#xA;  [6]: http://en.wikipedia.org/wiki/SheepShaver&#xD;&#xA;  [7]: http://download.info.apple.com/Apple_Support_Area/Apple_Software_Updates/English-North_American/Macintosh/System/Older_System/System_6.0.x/&#xD;&#xA;  [8]: http://download.info.apple.com/Apple_Support_Area/Apple_Software_Updates/English-North_American/Macintosh/System/Older_System/System_6.0.x/" />
  <row Id="368" PostHistoryTypeId="24" PostId="58" RevisionGUID="a90941d8-5571-42c2-8c80-e7743183847e" CreationDate="2013-03-01T13:52:02.863" Comment="Proposed by 97 approved by 17 edit id of 13" />
  <row Id="370" PostHistoryTypeId="5" PostId="133" RevisionGUID="667db32f-5384-47b6-af7e-104a2dd46668" CreationDate="2013-03-01T14:11:51.867" UserId="102" Comment="deleted 79 characters in body" Text="Many external links to Wikipedia are now dead links. I know not to trust Yahoo News, for example, since its links become dead links quite rapidly. " />
  <row Id="371" PostHistoryTypeId="10" PostId="145" RevisionGUID="e63d75ae-8b92-4421-896b-be21f965f5e0" CreationDate="2013-03-01T15:25:40.810" UserId="-1" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:91,&quot;DisplayName&quot;:&quot;Christian Pietsch&quot;},{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:31,&quot;DisplayName&quot;:&quot;Andy&quot;},{&quot;Id&quot;:42,&quot;DisplayName&quot;:&quot;Donald.McLean&quot;},{&quot;Id&quot;:76,&quot;DisplayName&quot;:&quot;Trevor Owens&quot;}]}" />
  <row Id="372" PostHistoryTypeId="2" PostId="152" RevisionGUID="505652a6-9043-493c-953f-76fc91f43630" CreationDate="2013-03-01T15:29:24.523" UserId="70" Text="1. No, you won't realistically wear out your external HD by normal use, except some rare models with bad firmware (google for &quot;WD green parking&quot;)&#xD;&#xA;2. But carrying it in your bag or pocket can limit its lifespan due to shocks.&#xD;&#xA;&#xD;&#xA;Anyway, I do not understand how does knowing the answer to this question helps you preserve data - there is no practical difference between 1% fail percentage and 10% fail percentage, you need backups both ways.&#xD;&#xA;" />
  <row Id="373" PostHistoryTypeId="10" PostId="137" RevisionGUID="42a04afb-891d-4306-a3ca-b1d53c75871a" CreationDate="2013-03-01T15:30:20.373" UserId="-1" CloseReasonId="3" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:126,&quot;DisplayName&quot;:&quot;anarchivist&quot;},{&quot;Id&quot;:118,&quot;DisplayName&quot;:&quot;John Lovejoy&quot;},{&quot;Id&quot;:70,&quot;DisplayName&quot;:&quot;wizzard0&quot;}]}" />
  <row Id="374" PostHistoryTypeId="2" PostId="153" RevisionGUID="7cb9ae38-4cce-4f79-bd87-5dd0b8dcef6c" CreationDate="2013-03-01T15:38:23.617" UserId="70" Text="1. Hard drives have dust filters and generally not collect any dust inside.&#xD;&#xA;2. Dust kills fans and therefore laptops&#xD;&#xA;3. &quot;electronic devices&quot; fear conductive dust and overheating, so in general dust is bad. But it is not usually the main source of failure." />
  <row Id="375" PostHistoryTypeId="10" PostId="132" RevisionGUID="f45346c7-677c-476c-9569-49f879c7f909" CreationDate="2013-03-01T15:38:39.157" UserId="-1" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:118,&quot;DisplayName&quot;:&quot;John Lovejoy&quot;},{&quot;Id&quot;:97,&quot;DisplayName&quot;:&quot;Michael Kjörling&quot;},{&quot;Id&quot;:70,&quot;DisplayName&quot;:&quot;wizzard0&quot;}]}" />
  <row Id="376" PostHistoryTypeId="2" PostId="154" RevisionGUID="a697a01a-d209-41d7-a7cb-d05a9ef4945b" CreationDate="2013-03-01T15:50:48.963" UserId="70" Text="I second the usage of Parchive (or other tools with similar functionality), but want to emphasize that in the end you should get a system where you have a set of disks (like a RAID) where you can tolerate a loss of any one disk and rebuild it from the remaining ones. &#xD;&#xA;&#xD;&#xA;This is better than having ability to recover a scratch from any disk, but failing to tolerate the complete disk loss, because they will fail over time one way or another, and you need a solid replacement strategy anyway." />
  <row Id="377" PostHistoryTypeId="2" PostId="155" RevisionGUID="7fb80b8e-33f6-4526-a9f0-35ac0d126d02" CreationDate="2013-03-01T15:57:38.777" UserId="21" Text="generally irc logs are plain txt files, each row containing the timestamp, the user, and the message.&#xD;&#xA;&#xD;&#xA;when log is not provided by the irc server the solution is to use an irc bouncer  (another irc server sitting between your client and the irc server) that logs for you.  [znc][1] is pretty simple to setup&#xD;&#xA;&#xD;&#xA;since these logs are text files you can use any technology (even grep) to search it. &#xD;&#xA;&#xD;&#xA;i should recommend [omega][2], a small cgi based on xapian&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://wiki.znc.in/Log&#xD;&#xA;  [2]: http://xapian.org/docs/omega/" />
  <row Id="378" PostHistoryTypeId="4" PostId="133" RevisionGUID="0455db1d-45eb-4b8b-9296-334cf68818af" CreationDate="2013-03-01T16:01:04.140" UserId="102" Comment="edited title" Text="What are some best practices when choosing news websites to link to when I want the links to work years from now?" />
  <row Id="379" PostHistoryTypeId="6" PostId="65" RevisionGUID="0ee8c5f8-24df-4519-bdae-a02e462a6e92" CreationDate="2013-03-01T16:03:45.050" UserId="109" Comment="edited tags" Text="&lt;file-management&gt;&lt;archive-format&gt;&lt;validation&gt;&lt;file-fixity&gt;" />
  <row Id="380" PostHistoryTypeId="2" PostId="156" RevisionGUID="d6615962-1bdb-4a1c-88c6-f13b70e33fb7" CreationDate="2013-03-01T16:09:31.090" UserId="21" Text="[**newsbeuter**][1] was and awesome news reader (i'm not using it anymore).  &#xD;&#xA;it can sync with [google reader][2] and save all the contents of feeds in a local sqlite database.&#xD;&#xA;&#xD;&#xA;if you are looking for a gui tool **NetNewswire** for osx also can sync with google reader and maintain an [html archive][3] of posts&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.newsbeuter.org/&#xD;&#xA;  [2]: http://www.newsbeuter.org/doc/newsbeuter.html#_google_reader_support&#xD;&#xA;  [3]: http://ranchero.com/netnewswire/help/3.2/en/htmlArchive.html" />
  <row Id="381" PostHistoryTypeId="5" PostId="156" RevisionGUID="f3e840c7-004b-49bb-b834-0eb1e5ad8470" CreationDate="2013-03-01T16:17:12.020" UserId="21" Comment="deleted 1 characters in body" Text="[**newsbeuter**][1] was an awesome news reader (i'm not using it anymore).  &#xD;&#xA;it can sync with [google reader][2] and save all the contents of feeds in a local sqlite database.&#xD;&#xA;&#xD;&#xA;if you are looking for a gui tool **NetNewswire** for osx also can sync with google reader and maintain an [html archive][3] of posts&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.newsbeuter.org/&#xD;&#xA;  [2]: http://www.newsbeuter.org/doc/newsbeuter.html#_google_reader_support&#xD;&#xA;  [3]: http://ranchero.com/netnewswire/help/3.2/en/htmlArchive.html" />
  <row Id="383" PostHistoryTypeId="2" PostId="158" RevisionGUID="e866943a-935c-4d30-802d-f31bd18d9b5b" CreationDate="2013-03-01T16:19:50.803" UserId="109" Text="I would emphasize the importance of workflow planning and documentation. &#xD;&#xA;&#xD;&#xA;A Submission Information Package (SIP), as described by the [OAIS reference model][1], can develop out of multiple steps: initial transfer of data, possibly moving that transfer from a dropbox to a workspace, format conversion, metadata creation/extraction. Understanding each step of the process will help clarify at what point data is being manipulated and potentially damaged and thus at what point it makes sense to check for damage before the process has moved too far ahead.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://public.ccsds.org/publications/archive/650x0b1.pdf" />
  <row Id="384" PostHistoryTypeId="2" PostId="159" RevisionGUID="28834df6-558f-479c-ac2c-055bb3f95794" CreationDate="2013-03-01T16:21:00.790" UserId="29" Text="Short-term storage seems cheap, but long-term storage for digital preservation is [expensive](http://blog.dshr.org/2012/05/lets-just-keep-everything-forever-in.html). Part of the solution to this problem is using archival appraisal to reduce the amount of data to preserve, but how do we appraise gigabytes or terabytes of data?&#xD;&#xA;&#xD;&#xA;Visualization tools like [WinDirStat](http://windirstat.info/) and [SpaceSniffer](http://www.uderzo.it/main_products/space_sniffer/) let you scan a folder structure quickly to prioritize potentially redundant data (e.g. system files) or core content (e.g. My Documents). Other tools like [C3PO](http://ifs.tuwien.ac.at/imp/c3po) let you survey technical metadata, allowing you to find oddball formats and preservation risks. Are there other tools used to quickly appraise data?" />
  <row Id="385" PostHistoryTypeId="1" PostId="159" RevisionGUID="28834df6-558f-479c-ac2c-055bb3f95794" CreationDate="2013-03-01T16:21:00.790" UserId="29" Text="What tools can we use to appraise content before digital preservation?" />
  <row Id="386" PostHistoryTypeId="3" PostId="159" RevisionGUID="28834df6-558f-479c-ac2c-055bb3f95794" CreationDate="2013-03-01T16:21:00.790" UserId="29" Text="&lt;storage&gt;&lt;born-digital&gt;&lt;data-curation&gt;" />
  <row Id="387" PostHistoryTypeId="2" PostId="160" RevisionGUID="8a948ec8-b184-49e1-a81c-f71b5a908094" CreationDate="2013-03-01T16:39:08.227" UserId="133" Text="Assuming that preserved content is kept in archival storage on spinning disks and that standard backup procedures are in place, is one complete copy (separate from the backup) of the content enough, provided it is stored remotely?" />
  <row Id="388" PostHistoryTypeId="1" PostId="160" RevisionGUID="8a948ec8-b184-49e1-a81c-f71b5a908094" CreationDate="2013-03-01T16:39:08.227" UserId="133" Text="How many copies of preserved content are needed?" />
  <row Id="389" PostHistoryTypeId="3" PostId="160" RevisionGUID="8a948ec8-b184-49e1-a81c-f71b5a908094" CreationDate="2013-03-01T16:39:08.227" UserId="133" Text="&lt;storage&gt;&lt;backup-and-recovery&gt;" />
  <row Id="390" PostHistoryTypeId="2" PostId="161" RevisionGUID="a4dc4712-c20d-49da-b3af-05a41d714793" CreationDate="2013-03-01T17:13:30.553" UserId="45" Text="The conservative answer would be three copies so you have a tie-breaker in case two copies disagree. If you have something like two online systems and a tape backup, that should qualify and the issue is particularly improved if you store fixities separately (also redundantly and checksummed, of course) and thus have a high degree of confidence in your ability to state whether any particular copy is correct.&#xD;&#xA;&#xD;&#xA;The other issue, which might have been implied by your “standard backup procedures“ note, is that I wouldn't trust any copy which hasn't been recently verified: spinning disks avoid a significant class of problems but they're still prone to bitrot and too many filesystems will silently return corrupted data. Something like an object store or modern filesystem (ZFS, btrfs, ???) which performs active scrubbing against a separate fixity calculated from the original data is a *much* safer proposition than, say, a neglected departmental backup server.&#xD;&#xA;&#xD;&#xA;To summarize: yes, if both active copies are on modern storage systems and you use a separate fixity checking system. No if you're forced to rely on legacy storage since you'd have to devote significant resources to auditing and validation and, presumably, rely on things like quorum tests." />
  <row Id="391" PostHistoryTypeId="2" PostId="162" RevisionGUID="36665834-a4e0-4a94-b136-00c8725d6e84" CreationDate="2013-03-01T17:22:25.877" UserId="45" Text="This not the formal answer but as [primarily] a software developer, I felt the need to note that I commonly use [Git](http://git-scm.com/) to do this because it's a version control system which can be used completely locally and uses strong checksums to track file contents.&#xD;&#xA;&#xD;&#xA;In practice, this means that I can work with incoming files like this (e.g. I've had to make technical corrections to partner-provided metadata, much of it in languages I cannot read):&#xD;&#xA;&#xD;&#xA;1. `git init` the directory&#xD;&#xA;2. `git add .` &amp; `git commit` to track the initial received version&#xD;&#xA;3. After each round of changes, repeat step 2&#xD;&#xA;&#xD;&#xA;This is a very lightweight process which provides full history &amp; integrity checks and has the nice aspect that you can easily synchronize copies with full history to other locations. It will work on Linux, Mac, Windows, etc. (GUI tools are available for all of these) and has very little friction once you've installed the software and learned a few basic operations.&#xD;&#xA;&#xD;&#xA;The downside is that the approach is slow for very large binary files or operations which alter many tens of thousands of files. There are tools like [bup](https://github.com/bup/bup) which use the same format but are optimized for binary content and I would recommend investigating that if you are faced with that challenge." />
  <row Id="392" PostHistoryTypeId="2" PostId="163" RevisionGUID="a81549a1-eb70-4c11-887f-d61221518082" CreationDate="2013-03-01T17:43:50.863" UserId="42" Text="Chris Adam's answer addresses problems with backup media, but there is also the issue of what is sometimes called &quot;business continuity&quot; - getting your organization back up and running in the event of a disaster.&#xD;&#xA;&#xD;&#xA;When making business continuity plans, you have to carefully consider various scenarios and whether or not it is even possible for the organization to recover in a particular scenario. &#xD;&#xA;&#xD;&#xA;So take a software development company. As long as most of the company's key personel survive, the company can recover if it has good, recent backups of its intellectual property (source code, customer data, contracts, etc). Assume that the company's main facilities have been completely destroyed and that all you have is whatever was off-site.&#xD;&#xA;&#xD;&#xA;If all you have is a bunch of tapes that have to be retrieved and then loaded somewhere, getting up and running again is going to take a significant amount of time. If one copy of each tape is all you have, and that one tape has a problem, then the organization is going to lose precious data during a time when it can ill afford to lose anything.&#xD;&#xA;&#xD;&#xA;The general rule that I go by is: three copies, one of them a local backup that is used for simple recoveries of accidentally deleted files, computer crashes and the like. The other two copies should both be offsite and in separate locations. Or use a cloud based backup service such as CrashPlan that takes care of the multiple redundant copies in multiple locations details for you." />
  <row Id="393" PostHistoryTypeId="4" PostId="4" RevisionGUID="836073c5-d8b9-42c3-835b-1872293ac36e" CreationDate="2013-03-01T17:48:14.050" UserId="133" Comment="Added &quot;media&quot; to question" Text="What physical media format(s) are going to be easiest to open in the future?" />
  <row Id="394" PostHistoryTypeId="6" PostId="4" RevisionGUID="836073c5-d8b9-42c3-835b-1872293ac36e" CreationDate="2013-03-01T17:48:14.050" UserId="133" Comment="Added &quot;media&quot; to question" Text="&lt;future-proofing&gt;&lt;media&gt;&lt;storage-media&gt;&lt;backup-and-recovery&gt;" />
  <row Id="395" PostHistoryTypeId="24" PostId="4" RevisionGUID="836073c5-d8b9-42c3-835b-1872293ac36e" CreationDate="2013-03-01T17:48:14.050" Comment="Proposed by 133 approved by 101 edit id of 15" />
  <row Id="396" PostHistoryTypeId="2" PostId="164" RevisionGUID="4ee41f81-0122-49f5-9008-420026664ba4" CreationDate="2013-03-01T18:08:47.663" UserId="113" Text="One possibility is [Archivematica][1]. &quot;Archivematica uses a micro-services design pattern to provide an integrated suite of software tools that allows users to process digital objects from ingest to access in compliance with the ISO-OAIS functional model.&quot;&#xD;&#xA;&#xD;&#xA;A second type of approach is described in a paper called &quot;[Automating Digital Processing at the Bentley Historical Library][2]&quot; that was presented by Michael Shallcross and Nancy Deromedi at iPRES 2012. They assembled a Windows-based processing workflow called &quot;AutoPro&quot; comprised of numerous off-the-shelf tools and custom batch scripts to facilitate appraisal. Their second slide lists the tools they are using under &quot;4. Digital Processing.&quot; I am not replicating the list here because the three brief documents they provide are very concise and it would be a shame to lose the context. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.archivematica.org/wiki/Main_Page&#xD;&#xA;  [2]: http://hdl.handle.net/2027.42/95923" />
  <row Id="397" PostHistoryTypeId="2" PostId="165" RevisionGUID="3c7b2cfe-97c8-4d8c-9e83-d5a32e509de2" CreationDate="2013-03-01T18:34:17.200" UserId="25" Text="A common mnemonic for this is 3-2-1:&#xD;&#xA;&#xD;&#xA; - THREE copies&#xD;&#xA; - On TWO different kinds of storage media (&quot;the cloud&quot; counts as one, yes)&#xD;&#xA; - ONE offsite.&#xD;&#xA;&#xD;&#xA;As a rule of thumb, it's not bad." />
  <row Id="398" PostHistoryTypeId="2" PostId="166" RevisionGUID="66c6c5ab-271f-4b55-a5f1-c396e12ab602" CreationDate="2013-03-01T18:38:15.310" UserId="25" Text="The ones that are easiest to audit and replicate data from. Silent failure is deadly.&#xD;&#xA;&#xD;&#xA;So, by way of example, spinning hard disks are better (yes, BETTER) than gold CD-ROMs, because auditing spinning disk for error and incipient failure is largely automatable and even automated. Copying from disk is easy, including over the network.&#xD;&#xA;&#xD;&#xA;Whereas CD-ROMs of whatever material require considerable and expensive-to-automate physical futzing in order to audit or copy from. Ergo they're rarely audited, ergo they become unopenable without notice. File and forget." />
  <row Id="400" PostHistoryTypeId="5" PostId="159" RevisionGUID="df5d4af6-13c6-42a9-94ed-e7ff754e0556" CreationDate="2013-03-01T18:54:53.130" UserId="29" Comment="added 38 characters in body" Text="Short-term storage seems cheap, but long-term storage for digital preservation is [expensive](http://blog.dshr.org/2012/05/lets-just-keep-everything-forever-in.html). Part of the solution to this problem is using archival appraisal to identify what content should be preserved and what content can be discarded, but how do we appraise gigabytes or terabytes of data?&#xD;&#xA;&#xD;&#xA;Visualization tools like [WinDirStat](http://windirstat.info/) and [SpaceSniffer](http://www.uderzo.it/main_products/space_sniffer/) let you scan a folder structure quickly to prioritize potentially redundant data (e.g. system files) or core content (e.g. My Documents). Other tools like [C3PO](http://ifs.tuwien.ac.at/imp/c3po) let you survey technical metadata, allowing you to look at rough estimates of format types to see what formats a creator used the most. Are there other tools used to quickly appraise data?" />
  <row Id="401" PostHistoryTypeId="10" PostId="135" RevisionGUID="2905bcdb-798a-46b7-8ac4-405bf69d60fd" CreationDate="2013-03-01T19:16:36.093" UserId="-1" CloseReasonId="4" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:118,&quot;DisplayName&quot;:&quot;John Lovejoy&quot;},{&quot;Id&quot;:70,&quot;DisplayName&quot;:&quot;wizzard0&quot;},{&quot;Id&quot;:43,&quot;DisplayName&quot;:&quot;jcmeloni&quot;}]}" />
  <row Id="402" PostHistoryTypeId="10" PostId="127" RevisionGUID="0dcc82bf-5e11-4b3e-8921-84655cbaf03b" CreationDate="2013-03-01T19:17:04.457" UserId="-1" CloseReasonId="3" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:32,&quot;DisplayName&quot;:&quot;Courtney C. Mumma&quot;},{&quot;Id&quot;:91,&quot;DisplayName&quot;:&quot;Christian Pietsch&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;mopennock&quot;},{&quot;Id&quot;:43,&quot;DisplayName&quot;:&quot;jcmeloni&quot;}]}" />
  <row Id="403" PostHistoryTypeId="10" PostId="128" RevisionGUID="c33b08c0-9297-4163-aad9-4c392b7d279d" CreationDate="2013-03-01T19:17:16.287" UserId="-1" CloseReasonId="4" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:32,&quot;DisplayName&quot;:&quot;Courtney C. Mumma&quot;},{&quot;Id&quot;:91,&quot;DisplayName&quot;:&quot;Christian Pietsch&quot;},{&quot;Id&quot;:43,&quot;DisplayName&quot;:&quot;jcmeloni&quot;}]}" />
  <row Id="404" PostHistoryTypeId="2" PostId="167" RevisionGUID="2144d567-0e61-4444-90ba-a79c492d2f43" CreationDate="2013-03-01T19:53:49.467" UserId="136" Text="The Curator's Workbench is useful for appraisal in some scenarios. It doesn't report much technical metadata, but it does help you capture the file structure and make a new arrangement of items. It will stage files and calculate checksums while you work with the folder structure and names.&#xD;&#xA;For more information, see the link:&#xD;&#xA;http://www.lib.unc.edu/blogs/cdr/index.php/about-the-curators-workbench/" />
  <row Id="405" PostHistoryTypeId="2" PostId="168" RevisionGUID="f853637c-57f3-4079-a115-e8b5f7fce109" CreationDate="2013-03-01T19:58:59.800" UserId="136" Text="Addressing this problem was a core design driver for the Curator's Workbench software at UNC. We needed to allow re-arrangement while leaving all original sources intact, i.e. read only. It's done by capturing the structure in a METS manifest; all subsequent manipulation is only editing the manifest and not the data.&#xD;&#xA;&#xD;&#xA;This also has the benefit of allowing staging and checksums to proceed in the background, while you perform appraisal and arrangement.&#xD;&#xA;&#xD;&#xA;For more information:&#xD;&#xA;http://www.lib.unc.edu/blogs/cdr/index.php/about-the-curators-workbench/" />
  <row Id="407" PostHistoryTypeId="4" PostId="151" RevisionGUID="1b52fbef-290e-4525-ba62-7377c630c03b" CreationDate="2013-03-01T20:26:00.617" UserId="119" Comment="edited title" Text="What disk image format for use vs preservation" />
  <row Id="408" PostHistoryTypeId="2" PostId="169" RevisionGUID="62a5b2ca-81ef-40d0-91c7-156b24b62879" CreationDate="2013-03-01T20:33:49.277" UserId="136" Text="One has to assume that in a preservation repository the external datastreams are also managed in some sense. So the risk will depend upon how well they are managed. A local solution always implies that you sustain local knowledge of the system and so there is some degree of risk in that. In contrast, the Fedora system knowledge is sustained by the DuraSpace organization and user community.&#xD;&#xA;&#xD;&#xA;In general there is more likely to be a complete and accurate audit trail for managed data streams, since changes come in via the Fedora APIM. Timestamps in Fedora will match up with when the data was written to storage. If configured properly, Fedora will calculate checksums whenever the data stream is written. The data will also be protected by the Fedora security layer; for instance delete may be forbidden by FeSL policies. External data streams will have to be protected by some other means.&#xD;&#xA;&#xD;&#xA;One disadvantage of managed datastreams is the ingest process. If Fedora manages the data, then Fedora must stream the data into storage. The data moves through the Fedora server. Unless you enhance the Fedora ExternalContentManager module, this means data is uploaded to the Fedora server over HTTP or HTTPS. For larger data this is a problem. Hopefully this constraint will go away at some point, allowing for other ingest strategies. There are workarounds. At UNC we extended the ExternalContentManager to support ingest from iRODS locations.&#xD;&#xA;&#xD;&#xA;Having to physically move the data, as opposed to a logical move, seems like a risk to me. It necessitates a checksum comparison, one calculated before ingest and one calculated by Fedora." />
  <row Id="409" PostHistoryTypeId="16" PostId="169" RevisionGUID="240c7f52-645b-4689-bcc4-4ffd6297caa7" CreationDate="2013-03-01T20:33:49.277" UserId="136" />
  <row Id="410" PostHistoryTypeId="2" PostId="170" RevisionGUID="fe2951a8-b495-46ab-b1fb-df9c3c5e24ed" CreationDate="2013-03-01T22:36:08.157" UserId="97" Text="I cannot comment on the specific products mentioned, but I do have some general thoughts on the issues raised in the question.&#xD;&#xA;&#xD;&#xA;&gt; 1) What image format should I be dumping if my intent is to use these images within emulators such as Sheepshaver, Basilisk II, or Mini vMac?&#xD;&#xA;&#xD;&#xA;Most emulators want simple byte reads of the media in question. On Unix and Unix-like systems, this is usually achieved using the `dd` tool which ships with the OS, although other options do exist; on Windows, separate imaging tools are normally needed.&#xD;&#xA;&#xD;&#xA;Barring such difficulties as inter-sector copy protection measures (which I believe were used to some extent at one point, although I don't know if they were employed on the Macintosh), this gives you a copy of all the data on the media that the original operating system would be able to access. Boot sector, file system metadata, recorded-as-unused blocks, everything gets copied.&#xD;&#xA;&#xD;&#xA;Depending on one's needs, knowledge of the file system used could then be used to extract files and folders from the media image if that is sufficient for preservation. If you are trying to preserve the *payload data* rather than the exact data and metadata structures on the disk, and if it can be done either losslessly or with acceptable losses, it makes sense to do so to not just image the disks but also transform the data into a more accessible format.&#xD;&#xA;&#xD;&#xA;&gt; 2) Is this image format I will be dumping for use purposes also sufficient for long term preservation? Should I also be dumping a format that preserves lower level information?&#xD;&#xA;&#xD;&#xA;In almost all cases (see caveat about copy protection above), I don't see any real use for any lower level information than what can be obtained by making a byte-by-byte read and copy of the media. You would then be looking at specialized tools to read (assuming magnetic media such as floppies; optical media such as CDs would be different, and solid-state media would be a whole different ballgame) the magnetic flux variations in the physical space *between* sectors. Such reads *could*, in principle and when properly analyzed, reveal traces of previously overwritten data, but they could just as well simply indicate drive read/write head misalignment. No two floppy disk drives have read/write heads which are *perfectly* aligned with each other, so some variance in the physical location of the written data is to be expected. Normally the read head(s) simply detect whatever signal happens to be strongest, since the difference between the desired and old signals will be orders of magnitude.&#xD;&#xA;&#xD;&#xA;The hard part about copying old Apple media in particular, such as old Macintosh or Apple II/III floppy disks, is that depending on the media size, [some of them use GCR encoding](http://digitalpreservation.stackexchange.com/a/139/97) rather than the MFM encoding which replaced it. Sourcing a working floppy disk drive today which can read GCR *and* can interface with modern hardware might be difficult, so a multi-stage approach might be needed (first transfer a binary image of the media onto a system which can natively read GCR floppies, then transfer that image by some other means onto a more modern system for further use)." />
  <row Id="411" PostHistoryTypeId="5" PostId="170" RevisionGUID="f54b6ecc-12ae-42d1-b79f-b9b75969cd63" CreationDate="2013-03-01T22:48:56.263" UserId="97" Comment="deleted 9 characters in body" Text="I cannot comment on the specific products mentioned, but I do have some general thoughts on the issues raised in the question.&#xD;&#xA;&#xD;&#xA;&gt; 1) What image format should I be dumping if my intent is to use these images within emulators such as Sheepshaver, Basilisk II, or Mini vMac?&#xD;&#xA;&#xD;&#xA;Most emulators want simple byte reads of the media in question. On Unix and Unix-like systems, this is usually achieved using the `dd` tool which ships with the OS, although other options do exist; on Windows, separate imaging tools are normally needed.&#xD;&#xA;&#xD;&#xA;Barring such difficulties as inter-sector copy protection measures (which I believe were used to some extent at one point, although I don't know if they were employed on the Macintosh), this gives you a copy of all the data on the media that the original operating system would be able to access. Boot sector, file system metadata, recorded-as-unused blocks, everything gets copied.&#xD;&#xA;&#xD;&#xA;Depending on one's needs, knowledge of the file system used could then be used to extract files and folders from the media image if that is sufficient for preservation. If you are trying to preserve the *payload data* rather than the exact data and metadata structures on the disk, and if it can be done either losslessly or with acceptable losses, it makes sense to not just image the disks but also transform the data into a more accessible format. The raw disk images could still be retained, but used for secondary access should any specific need arise.&#xD;&#xA;&#xD;&#xA;&gt; 2) Is this image format I will be dumping for use purposes also sufficient for long term preservation? Should I also be dumping a format that preserves lower level information?&#xD;&#xA;&#xD;&#xA;In almost all cases (see caveat about copy protection above), I don't see any real use for any lower level information than what can be obtained by making a byte-by-byte read and copy of the media. You would then be looking at specialized tools to read (assuming magnetic media such as floppies; optical media such as CDs would be different, and solid-state media would be a whole different ballgame) the magnetic flux variations in the physical space *between* sectors. Such reads *could*, in principle and when properly analyzed, reveal traces of previously overwritten data, but they could just as well simply indicate drive read/write head misalignment. No two floppy disk drives have read/write heads which are *perfectly* aligned with each other other than by pure chance, so some variance in the physical location of the written data is to be expected. Normally the read head(s) simply detect whatever signal happens to be strongest, since the difference between the desired and old signals will be orders of magnitude.&#xD;&#xA;&#xD;&#xA;The hard part about copying old Apple media in particular, such as old Macintosh or Apple II/III floppy disks, is that depending on the media size, [some of them use GCR encoding](http://digitalpreservation.stackexchange.com/a/139/97) rather than the MFM encoding which eventually replaced it. Sourcing a working floppy disk drive today which can read GCR *and* can interface with modern hardware might be difficult, so a multi-stage approach might be needed: first transfer a binary image of the media onto a system which can natively read GCR floppies, then transfer that image by some other means onto a more modern system for further use." />
  <row Id="413" PostHistoryTypeId="2" PostId="171" RevisionGUID="95921c02-658b-4e52-b5ad-456b17a0ec50" CreationDate="2013-03-02T03:31:06.750" UserId="114" Text="We are looking at what it takes to become a trusted digital repository (TDR)&#xD;&#xA;&#xD;&#xA;I have gone through the ISO standard and there is a lot of work and money to gain this certification&#xD;&#xA;&#xD;&#xA;So I have a couple of questions&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; 1. is it worthwhile doing the certification - some institutions who have&#xD;&#xA;    failed the test have indicated that it is of no value - not sure if&#xD;&#xA;    this is a case of 'sour grapes'&#xD;&#xA;    &#xD;&#xA;   &#xD;&#xA; 2. if we were certified would other institutions be willing to place&#xD;&#xA;    their archives into the TDR&#xD;&#xA;&#xD;&#xA;Your collective thoughts would be much appreciated&#xD;&#xA;" />
  <row Id="414" PostHistoryTypeId="1" PostId="171" RevisionGUID="95921c02-658b-4e52-b5ad-456b17a0ec50" CreationDate="2013-03-02T03:31:06.750" UserId="114" Text="Trauted Digital Repository" />
  <row Id="415" PostHistoryTypeId="3" PostId="171" RevisionGUID="95921c02-658b-4e52-b5ad-456b17a0ec50" CreationDate="2013-03-02T03:31:06.750" UserId="114" Text="&lt;tdr&gt;&lt;digiatl&gt;&lt;repository&gt;&lt;certification&gt;" />
  <row Id="416" PostHistoryTypeId="10" PostId="130" RevisionGUID="cd13be31-cae6-4873-8d23-ab7c8890af1c" CreationDate="2013-03-02T03:45:41.913" UserId="-1" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:97,&quot;DisplayName&quot;:&quot;Michael Kjörling&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;mopennock&quot;},{&quot;Id&quot;:43,&quot;DisplayName&quot;:&quot;jcmeloni&quot;},{&quot;Id&quot;:42,&quot;DisplayName&quot;:&quot;Donald.McLean&quot;}]}" />
  <row Id="417" PostHistoryTypeId="2" PostId="172" RevisionGUID="46b0b06f-2497-4833-b4dc-f470bd572a2b" CreationDate="2013-03-02T09:45:56.310" UserId="105" Text="Anyone can upload anything to the Internet Archive, independent of the Wayback Machine. The Wayback Machine is hard or impossible to search; you really need to know the url of the content you want to access. On the other hand, regular items in the archive can have arbitrary metadata and are much easier to search for.&#xD;&#xA;&#xD;&#xA;Aside from that, a simple webpage is not very expensive, and will tend to be more authoritative than an item that is merely in an archive." />
  <row Id="418" PostHistoryTypeId="2" PostId="173" RevisionGUID="fed717de-a664-4797-b6de-5f65f8ae7ab8" CreationDate="2013-03-02T09:53:14.137" UserId="105" Text="Why not hedge your bets by storing the originals and converting to other formats automatically? It seems unlikely that the extra storage requirements would be prohibitive.&#xD;&#xA;&#xD;&#xA;You could also do the future a favor and store pre-rendered maps with the tracks highlighted, so that after civilization falls our descendants will be able to reverse-engineer the WGS84 datum, allowing them to understand all that they have lost." />
  <row Id="420" PostHistoryTypeId="6" PostId="151" RevisionGUID="a96b7f31-e565-4385-8d1d-f572cfdc6ed7" CreationDate="2013-03-02T11:52:29.630" UserId="97" Comment="edited tags" Text="&lt;floppy-disk&gt;&lt;disk-image&gt;&lt;kryoflux&gt;&lt;computer-emulation&gt;" />
  <row Id="421" PostHistoryTypeId="2" PostId="174" RevisionGUID="e768ee48-c29a-47db-906c-742255acd14e" CreationDate="2013-03-02T13:43:23.833" UserId="31" Text="Another excellent option is to use [the Arxiv pre-print server](http://arxiv.org/), which is a great way to get papers online with appropriate metadata (if they have a suitable category that applies to your work)." />
  <row Id="422" PostHistoryTypeId="2" PostId="175" RevisionGUID="83aef7f4-bfed-4bb7-9589-b010ec7a94af" CreationDate="2013-03-02T13:56:39.957" UserId="31" Text="For the web archives I work with, we use [Apache Tika](http://tika.apache.org/) to extract properties of interest along with text (for search indexing) along with a [few extensions of our own](https://github.com/ukwa/warc-discovery). This works well from Java and on streamed data, which suites our HDFS-hosted WARC files very well." />
  <row Id="423" PostHistoryTypeId="4" PostId="171" RevisionGUID="e3aaa7f9-54a5-4ade-b408-9a12815dfbba" CreationDate="2013-03-02T15:03:58.480" UserId="42" Comment="Corrected typo in title" Text="Trusted Digital Repository" />
  <row Id="424" PostHistoryTypeId="24" PostId="171" RevisionGUID="e3aaa7f9-54a5-4ade-b408-9a12815dfbba" CreationDate="2013-03-02T15:03:58.480" Comment="Proposed by 42 approved by 101 edit id of 16" />
  <row Id="425" PostHistoryTypeId="2" PostId="176" RevisionGUID="d18cfdff-6054-4c3e-bdce-f37a76425065" CreationDate="2013-03-02T15:39:43.033" UserId="29" Text="The digital archive certification world is a bit of a mess right now. Too many acronyms and ISO standards and they have too similar of names.&#xD;&#xA;&#xD;&#xA;At this moment, no one is certified to certify a repository as a TDR, also known as ISO 16363. First, we need to establish a standard for certifying the auditors. This is in draft form as ISO 16919. The current roadblock is that the standard needs to define the distribution of auditor certifying organizations. National? Regional? Supranational? It's still in debate.&#xD;&#xA;&#xD;&#xA;In the meantime, we have the provisional auditing scheme based on OAIS/ISO 14721 known as TRAC  that served as the basis for TDR/ISO 16363. Only the Center for Research Libraries (CRL) is certified to use TRAC. As far as I understand, CRL is auditing repositories using TDR/ISO 16363 since the standard has been accepted but assigning a TRAC label to them since they are not certified to give TDR certifications.&#xD;&#xA;&#xD;&#xA;1) Is certification worth it? I would say it depends on the size of the institution. It is expensive, but having a third-party evaluate the current preservation capability of a system is essential. Certification should measure the current effectiveness of administrative, technical, and staff capabilities in a more objective manner. As organizations grow in size and budget, it becomes more difficult to keep an objective view of all of these factors. If you work through the self-auditing versions of TRAC and TDR/ISO 16363, you'll see that's it's an immense amount of work and very difficult to remain objective with every metric.&#xD;&#xA;&#xD;&#xA;2) Would you like to position your archive as a digital repository for institutions that lack the infrastructure? Chronopolis certainly does this for research data, and it uses it's TRAC certification as a seal of approval. However, I think certification itself will not automatically attract clients. It's most effective as a feature to distinguish one repository from another." />
  <row Id="426" PostHistoryTypeId="2" PostId="177" RevisionGUID="cdd566a1-7229-4784-9157-c5bce3312a02" CreationDate="2013-03-02T15:43:16.010" UserId="76" Text="I'm increasingly seeing listings for &quot;Digital Archivists&quot; at a range of cultural heritage organizations and many of them look very different from each other. &#xD;&#xA;&#xD;&#xA;As academic programs increasingly try to prepare students for careers as digital archivists it would be helpful to have a sense of what the qualifications they should be developing are. With that said, I've seen listings for digital archivists that: &#xD;&#xA;&#xD;&#xA; - [want people with master's degrees in computer science][1]&#xD;&#xA; - [look much more like digital asset manager jobs][2] and are primarily&#xD;&#xA;   focused on organizing digital assets and doing things like running&#xD;&#xA;   content management systems.&#xD;&#xA; - [want the candidate to have a PhD][3].&#xD;&#xA; - [focus on archival description and are looking for people with an MA&#xD;&#xA;   or MILS][4]&#xD;&#xA; - include varying degrees of technical skills ([in this case MySQL and&#xD;&#xA;   Archivematica][5])&#xD;&#xA;&#xD;&#xA;So, what should the general qualifications for a digital archivist job be in terms of training, credentials and technical chops?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://jobs.code4lib.org/job/3519/&#xD;&#xA;  [2]: http://jobs.code4lib.org/job/4167/&#xD;&#xA;  [3]: http://jobs.code4lib.org/job/1472/&#xD;&#xA;  [4]: http://jobs.code4lib.org/job/342/&#xD;&#xA;  [5]: http://jobs.code4lib.org/job/4702/" />
  <row Id="427" PostHistoryTypeId="1" PostId="177" RevisionGUID="cdd566a1-7229-4784-9157-c5bce3312a02" CreationDate="2013-03-02T15:43:16.010" UserId="76" Text="What should be the qualifications for digital archivist jobs?" />
  <row Id="428" PostHistoryTypeId="3" PostId="177" RevisionGUID="cdd566a1-7229-4784-9157-c5bce3312a02" CreationDate="2013-03-02T15:43:16.010" UserId="76" Text="&lt;education&gt;&lt;training&gt;&lt;digital-archivist&gt;&lt;jobs&gt;" />
  <row Id="429" PostHistoryTypeId="10" PostId="138" RevisionGUID="1d697a29-04c6-4152-9e4f-ae4fb14d2332" CreationDate="2013-03-02T15:43:43.163" UserId="-1" CloseReasonId="3" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:43,&quot;DisplayName&quot;:&quot;jcmeloni&quot;},{&quot;Id&quot;:26,&quot;DisplayName&quot;:&quot;Dmitry Brant&quot;},{&quot;Id&quot;:68,&quot;DisplayName&quot;:&quot;Adam-E&quot;},{&quot;Id&quot;:29,&quot;DisplayName&quot;:&quot;Nick Krabbenhoeft&quot;}]}" />
  <row Id="431" PostHistoryTypeId="5" PostId="176" RevisionGUID="f95ddf6f-715c-4908-8d52-ea5e66991b26" CreationDate="2013-03-02T16:04:17.900" UserId="29" Comment="added 483 characters in body" Text="The digital archive certification world is a bit of a mess right now. Too many acronyms and ISO standards and they have too similar of names.&#xD;&#xA;&#xD;&#xA;At this moment, no one is certified to certify a repository as a TDR, also known as [ISO 16363](http://www.iso16363.org/news/iso-16363-2012-now-published-on-iso-site-as-international-standard/). First, we need to establish a standard for certifying the auditors. This is in draft form as [ISO 16919](http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=57950). The current roadblock is that the standard needs to define the distribution of auditor certifying organizations. National? Regional? Supranational? It's still in debate.&#xD;&#xA;&#xD;&#xA;In the meantime, we have the provisional auditing scheme based on OAIS/ISO 14721 known as [TRAC](http://www.crl.edu/archiving-preservation/digital-archives/metrics-assessing-and-certifying)  that served as the basis for TDR/ISO 16363. Only the Center for Research Libraries (CRL) is certified to use TRAC. As far as I understand, CRL is auditing repositories using TDR/ISO 16363 since the standard has been accepted but assigning a TRAC label to them since they are not certified to give TDR certifications.&#xD;&#xA;&#xD;&#xA;1) Is certification worth it? I would say it depends on the size of the institution. It is expensive, but having a third-party evaluate the current preservation capability of a system is essential. Certification should measure the current effectiveness of administrative, technical, and staff capabilities in a more objective manner. As organizations grow in size and budget, it becomes more difficult to keep an objective view of all of these factors. If you work through the self-auditing versions of TRAC and [TDR/ISO 16363](http://www.iso16363.org/assets/Self-AssessmentTemplateforISO16363.xls), you'll see that's it's an [immense amount of work](http://www.iso16363.org/preparing-for-an-audit/) and very difficult to remain objective with every metric.&#xD;&#xA;&#xD;&#xA;2) Would you like to position your archive as a digital repository for institutions that lack the infrastructure? [Chronopolis](http://chronopolis.sdsc.edu/) certainly does this for research data, and it uses it's TRAC certification as a seal of approval (it's the first link in their menu structure). However, I think certification itself will not automatically attract clients. It's most effective as a feature to distinguish one repository from another." />
  <row Id="432" PostHistoryTypeId="2" PostId="178" RevisionGUID="e65b93cf-53ca-4fe3-80d8-0b7c79923e5b" CreationDate="2013-03-02T16:42:19.293" UserId="133" Text="I'm skeptical that a full-blown certification regime will ever get much traction because it's expensive, and more importantly, it's unclear that becoming certified provides much in the way of return. &#xD;&#xA;&#xD;&#xA;The original idea behind TDR a decade ago was that data users would want hard proof that repositories were doing the right thing. Speculation was that, absent such proof, repositories would fall short of their mission to support research. Is there current evidence for this hypotheses? Are users really that worried about what repositories are doing? Are  institutions reluctant to transfer data on this basis? Sure, certification is a plus, but how much of one? If it really conveyed a major advantage, wouldn't the process have moved ahead faster over the last 10 years?&#xD;&#xA;&#xD;&#xA;There is a need for model guidance on developing and improving repository practices, and there are some good candidates for this purpose. The Data Seal of Approval, created by the [Data Archiving and Networked Services][1] (DANS) archive in The Netherlands is one (site is currently down, ironically). The &quot;seal of approval&quot; is granted after a repository runs through a self-assessment of 16 basic guidelines. The process is much less intensive (and expensive) than TDR certification. And it does grant third-party approval in cases where that's needed.&#xD;&#xA;&#xD;&#xA;Another potential source of model guidance is simpler still: the [Levels of Digital Preservation][2] developed collaboratively through the National Digital Stewardship Alliance. This guidance (still in beta) offers four levels of activity, from basic to advanced, for major preservation functions, such as storage and geographic location, file fixity and file formats. This isn't the be all and end all, but it provides an excellent way for an institution to assess it's practices and chart a path toward improvement.&#xD;&#xA;&#xD;&#xA;Efforts such as these should be extended and improved with an eye to helping institutions make cost-effective, risk-based decisions about digital preservation practices.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datasealofapproval.org/?q=node/12&#xD;&#xA;  [2]: http://blogs.loc.gov/digitalpreservation/2012/11/ndsa-levels-of-digital-preservation-release-candidate-one/" />
  <row Id="433" PostHistoryTypeId="2" PostId="179" RevisionGUID="d0b90486-d09d-46e2-9874-2521e508091c" CreationDate="2013-03-02T16:47:04.250" UserId="156" Text=" I collect computers and old games. What hardware consumer affordable devices exists in the market for preserving media? I know of the Kryoflux, which can preserve certain disks, but what more exists?" />
  <row Id="434" PostHistoryTypeId="1" PostId="179" RevisionGUID="d0b90486-d09d-46e2-9874-2521e508091c" CreationDate="2013-03-02T16:47:04.250" UserId="156" Text="What hardware consumer devices exists for digital preservation?" />
  <row Id="435" PostHistoryTypeId="3" PostId="179" RevisionGUID="d0b90486-d09d-46e2-9874-2521e508091c" CreationDate="2013-03-02T16:47:04.250" UserId="156" Text="&lt;future-proofing&gt;&lt;hardware&gt;&lt;kryoflux&gt;" />
  <row Id="436" PostHistoryTypeId="2" PostId="180" RevisionGUID="919397c2-fd82-40dd-97ca-8192ab325e0b" CreationDate="2013-03-02T17:00:47.220" UserId="133" Text="Trying to answer this question means aiming at a moving target, as the field is still emergent. The best recent source I've seen is [Digital Curation in the Academic Library Job Market][1] (PDF) by Jeonghyun Kim and others. They reviewed 100+ job announcements &quot;to identify competencies required of individuals working in the digital curation field.&quot; Quickly summarizing, they found 85% of jobs called for a MLS and 66% for specific curatorial experience. The most common skills and knowledge called for were: &#xD;&#xA;&#xD;&#xA; - Working in an Information Technology Intensive Environment (knowledge of multiple&#xD;&#xA;   operating systems and web architectures, etc.)&#xD;&#xA; - Standards and Specifications (MARC, Dublin Core, METS, MODS, PREMIS, etc.)&#xD;&#xA; - Project Management &#xD;&#xA; - Personal and Interpersonal Skills&#xD;&#xA;&#xD;&#xA;These findings relate to only a section of the field, but they seem reasonable in a broad, current sense. &#xD;&#xA;&#xD;&#xA;  [1]: https://www.asis.org/asist2012/proceedings/Submissions/283.pdf" />
  <row Id="437" PostHistoryTypeId="2" PostId="181" RevisionGUID="0d0538dc-d2f1-4ad2-915f-0c92723213b4" CreationDate="2013-03-02T17:06:36.567" UserId="93" Text="A few prototypes have been hacked together at [SPRUCE mashups][1] along these lines and, like Andy's work with the web archive, have used Apache Tika. This one seems more pertinent - [Extracting and aggregating metadata with Apache Tika][2]. The datasets in question were mainly text and PDF. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://wiki.opf-labs.org/display/SPR/SPRUCE+Events&#xD;&#xA;  [2]: http://wiki.opf-labs.org/display/SPR/Extracting+and+aggregating+metadata+with+Apache+Tika" />
  <row Id="438" PostHistoryTypeId="5" PostId="179" RevisionGUID="e6fd15e0-88c6-4cba-9ff8-4fc834176f57" CreationDate="2013-03-02T18:52:32.787" UserId="156" Comment="added 15 characters in body" Text=" I collect computers and old games. What hardware consumer affordable devices exists in the market for preserving floppy and tapemedia? I know of the Kryoflux, which can preserve certain floppydisks, but what more exists?" />
  <row Id="439" PostHistoryTypeId="5" PostId="178" RevisionGUID="d33d7864-dab2-4b70-b055-be24ff64a122" CreationDate="2013-03-02T20:04:14.737" UserId="133" Comment="edited body" Text="I'm skeptical that a full-blown certification regime will ever get much traction because it's expensive, and more importantly, it's unclear that becoming certified provides much in the way of return. &#xD;&#xA;&#xD;&#xA;The original idea behind TDR a decade ago was that data users would want hard proof that repositories were doing the right thing. Speculation was that, absent such proof, repositories would fall short of their mission to support research. Is there current evidence for this hypothesis? Are users really that worried about what repositories are doing? Are  institutions reluctant to transfer data on this basis? Sure, certification is a plus, but how much of one? If it really conveyed a major advantage, wouldn't the process have moved ahead faster over the last 10 years?&#xD;&#xA;&#xD;&#xA;There is a need for model guidance on developing and improving repository practices, and there are some good candidates for this purpose. The Data Seal of Approval, created by the [Data Archiving and Networked Services][1] (DANS) archive in The Netherlands is one (site is currently down, ironically). The &quot;seal of approval&quot; is granted after a repository runs through a self-assessment of 16 basic guidelines. The process is much less intensive (and expensive) than TDR certification. And it does grant third-party approval in cases where that's needed.&#xD;&#xA;&#xD;&#xA;Another potential source of model guidance is simpler still: the [Levels of Digital Preservation][2] developed collaboratively through the National Digital Stewardship Alliance. This guidance (still in beta) offers four levels of activity, from basic to advanced, for major preservation functions, such as storage and geographic location, file fixity and file formats. This isn't the be all and end all, but it provides an excellent way for an institution to assess it's practices and chart a path toward improvement.&#xD;&#xA;&#xD;&#xA;Efforts such as these should be extended and improved with an eye to helping institutions make cost-effective, risk-based decisions about digital preservation practices.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datasealofapproval.org/?q=node/12&#xD;&#xA;  [2]: http://blogs.loc.gov/digitalpreservation/2012/11/ndsa-levels-of-digital-preservation-release-candidate-one/" />
  <row Id="440" PostHistoryTypeId="2" PostId="182" RevisionGUID="91ee06cd-1d18-4f4a-acbb-886d88f28941" CreationDate="2013-03-02T20:11:19.830" UserId="133" Text="This could include emulation of applications, operating systems and hardware platforms." />
  <row Id="441" PostHistoryTypeId="1" PostId="182" RevisionGUID="91ee06cd-1d18-4f4a-acbb-886d88f28941" CreationDate="2013-03-02T20:11:19.830" UserId="133" Text="What are the advantages and disadvantages of emulation as a digital preservation strategy?" />
  <row Id="442" PostHistoryTypeId="3" PostId="182" RevisionGUID="91ee06cd-1d18-4f4a-acbb-886d88f28941" CreationDate="2013-03-02T20:11:19.830" UserId="133" Text="&lt;computer-emulation&gt;" />
  <row Id="443" PostHistoryTypeId="2" PostId="183" RevisionGUID="7bf674d9-8a68-44ab-b184-37e6e88558ba" CreationDate="2013-03-02T20:20:33.910" UserId="133" Text="To what extent (if any) are actions related to metadata, fixity, storage, etc., required to achieve the lowest threshold of preservation?" />
  <row Id="444" PostHistoryTypeId="1" PostId="183" RevisionGUID="7bf674d9-8a68-44ab-b184-37e6e88558ba" CreationDate="2013-03-02T20:20:33.910" UserId="133" Text="What are the bare minimal steps needed to consider digital content preserved?" />
  <row Id="445" PostHistoryTypeId="3" PostId="183" RevisionGUID="7bf674d9-8a68-44ab-b184-37e6e88558ba" CreationDate="2013-03-02T20:20:33.910" UserId="133" Text="&lt;file-fixity&gt;&lt;archival-material&gt;&lt;ingest&gt;" />
  <row Id="446" PostHistoryTypeId="6" PostId="6" RevisionGUID="3789d2c3-ddc4-4b42-956a-424a808712f9" CreationDate="2013-03-02T20:26:09.990" UserId="133" Comment="edited tags" Text="&lt;out-of-date-format&gt;&lt;data-recovery&gt;&lt;forensics&gt;&lt;storage-media&gt;&lt;record-modernization&gt;" />
  <row Id="447" PostHistoryTypeId="6" PostId="17" RevisionGUID="525c4cca-8470-4776-a429-36001dceed8c" CreationDate="2013-03-02T20:27:37.680" UserId="133" Comment="edited tags" Text="&lt;digitize&gt;&lt;file-formats&gt;&lt;scanner&gt;&lt;photos&gt;" />
  <row Id="449" PostHistoryTypeId="6" PostId="64" RevisionGUID="fb93082e-076d-4e2f-965c-7a80bc313bc0" CreationDate="2013-03-02T20:32:21.840" UserId="133" Comment="edited tags" Text="&lt;file-formats&gt;&lt;publishing&gt;&lt;pdf&gt;" />
  <row Id="450" PostHistoryTypeId="6" PostId="8" RevisionGUID="46b81201-4001-4f81-b533-3e4588141ad5" CreationDate="2013-03-02T20:34:24.197" UserId="133" Comment="edited tags" Text="&lt;media&gt;&lt;forensics&gt;&lt;imaging&gt;&lt;file-format-migration&gt;" />
  <row Id="451" PostHistoryTypeId="6" PostId="8" RevisionGUID="32d64828-4903-47fb-8aad-f3b98df50964" CreationDate="2013-03-02T20:34:24.243" UserId="133" Comment="edited tags" Text="&lt;media&gt;&lt;forensics&gt;&lt;file-format-migration&gt;&lt;imaging&gt;" />
  <row Id="452" PostHistoryTypeId="2" PostId="184" RevisionGUID="76808354-89a0-444a-ad1b-8f63a748510e" CreationDate="2013-03-02T21:00:50.617" UserId="133" Text="Necessary in the sense of potentially needing to extract additional information from the physical object." />
  <row Id="453" PostHistoryTypeId="1" PostId="184" RevisionGUID="76808354-89a0-444a-ad1b-8f63a748510e" CreationDate="2013-03-02T21:00:50.617" UserId="133" Text="Is it necessary to save original media after all content has been copied from them?" />
  <row Id="454" PostHistoryTypeId="3" PostId="184" RevisionGUID="76808354-89a0-444a-ad1b-8f63a748510e" CreationDate="2013-03-02T21:00:50.617" UserId="133" Text="&lt;storage-media&gt;&lt;forensics&gt;" />
  <row Id="457" PostHistoryTypeId="6" PostId="123" RevisionGUID="b81f940f-1480-4a68-a8b6-4097dbd7447e" CreationDate="2013-03-02T22:02:55.147" UserId="68" Comment="edited tags" Text="&lt;file-format-migration&gt;&lt;vintage-computing&gt;&lt;data-conversion&gt;&lt;abandonware&gt;" />
  <row Id="459" PostHistoryTypeId="2" PostId="185" RevisionGUID="2d0d12aa-7f0d-417b-8017-70684246a160" CreationDate="2013-03-02T22:26:35.047" UserId="80" Text="Facebook offers an &quot;archive&quot; function from Settings &gt; Account Settings &gt; Download a Copy of Your Facebook Data. I've never started a group on Facebook. Perhaps if it is a group you started it is available for download? " />
  <row Id="460" PostHistoryTypeId="2" PostId="186" RevisionGUID="79e775de-f11e-4eb4-8241-ea72904a5aa1" CreationDate="2013-03-02T22:40:51.527" UserId="-1" Text="" />
  <row Id="461" PostHistoryTypeId="2" PostId="187" RevisionGUID="a603232f-473a-4e05-8a1b-06e950f3e2b0" CreationDate="2013-03-02T22:40:51.527" UserId="-1" Text="" />
  <row Id="462" PostHistoryTypeId="2" PostId="188" RevisionGUID="ab815157-1c13-40a0-af84-cbf9f170b46e" CreationDate="2013-03-03T00:35:26.467" UserId="109" Text="As someone who has held &quot;digital archivist&quot; positions in academic archives and special collections, I would say a firm understanding of archival principles and experience with a wide range of archival skills is the sine qua non requirement of any archivist.&#xD;&#xA;&#xD;&#xA;The approach necessary to properly handle archival materials in physical and digital form are, at a high-level, not substantially different. Some of the unique skills that make archivists well-suited to contemporary positions that require managing digital as well as physical materials are:&#xD;&#xA;&#xD;&#xA; - knowledge of XML and XSLT (sad but true);&#xD;&#xA; - knowledge of at least one scripting language for data janitorial work;&#xD;&#xA; - willingness to learn and implement new technologies, ask questions, and engage with the archival and technical community;&#xD;&#xA; - knowledge of IA and UX principles.&#xD;&#xA;&#xD;&#xA;Anything else depends on the specific needs of the institution and can include application development, digital repository administration, masters degree in the humanities, etc...&#xD;&#xA;&#xD;&#xA;" />
  <row Id="463" PostHistoryTypeId="2" PostId="189" RevisionGUID="e18280e1-1799-4a52-b381-7916c93026ac" CreationDate="2013-03-03T02:01:03.677" UserId="26" Text="The title of the question contradicts the body of the question... if it's possible to &quot;extract additional information&quot; from the media, then you haven't copied all possible content from the media!&#xD;&#xA;&#xD;&#xA;To correctly preserve digital media would mean having an exact binary image of said media, which by definition contains *all* the data stored on it. As a bonus, we could save a photo of the media in addition to the binary image.&#xD;&#xA;&#xD;&#xA;After a proper binary image has been saved, the only reason to keep the original media would be for historical or nostalgia purposes, since the media would never need to be **accessed** again.&#xD;&#xA;&#xD;&#xA;For example, I still have an original Zork I floppy disk for Commodore, even though I have an image of it that I use with an emulator. I'll be sad if I ever lose the original disk, but I'll always have the disk in virtual form, and I'll never need to access the original disk again.&#xD;&#xA;&#xD;&#xA;Generally I would argue that *it's the data* that is important, not the medium on which it's stored. As long as the data makes it onto the most modern medium, the old medium can be discarded." />
  <row Id="464" PostHistoryTypeId="5" PostId="189" RevisionGUID="9f19ede5-768c-4771-8dd5-8bab981442d4" CreationDate="2013-03-03T02:19:52.773" UserId="26" Comment="deleted 199 characters in body" Text="To correctly preserve digital media would mean having an exact binary image of said media, which by definition contains *all* the data stored on it. As a bonus, we could save a photo of the media in addition to the binary image.&#xD;&#xA;&#xD;&#xA;After a proper binary image has been saved, the only reason to keep the original media would be for historical or nostalgia purposes, since the media would never need to be **accessed** again.&#xD;&#xA;&#xD;&#xA;For example, I still have an original Zork I floppy disk for Commodore, even though I have an image of it that I use with an emulator. I'll be sad if I ever lose the original disk, but I'll always have the disk in virtual form, and I'll never need to access the original disk again.&#xD;&#xA;&#xD;&#xA;Generally I would argue that *it's the data* that is important, not the medium on which it's stored. As long as the data makes it onto the most modern medium, the old medium can be discarded." />
  <row Id="466" PostHistoryTypeId="2" PostId="190" RevisionGUID="a38084de-c357-48b3-91ed-02a3bab228f2" CreationDate="2013-03-03T11:11:43.037" UserId="57" Text="Preservation of software, as opposed to digital media, is an entirely different ball of wax.  Part of it may be driven by the need to view stored media that only a specific application or operating system can understand.  But, you could argue it would be useful to preserve the application, platform or operating system, just for historical reasons.  &#xD;&#xA;&#xD;&#xA;You could argue that `abandonware` fits this case.  There are people who want to retain the ability to play old games, and sometimes run old applications, but usually companies do not sell the game anymore, and/or don't support them with newer operating systems.  This brings with it entirely different concerns around legality of distributing old software, even if it's not commercially available anymore .  While some companies have gone out of business, the IP usually still belongs to somebody out there, &#xD;&#xA;&#xD;&#xA;You might also ask yourself, if it's worth it to actually keep old software around in a runnable state.  Why would you want to run WordStar 4.0 these days, for example?  Maybe if you're really nostalgic about old word processors, but otherwise, probably only of interest to a computer museum.  Any data that could have been useful in those old platforms and/or applications has probably already been migrated to newer platforms, so there is no need to keep the old software around." />
  <row Id="467" PostHistoryTypeId="5" PostId="34" RevisionGUID="6ee49298-6311-4648-b672-e319e6950a81" CreationDate="2013-03-03T11:23:57.973" UserId="91" Comment="added JHOVE" Text="I have not seen these file extensions before. On the other hand, there are other ways to find out what a file contains. Unixoid systems such as Linux and Mac OS X come with a little tool called `file` which can probe the first bytes of any file and then make an educated guess about the format it contains. To do that, it uses a database of characteristic patterns which is usually located in `/usr/share/misc/magic`. The magic database on my computer contains 11,543 non-comment lines, so I guess it can detect more than 10,000 file formats. There is a good chance it will be able to identify your file formats.&#xD;&#xA;&#xD;&#xA;The Java world offers several more format identification tools: [JHOVE2][1] is a framework for “format-aware characterization of digital objects”, [Tika][2] calls itself a “content analysis toolkit”, and a slightly dated alternative is called [DROID][3] for “Digital Record and Object Identification”.&#xD;&#xA;&#xD;&#xA;All of these tools are free and open-source software.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://jhove2.org/&#xD;&#xA;  [2]: https://tika.apache.org/&#xD;&#xA;  [3]: http://digital-preservation.github.com/droid/" />
  <row Id="468" PostHistoryTypeId="6" PostId="171" RevisionGUID="5b8c3d8d-2382-46fc-bb8d-c5806d3dc56a" CreationDate="2013-03-03T11:46:54.523" UserId="97" Comment="edited tags" Text="&lt;tdr&gt;&lt;certification&gt;" />
  <row Id="469" PostHistoryTypeId="2" PostId="191" RevisionGUID="82a6e47c-db12-42e7-a082-aaf5c4964217" CreationDate="2013-03-03T11:47:17.163" UserId="-1" Text="" />
  <row Id="470" PostHistoryTypeId="2" PostId="192" RevisionGUID="e99f2aa0-12f6-4a9f-866f-5f6085e18043" CreationDate="2013-03-03T11:47:17.163" UserId="-1" Text="" />
  <row Id="471" PostHistoryTypeId="2" PostId="193" RevisionGUID="e8b271bb-0d87-41a7-a841-7dfa7ddc5101" CreationDate="2013-03-03T14:30:52.137" UserId="14" Text="The way Blu-Ray discs and DVDs are constructed may give one a durability advantage over the other, but I lack the expertise to draw a firm conclusion. &#xD;&#xA;&#xD;&#xA;Some points: Blu-Ray is higher density, which requires a smaller laser spot when reading. This limits the thickness of the cover layer to 0.1 mm, since a thicker cover would distort the spot. A single-sided DVD uses only half its thickness for recording; I don't know whether this means it's very well-protected from the top or very vulnerable on the bottom, or both. Blu-Ray has a mandatory 2-nanometer hardcoat on the reading side, but the better DVDs have the equivalent. The higher density of Blu-Ray means that damage of a given size will take out more bits. However, Blu-Ray claims to have a better error correction algorithm.&#xD;&#xA;&#xD;&#xA;I'd like to compare best against best, which means archival-quality, single layer DVD+R against archival-quality, single layer BD R, so we don't get bogged down in side issues like rewritable discs." />
  <row Id="472" PostHistoryTypeId="1" PostId="193" RevisionGUID="e8b271bb-0d87-41a7-a841-7dfa7ddc5101" CreationDate="2013-03-03T14:30:52.137" UserId="14" Text="How do DVD and Blu-Ray compare in durability?" />
  <row Id="473" PostHistoryTypeId="3" PostId="193" RevisionGUID="e8b271bb-0d87-41a7-a841-7dfa7ddc5101" CreationDate="2013-03-03T14:30:52.137" UserId="14" Text="&lt;storage&gt;&lt;storage-media&gt;" />
  <row Id="475" PostHistoryTypeId="2" PostId="194" RevisionGUID="d60a90fd-cc5d-4f14-bd64-0e38b0c82c35" CreationDate="2013-03-03T17:27:40.047" UserId="93" Text="If you've extracted absolutely all of the content from the original media, including any information printed on the outside and perhaps also on the case, **and you can confirm this has been done**, then in most instances it should not be necessary to save the original media. But there absolutely must be confirmation that all information and content has been correctly copied before the original media is discarded. There should also be a record of the copying process - who copied, how, when, etc.&#xD;&#xA;&#xD;&#xA;That said, there are inevitably instances where it is preferable to retain it, eg:&#xD;&#xA;&#xD;&#xA; - where the original media itself is of interest as a physical object&#xD;&#xA; - where there is an institutional requirement to maintain the original media, even if it degrades&#xD;&#xA;&#xD;&#xA;The original media should never simply be disposed of as a matter of course. Institutions should preferably have a policy stating and justifying the process before any destruction takes place. &#xD;&#xA;" />
  <row Id="476" PostHistoryTypeId="2" PostId="195" RevisionGUID="5161eef0-9590-47ce-b648-44e842c33ea2" CreationDate="2013-03-03T17:52:39.977" UserId="162" Text="I think this would depend on the kind of media you are working with. VHS tapes are very different &quot;media&quot; than CDs. Personally, I would be cautious about destroying any media, even if I believed I had extracted all of the content as faithfully as was present in the original.  " />
  <row Id="477" PostHistoryTypeId="2" PostId="196" RevisionGUID="228dcc8b-092c-4e2e-8d59-1feef895afe9" CreationDate="2013-03-03T18:00:46.460" UserId="162" Text="Bill Lefurgy and Aaron Rubinstein have presented excellent responses to this question. I would add that it would be critical for the digital archivist to also understand the content (its importance, historical context, etc.) held in the archive.  " />
  <row Id="478" PostHistoryTypeId="2" PostId="197" RevisionGUID="879fd253-af79-4a5a-8ec1-6a26aa5d8ea7" CreationDate="2013-03-03T21:42:37.197" UserId="76" Text="At this point there are tons of flash games and interactives out there. Many of these have become significant works that we are going to want to be able to play well into the future. So, what does it take to preserve a flash game so it can be engaged with in the same way again in the future? &#xD;&#xA;&#xD;&#xA; - Would it make sense to simply save copies of the files for the game?&#xD;&#xA; - Given that flash games play in web browsers, and come with&#xD;&#xA;   dependencies on particular browser plugins, would one also want to&#xD;&#xA;   have a copy of the browser and the plugin on hand?&#xD;&#xA; - Or would one just need to document the dependencies (for example, the&#xD;&#xA;   version of flash it was made to run on)?&#xD;&#xA; - Are there other key issues one should consider in attempting to preserve a flash game so that it can be played in the future? " />
  <row Id="479" PostHistoryTypeId="1" PostId="197" RevisionGUID="879fd253-af79-4a5a-8ec1-6a26aa5d8ea7" CreationDate="2013-03-03T21:42:37.197" UserId="76" Text="What would it take to preserve a basic flash game?" />
  <row Id="480" PostHistoryTypeId="3" PostId="197" RevisionGUID="879fd253-af79-4a5a-8ec1-6a26aa5d8ea7" CreationDate="2013-03-03T21:42:37.197" UserId="76" Text="&lt;file-formats&gt;&lt;web-archiving&gt;&lt;video-games&gt;" />
  <row Id="481" PostHistoryTypeId="2" PostId="198" RevisionGUID="e627ec56-0bd5-44d7-97cc-c0e52a7d6b16" CreationDate="2013-03-04T06:57:50.027" UserId="74" Text="The great problem with flash technology is that it is closed technology, controlled by one company. As a rule of the thumb, you should avoid such things if you want to use something for the very long time. Already now there are compatibility issues, for example temporal lack of flash plugin for Linux, low performance of plugin on Windows x64. &#xD;&#xA;&#xD;&#xA;But assuming you are already 'in', the best way to be able to use the program in the future is to create **snapshot** of the operating system with functioning flash plugin (the **image** of the hard disk). There are already [x86 emulators][1] and it is rational to expect that even better versions will be available in the future. You should also note the details of your hardware, it **could** be necessary to create the hardware emulator to run that disk image.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.thefreecountry.com/emulators/pc.shtml" />
  <row Id="482" PostHistoryTypeId="2" PostId="199" RevisionGUID="9a337383-0737-4144-93a8-d2c88c6f92c8" CreationDate="2013-03-04T08:02:13.180" UserId="89" Text="A university degree in [computational linguistics][1], also known as *natural language processing* (NLP). Computational linguistics isn’t computer science, it is “the programmer’s corner” of applied linguistics. Computational linguists are well-trained in information extraction, know to make use of regular expressions and have a deep insight in techniques such as OCR, speech recognition and search engine operation. Owning a university degree also implies the qualification to self-educate which is important, too, since there is a variety of computer systems around, including new ones mushrooming day by day.&#xD;&#xA;&#xD;&#xA;One more fact that—to my eyes—favours computational linguists over computer scientists is that computer science focuses a lot on well modeled software (which quickly results in large, ressource consuming software architectures) meanwhile computational linguists had to learn to do their job with little equipment for historical reasons, since computers have had little ressources in past days [at least those the linguistics department got approval for] while corpora processing frequently involves processing *very* large amounts of data.&#xD;&#xA;&#xD;&#xA;  [1]: http://aclweb.org/aclwiki/index.php?title=Frequently_asked_questions_about_Computational_Linguistics#What_is_Computational_Linguistics.3F&#xD;&#xA;" />
  <row Id="484" PostHistoryTypeId="2" PostId="200" RevisionGUID="3fc93542-b713-40f5-90da-dad7f6c2d276" CreationDate="2013-03-04T10:57:44.840" UserId="21" Text="few weeks ago a [message][1] on the list [DIGITAL-PRESERVATION@JISCMAIL.AC.UK][2] announced the TDR certification of Scholars Portal&#xD;&#xA;&#xD;&#xA;i've found extremely useful and clear their [wiki][3] with a well documented audit progress&#xD;&#xA;&#xD;&#xA;http://spotdocs.scholarsportal.info/display/OAIS/Document+Checklist&#xD;&#xA;http://spotdocs.scholarsportal.info/display/OAIS/TRAC2+Progress+Overview&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ocul.on.ca/node/1637&#xD;&#xA;  [2]: https://www.jiscmail.ac.uk/cgi-bin/webadmin?A0=digital-preservation&#xD;&#xA;  [3]: http://spotdocs.scholarsportal.info/display/OAIS/Home" />
  <row Id="486" PostHistoryTypeId="10" PostId="183" RevisionGUID="a3120ca9-cf9b-49ba-8e53-52757474d252" CreationDate="2013-03-04T13:08:29.730" UserId="-1" CloseReasonId="4" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:31,&quot;DisplayName&quot;:&quot;Andy Jackson&quot;},{&quot;Id&quot;:68,&quot;DisplayName&quot;:&quot;Adam-E&quot;},{&quot;Id&quot;:57,&quot;DisplayName&quot;:&quot;mjuarez&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;mopennock&quot;}]}" />
  <row Id="487" PostHistoryTypeId="2" PostId="201" RevisionGUID="8a49b55b-da83-492c-ab0e-dcbd6cbec5a6" CreationDate="2013-03-04T14:55:12.133" UserId="171" Text="Source: I am the digital preservation contact for an archive that was recently certified as a TDR (Scholars Portal). (http://www.ocul.on.ca/node/1639)&#xD;&#xA;&#xD;&#xA;In response to your preamble saying it's time intensive and expensive: It's definitely time- and resource- intensive to do a proper evaluation by TRAC/ISO16363. That said, so is the process of digital preservation. The standards can be used in a couple of ways: to improve your existing processes if you think they are not robust in their support for long-term preservation, or to demonstrate to others that you are doing a good job of these tasks if you are. The amount of resources you'll want to put into either of those tasks is proportionate to the importance of digital preservation in your organization's mission. &#xD;&#xA;&#xD;&#xA;The argument that complying with TRAC/ISO 16363 is expensive seems a bit specious to me. Is it expensive to go through the full audit and be certified by an outside agency? It can be, but at the same time if your community or organization needs assurance, then outside certification can be an effective way to show that. Look at the organiztions who have undergone the formal audit: Portico, HathiTrust, Chronopolis, Scholars Portal. Without exception, these are organizations who accept money (sometimes in the form of subscription fees, sometimes in the form of regular funding) to &quot;do digital preservation&quot;. Those who are paying the money likely want some assurance it's being done correctly. &#xD;&#xA;&#xD;&#xA;So, in answer to your questions:&#xD;&#xA;&#xD;&#xA;1) I'd say, if you need to prove to somebody that you're doing a good job of digital preservation (say because they give you money), this is a good way to do it. If you're looking for a way to evaluate your own processes, then I think the standard still has a lot of value as a tool for guided self-reflection. You can also look at other self-evaluation tools like the Data Seal of Approval (http://datasealofapproval.org/). Note that functionally, the DSA and TRAC are basically the same. I happen to like the TRAC way of breaking down the evidence a little better because it's a bit more granular. That structure is what enabled us to create the wiki where we posted all of our evidence from our TRAC audit (http://spotdocs.scholarsportal.info/display/OAIS/Home). &#xD;&#xA;&#xD;&#xA;I'm also of the opinion that being able to post a point-by-point dissection of our preservation operations in an organized and cogent way is at least as useful as the formal certification because it lets anyone who cares see exactly what we're doing. So, in this regard, the time spent preparing for the audit was doubly useful in establishing trust.&#xD;&#xA;&#xD;&#xA;2) No, I don't even know who you are. Any kind of validation, whether it's TRAC/ISO16363/DSA-based, and whether it's self- or external auditing is only useful to validate your processes as they fill the needs of your user community (Designated Community in OAIS parlance). TDR certification is not some kind of magic bullet that is going to make you a heavy hitter in preservation overnight. In fact, a number of the audit points depend on having a demonstrated history of service and successful preservation. &#xD;&#xA;&#xD;&#xA;TLDR; If you're just starting out in digital preservation, TDR certification is probably not worth it, but the standard (and others like it) are useful tools to ensure you're developing services in a sound way. If you're already doing preservation and want to demonstrate to the outside world that you're doing a good job, certification can be one way to do that. So can plain old openness.&#xD;&#xA;&#xD;&#xA;In any case, certification isn't a magic bullet to all of a sudden make you a great repository. It's all about reinforcing the trust that your community has in you and becoming more accountable. Whether that fits into your business case is up to you." />
  <row Id="488" PostHistoryTypeId="10" PostId="182" RevisionGUID="aa2c1746-509f-4555-9571-d17b5a16b4ed" CreationDate="2013-03-04T14:57:01.813" UserId="-1" CloseReasonId="4" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:31,&quot;DisplayName&quot;:&quot;Andy Jackson&quot;},{&quot;Id&quot;:68,&quot;DisplayName&quot;:&quot;Adam-E&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;mopennock&quot;},{&quot;Id&quot;:29,&quot;DisplayName&quot;:&quot;Nick Krabbenhoeft&quot;}]}" />
  <row Id="489" PostHistoryTypeId="5" PostId="176" RevisionGUID="d43d3313-03e0-4fed-a800-409f4b028e51" CreationDate="2013-03-04T15:00:47.910" UserId="29" Comment="added 57 characters in body" Text="The digital archive certification world a little messy right now. There are multiple acronyms (TRAC, TDR, OAIS) and ISO standards (14721, 16363, 16919) and they have very similar of names.&#xD;&#xA;&#xD;&#xA;At this moment, no auditing body is certified to certify a repository as a TDR, also known as [ISO 16363](http://www.iso16363.org/news/iso-16363-2012-now-published-on-iso-site-as-international-standard/). First, we need to establish a standard for certifying the auditors. This is in draft form as [ISO 16919](http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=57950). The current roadblock is that the standard needs to define the distribution of auditor certifying organizations. National? Regional? Supranational? It's still in debate.&#xD;&#xA;&#xD;&#xA;In the meantime, we have the provisional auditing scheme based on OAIS/ISO 14721 known as [TRAC](http://www.crl.edu/archiving-preservation/digital-archives/metrics-assessing-and-certifying)  that served as the basis for TDR/ISO 16363. Only the Center for Research Libraries (CRL) is certified to use TRAC. As far as I understand, CRL is auditing repositories using TDR/ISO 16363 since the standard has been accepted but assigning a TRAC label to them since they are not certified to give TDR certifications.&#xD;&#xA;&#xD;&#xA;1) Is certification worth it? I would say it depends on the size of the institution. It is expensive, but having a third-party evaluate the current preservation capability of a system is essential. Certification should measure the current effectiveness of administrative, technical, and staff capabilities in a more objective manner. As organizations grow in size and budget, it becomes more difficult to keep an objective view of all of these factors. If you work through the self-auditing versions of TRAC and [TDR/ISO 16363](http://www.iso16363.org/assets/Self-AssessmentTemplateforISO16363.xls), you'll see that's it's an [immense amount of work](http://www.iso16363.org/preparing-for-an-audit/) and very difficult to remain objective with every metric.&#xD;&#xA;&#xD;&#xA;2) Would you like to position your archive as a digital repository for institutions that lack the infrastructure? [Chronopolis](http://chronopolis.sdsc.edu/) certainly does this for research data, and it uses it's TRAC certification as a seal of approval (it's the first link in their menu structure). However, I think certification itself will not automatically attract clients. It's most effective as a feature to distinguish one repository from another." />
  <row Id="490" PostHistoryTypeId="2" PostId="202" RevisionGUID="2af5e33b-1bfc-43b3-8581-2c58cd3fcd21" CreationDate="2013-03-04T15:07:53.010" UserId="31" Text="Yes, it probably makes sense to just save copies of the files of the game. However, the first assumption here is that these Flash games don't rely on any dynamic resource dependencies (as per this related post on [Preserving Flash-driven Art](http://rhizome.org/editorial/2013/feb/25/preserving-flash/)) - i.e. that they truly are self-contained. This is difficult to be sure about, unfortunately, short of playing though the whole game with your network connection switched off.&#xD;&#xA;&#xD;&#xA;The second assumption is that the Flash game is not coded in any way that critically depends on being embedded in a web browser. This can vary from simple dependence on critical parameters that are only declared in the parent page, through to nasty cross-browser/plugin scripting. Again, it's difficult to be sure about this limitation without manual testing.&#xD;&#xA;&#xD;&#xA;Beyond that, the critical information is the publication date so that a contemporary playback environment can be constructed. Having said that, if you expect to provide access, I would also recommend taking copies of the dependent software. I'm a little more skeptical of the long-term stability of specific virtual machine images (which can have subtle dependencies like virtual hardware that may be deprecated), so I would ideally also store the installation software for a suitable browser and Flash plugin so that a suitable VM could be rebuild on top of a reconstructed OS image.&#xD;&#xA;&#xD;&#xA;Finally, you could also try a SWF-to-HTML5 migration (e.g. [Google's Swiffy](https://www.google.com/doubleclick/studio/swiffy/) or [Adobe's solution](http://www.adobe.com/uk/products/flash/flash-to-html5.html)) as a backup/alternative option. They are both closed-source products, but the migrated versions of the SWF files would be in open formats." />
  <row Id="491" PostHistoryTypeId="5" PostId="192" RevisionGUID="13562e07-6767-4177-8138-f629b233b952" CreationDate="2013-03-04T15:17:46.330" UserId="97" Comment="added 26 characters in body" Text="Trusted Digital Repository" />
  <row Id="492" PostHistoryTypeId="24" PostId="192" RevisionGUID="13562e07-6767-4177-8138-f629b233b952" CreationDate="2013-03-04T15:17:46.330" Comment="Proposed by 97 approved by 101 edit id of 19" />
  <row Id="493" PostHistoryTypeId="5" PostId="187" RevisionGUID="79f4bfdd-8320-41e5-85d6-261aaca8bc87" CreationDate="2013-03-04T15:17:51.027" UserId="68" Comment="added 103 characters in body" Text="Vintage computing or retrocomputing is the use of older computer hardware and software in modern times." />
  <row Id="494" PostHistoryTypeId="24" PostId="187" RevisionGUID="79f4bfdd-8320-41e5-85d6-261aaca8bc87" CreationDate="2013-03-04T15:17:51.027" Comment="Proposed by 68 approved by 101 edit id of 18" />
  <row Id="495" PostHistoryTypeId="5" PostId="186" RevisionGUID="b4530baf-9ab1-4008-a4c0-32909786672c" CreationDate="2013-03-04T15:17:54.613" UserId="68" Comment="added 552 characters in body" Text="Vintage computing or retrocomputing is the use of older computer hardware and software in modern times. Retrocomputing is usually classed as a hobby and recreation rather than a practical application of technology. Enthusiasts often collect rare and valuable hardware and software for sentimental reasons. &#xD;&#xA;&#xD;&#xA;Retrocomputing can also have a valuable application in the field of digital preservation and allow the possibilty to archive media from an obsolete format that would otherwise be difficult or impossible to retrieve with more modern equipment." />
  <row Id="496" PostHistoryTypeId="24" PostId="186" RevisionGUID="b4530baf-9ab1-4008-a4c0-32909786672c" CreationDate="2013-03-04T15:17:54.613" Comment="Proposed by 68 approved by 101 edit id of 17" />
  <row Id="498" PostHistoryTypeId="2" PostId="203" RevisionGUID="0a81faee-9693-48a0-8939-5d38db325e37" CreationDate="2013-03-04T15:29:45.503" UserId="173" Text="When preparing a submission information package with fixity information, which hash takes more server resources to calculate, MD5 or SHA-1?" />
  <row Id="499" PostHistoryTypeId="1" PostId="203" RevisionGUID="0a81faee-9693-48a0-8939-5d38db325e37" CreationDate="2013-03-04T15:29:45.503" UserId="173" Text="Fixity calculation resources" />
  <row Id="500" PostHistoryTypeId="3" PostId="203" RevisionGUID="0a81faee-9693-48a0-8939-5d38db325e37" CreationDate="2013-03-04T15:29:45.503" UserId="173" Text="&lt;fixity&gt;&lt;md5&gt;&lt;sha-1&gt;" />
  <row Id="501" PostHistoryTypeId="2" PostId="204" RevisionGUID="9c12122d-796d-4474-8803-03c8dcd187ce" CreationDate="2013-03-04T15:40:35.970" UserId="97" Text="SHA-1 is a much more complex hashing algorithm than is MD5, but neither of them is computationally &quot;cheap&quot;.&#xD;&#xA;&#xD;&#xA;That said, when running off spinning-platter hard drives and reasonably modern hardware, **neither** hashing algorithm is going to be the limiting factor. The limiting factor in almost any realistic case will rather be disk read throughput during the calculations.&#xD;&#xA;&#xD;&#xA;You should rather be concerned with the risk of changes (intentional or unintentional) in data resulting in an unchanged hash value. For that, you need a long (providing enough space to minimize the risk of collisions), good (to increase the likelihood that any change results in a different hash) hashing algorithm. MD5 is cryptographically broken, and there are serious doubts about the long-term cryptographic security of SHA-1. The combination of the two, however, may very well be viable from an archive ingestion point of view (to ensure that the data received matches what was sent)." />
  <row Id="502" PostHistoryTypeId="2" PostId="205" RevisionGUID="1fc7c2b8-d3ba-44b6-9f5e-932d64c2649f" CreationDate="2013-03-04T15:42:02.393" UserId="173" Text="The M-DISC has been found by the United States Naval Air Warfare Center Weapons Division to withstand accelerated aging testing with no data degradation. No other optical disc has demonstrated such a capability. Why hasn't the digital preservation industry adopted the M-DISC as a recommended storage medium? " />
  <row Id="503" PostHistoryTypeId="1" PostId="205" RevisionGUID="1fc7c2b8-d3ba-44b6-9f5e-932d64c2649f" CreationDate="2013-03-04T15:42:02.393" UserId="173" Text="M-DISC as an archival storage medium" />
  <row Id="504" PostHistoryTypeId="3" PostId="205" RevisionGUID="1fc7c2b8-d3ba-44b6-9f5e-932d64c2649f" CreationDate="2013-03-04T15:42:02.393" UserId="173" Text="&lt;storage&gt;&lt;archival&gt;&lt;medium&gt;&lt;data&gt;&lt;degradation&gt;" />
  <row Id="505" PostHistoryTypeId="2" PostId="206" RevisionGUID="8ff9ce60-645c-4e6f-9a69-539f644cff38" CreationDate="2013-03-04T16:30:47.893" UserId="94" Text="The question seems to make the assumption that preserving a game means preserving the digital bits that make up the game, and/or preserving the experience of playing the game. This may well be the aim of the preservation activity (in which case the other answers provide some great detail on this), but equally, it might perhaps not be. Some have argued that preserving the ability to play games might not actually be the best way to achieve preservation of gaming as we know it.&#xD;&#xA;&#xD;&#xA;James Newman and Daniel Ashton have, for example, eloquently made the argument that [other approaches may better capture the essence of preserving the experience of playing games][1]. They suggest that player authored documents (such as game walkthroughs) do a great job of capturing the experience of gameplay, and could provide more understanding of that experience to future users/gamers/historians than simply being presented with a running game.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.transformationsjournal.org/journal/issue_20/article_03.shtml" />
  <row Id="506" PostHistoryTypeId="2" PostId="207" RevisionGUID="96039ae3-2d65-49c8-8223-7f6b3ca32439" CreationDate="2013-03-04T16:43:17.347" UserId="175" Text="I'm working on my PhD about emulation as preservation method. I did my bachelor's thesis and master's thesis from this same subject. Many times I've found myself thinking this question: Is it impossible to produce old electronics in present factories? If, let's say Intel would get an order to produce one 8080 processor, just as &quot;clumsy&quot; (from present day perspective) as it was back then, with the same technology and same architecture. 100% the same without using any modern techniques?&#xD;&#xA;&#xD;&#xA;I have asked this question from few engineers and usually they avoid the question by counterquestion or then they say this is a wrong question to ask, because &quot;who would want to have old ones?&quot;&#xD;&#xA;&#xD;&#xA;I have started to believe that with this counterquestion or correcting my question they are trying to hide the fact that they do not know the answer.&#xD;&#xA;&#xD;&#xA;In digital preservation this question is interesting because, if we don't have a single working machine available, where the program was supposed to run, could we produce one? (Let's play now we don't have to care about the costs).&#xD;&#xA;&#xD;&#xA;From my own studies I have come to conclusion that producing replicas from old ones that would be 100% the same as 30 years ago is theoretically possible but in practice impossibility, because machines that make machines in factories have been made to produce new technology and the old technology is scrapped. These new machines that make machines could make a machine that logically functions just like the old one, but they would do it in a new, more developed way.&#xD;&#xA;&#xD;&#xA;To produce a new series of old 8-bit computers would require whole factories and assembly lines that are similar to the ones that was used in 80's and rebuilding these would be too expensive even for the most gigantic companies.&#xD;&#xA;&#xD;&#xA;Is my conclusion right or wrong or partially right/wrong? &#xD;&#xA;&#xD;&#xA;Sincerely,&#xD;&#xA;&#xD;&#xA;Samuel Ranta&#xD;&#xA;PhD-student &amp; co-ordinator in Memornet programme.&#xD;&#xA;Pinni B 2121d&#xD;&#xA;Kanslerinrinne 1 &#xD;&#xA;33014&#xD;&#xA;University of Tampere&#xD;&#xA;Finland&#xD;&#xA;http://www.uta.fi/sis/yhteystiedot/henkilokunta/samuelranta.html&#xD;&#xA;GSM: +358443273443&#xD;&#xA;" />
  <row Id="507" PostHistoryTypeId="1" PostId="207" RevisionGUID="96039ae3-2d65-49c8-8223-7f6b3ca32439" CreationDate="2013-03-04T16:43:17.347" UserId="175" Text="Is it impossible to produce old electronics in new factories?" />
  <row Id="508" PostHistoryTypeId="3" PostId="207" RevisionGUID="96039ae3-2d65-49c8-8223-7f6b3ca32439" CreationDate="2013-03-04T16:43:17.347" UserId="175" Text="&lt;digital&gt;&lt;preservation&gt;&lt;technology&gt;&lt;emulation&gt;&lt;computer&gt;" />
  <row Id="509" PostHistoryTypeId="5" PostId="182" RevisionGUID="b2721a55-ffc5-4aa1-adef-2dd6674295dc" CreationDate="2013-03-04T17:07:54.493" UserId="133" Comment="Refocused the question to address concerns about generality." Text="What are the pros and cons, if the collection is from one person and contains many attachments in various formats? " />
  <row Id="510" PostHistoryTypeId="4" PostId="182" RevisionGUID="b2721a55-ffc5-4aa1-adef-2dd6674295dc" CreationDate="2013-03-04T17:07:54.493" UserId="133" Comment="Refocused the question to address concerns about generality." Text="Is emulation a good option for preserving large email collections?" />
  <row Id="512" PostHistoryTypeId="2" PostId="209" RevisionGUID="c09b7d68-814c-48c8-b3b3-6f5ea3191c97" CreationDate="2013-03-04T17:50:21.243" UserId="93" Text="I suspect it isn't impossible, more that from a cost perspective it would be an undesirable venture for a commercial company to undertake. To 'rollback' the technology in a factory so that it produced hardware in exactly the same way as in previous decades would be phenomenally expensive. But is it really necessary to produce the technology in exactly the same way, or is it acceptable to use new production methods to create it? That would depend on your requirements and the importance of the process.&#xD;&#xA;&#xD;&#xA;The [Computer Conservation Society] [1] does an excellent job in restoring and/or recreating old machine. CCS volunteers [recently restored Flossie][2] (also known as CT 1301), though how they validated the functionality of the restoration is unclear. From what I've seen though, they do not attempt to do so in modern factories.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.computerconservationsociety.org/&#xD;&#xA;  [2]: http://www.bbc.co.uk/news/uk-england-kent-20031487" />
  <row Id="513" PostHistoryTypeId="2" PostId="210" RevisionGUID="b70242b6-b1a6-4dd1-8203-64e9e0ccac20" CreationDate="2013-03-04T18:23:43.387" UserId="42" Text="I can only speak as to why I don't/would not use M-Disc for my own personal archiving.&#xD;&#xA;&#xD;&#xA;My backup consists of virtually all of the digital media that my family consumes: &#xD;&#xA;&#xD;&#xA;1. digital video of movies, TV shows and home movies&#xD;&#xA;&#xD;&#xA;2. digital music&#xD;&#xA;&#xD;&#xA;3. digital images (from a digital camera, or digitized prints)&#xD;&#xA;&#xD;&#xA;4. ebooks&#xD;&#xA;&#xD;&#xA;5. other files created by us in various formats&#xD;&#xA;&#xD;&#xA;The total size of this content is approximately 1.5 TB, which would require more than 300 M-Discs, just for one copy. I don't see my storage needs doing anything but getting larger in the future.&#xD;&#xA;&#xD;&#xA;With a per-disc cost of nearly $3.00, it would cost me around $1,000 to make a single backup  of my data. There's no possible way that I could justify that kind of expenditure, or the time involved to create and maintain an archive using such a medium." />
  <row Id="515" PostHistoryTypeId="2" PostId="211" RevisionGUID="6d622317-1431-4fc1-b1cd-9275a8ff36fb" CreationDate="2013-03-04T18:52:43.183" UserId="74" Text="We have tanks and airplanes, but there are still people producing swords. But to produce a sword you need a skilled blacksmith and a bit of tools. To produce, say, 286 computer, you need a whole factory. &#xD;&#xA;&#xD;&#xA;When someone wants to sacrifice his free time and a few thousand $ for his blacksmithing hobby, it's just a hobby like anything else. But if someone was to sacrifice a few million $ and tens (when not hundreds) of people's full-time job to produce old electronics with no practical meaning but only for hobby, this is a social problem. Such huge resources could be used for more rational and practical aim.&#xD;&#xA;&#xD;&#xA;For data and software preservation, software emulation is fully enough. The aim is to be able to run the software or use the data, not run them on exactly the same device as 20 or 30 years ago. " />
  <row Id="516" PostHistoryTypeId="2" PostId="212" RevisionGUID="81f02c03-6cbd-4662-bd00-b1e8e6ede923" CreationDate="2013-03-04T19:02:32.250" UserId="74" Text="Some even very old educational software is still of value today and it is worth preserving.&#xD;&#xA;&#xD;&#xA;The problem is that nowadays the licence rights are very important topic, but 20 or 30 years ago it was not considered important for many programmers, who have made such software peaces, don't placing any licence or even their names. But how can you tell if such program, found in some archive in school workshop was intentionally made without credits, or such credits were removed by someone?&#xD;&#xA;&#xD;&#xA;Can the archiving such software for preservation theoretically and practically violate someone's intellectual or material rights? Note that such software is to be considered coming from unknown source, because no one is going no to tell, where such program come from, and who have copied them? It may be some student's work or pirated software of some company who doesn't exist anymore." />
  <row Id="517" PostHistoryTypeId="1" PostId="212" RevisionGUID="81f02c03-6cbd-4662-bd00-b1e8e6ede923" CreationDate="2013-03-04T19:02:32.250" UserId="74" Text="How to deal with preservation of software with unknown author and licence?" />
  <row Id="518" PostHistoryTypeId="3" PostId="212" RevisionGUID="81f02c03-6cbd-4662-bd00-b1e8e6ede923" CreationDate="2013-03-04T19:02:32.250" UserId="74" Text="&lt;software&gt;&lt;law&gt;" />
  <row Id="520" PostHistoryTypeId="10" PostId="207" RevisionGUID="f0033919-9e71-4d34-8bfe-706b84d1f89c" CreationDate="2013-03-04T21:17:20.783" UserId="101" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:133,&quot;DisplayName&quot;:&quot;Bill Lefurgy&quot;},{&quot;Id&quot;:94,&quot;DisplayName&quot;:&quot;Paul Wheatley&quot;},{&quot;Id&quot;:101,&quot;DisplayName&quot;:&quot;Robert Cartaino&quot;}]}" />
  <row Id="521" PostHistoryTypeId="2" PostId="213" RevisionGUID="02936714-edc1-470c-a179-66f5ce9d302c" CreationDate="2013-03-04T21:29:19.967" UserId="76" Text="[Islandora][1] uses [Drupal][2] as a frontend for a [Fedora Repository][3], [Hydra][4] integrates [Solr][5] with a fedora repository and serves as a [Ruby on Rails][6] platform for running the front end to a Fedora repository. &#xD;&#xA;&#xD;&#xA;If an organization has decided they want to use Fedora and is deciding between running these two potential front-end systems what specific comparative issues should they be considering? &#xD;&#xA;&#xD;&#xA;I'm particularly interested in differences in terms of: &#xD;&#xA;&#xD;&#xA; - resources to implement&#xD;&#xA; - resources to maintain&#xD;&#xA; - necessary IT infrastructure&#xD;&#xA; - technical knowledge for staff running the system&#xD;&#xA; - flexibility in what they can accomplish&#xD;&#xA; - differences in communities related to their open source software dependencies&#xD;&#xA;&#xD;&#xA;Oh, and if there are other key issues you think should be on this list feel free to suggest edits to the question. &#xD;&#xA;&#xD;&#xA;  [1]: http://islandora.ca&#xD;&#xA;  [2]: http://drupal.org/&#xD;&#xA;  [3]: http://fedora-commons.org/&#xD;&#xA;  [4]: https://wiki.duraspace.org/display/hydra/The+Hydra+Project&#xD;&#xA;  [5]: http://lucene.apache.org/solr/&#xD;&#xA;  [6]: http://rubyonrails.org/" />
  <row Id="522" PostHistoryTypeId="1" PostId="213" RevisionGUID="02936714-edc1-470c-a179-66f5ce9d302c" CreationDate="2013-03-04T21:29:19.967" UserId="76" Text="Key issues to consider when deciding between Islandora and Hydra as front-ends to a Fedora Repository" />
  <row Id="523" PostHistoryTypeId="3" PostId="213" RevisionGUID="02936714-edc1-470c-a179-66f5ce9d302c" CreationDate="2013-03-04T21:29:19.967" UserId="76" Text="&lt;software&gt;&lt;content-management&gt;&lt;infrastructure&gt;" />
  <row Id="527" PostHistoryTypeId="6" PostId="212" RevisionGUID="d9b7e668-3bdb-49ef-a2fc-4a9454582896" CreationDate="2013-03-04T21:38:18.370" UserId="118" Comment="edited tags" Text="&lt;software&gt;&lt;law&gt;&lt;copyright&gt;" />
  <row Id="528" PostHistoryTypeId="6" PostId="145" RevisionGUID="6c15ef60-7ca1-4cac-84c0-de3f769854ea" CreationDate="2013-03-04T21:43:25.513" UserId="118" Comment="edited tags" Text="&lt;digitize&gt;&lt;legal&gt;&lt;unclear-location&gt;&lt;copyright&gt;" />
  <row Id="529" PostHistoryTypeId="6" PostId="87" RevisionGUID="bd9eb533-393a-4b3c-8d37-488c2d0fd14f" CreationDate="2013-03-04T21:44:02.583" UserId="118" Comment="edited tags" Text="&lt;web-archiving&gt;&lt;legal&gt;&lt;digital-born&gt;&lt;crawling&gt;&lt;copyright&gt;" />
  <row Id="530" PostHistoryTypeId="6" PostId="83" RevisionGUID="68f5942f-35ce-42b6-9e1a-6a3f6c7b57f0" CreationDate="2013-03-04T21:44:32.893" UserId="118" Comment="edited tags" Text="&lt;digitize&gt;&lt;legal&gt;&lt;copyright&gt;&lt;law&gt;&lt;scanning&gt;" />
  <row Id="531" PostHistoryTypeId="10" PostId="205" RevisionGUID="97cfcabf-07a8-4eb2-bdea-f3e7e4d1fa21" CreationDate="2013-03-04T22:03:53.390" UserId="-1" CloseReasonId="3" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:94,&quot;DisplayName&quot;:&quot;Paul Wheatley&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;mopennock&quot;},{&quot;Id&quot;:31,&quot;DisplayName&quot;:&quot;Andy Jackson&quot;},{&quot;Id&quot;:4,&quot;DisplayName&quot;:&quot;walker&quot;},{&quot;Id&quot;:14,&quot;DisplayName&quot;:&quot;gmcgath&quot;}]}" />
  <row Id="533" PostHistoryTypeId="2" PostId="215" RevisionGUID="bc0ca5c5-eb11-47fd-ab81-568203d0fcb6" CreationDate="2013-03-04T22:52:06.710" UserId="76" Text="Many organizations are using [Microsoft SharePoint][1] as a tool to both manage content and collaborate. When archives (either as an institutional archive or as the archival home of the organization's materials) are tasked with preserving this content they face some challenges. First and foremost is defining what it is in the SharePoint content that they actually want to preserve.&#xD;&#xA;&#xD;&#xA;In what cases would it make sense to just extract the documents and preserve them and in what cases does it make sense to try and capture the date that exists in SharePoint itself. To clarify, this is more of a significant properties and feasibility question than a how-to question. How one would actually go about either extracting documents or trying to preserve an entire SharePoint environment are each interesting questions in their own right (that I imagine will come up on this site at some point). However, before getting to those questions the first question at hand is how do you go about deciding what inside SharePoint is the actual content you are tasked to preserve. &#xD;&#xA;&#xD;&#xA;So what issues would an archives want to consider in deciding on the significant properties they need to preserve in SharePoint content? &#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Microsoft_SharePoint" />
  <row Id="534" PostHistoryTypeId="1" PostId="215" RevisionGUID="bc0ca5c5-eb11-47fd-ab81-568203d0fcb6" CreationDate="2013-03-04T22:52:06.710" UserId="76" Text="What issues does an archives need to consider when developing a plan for accessioning organizations Sharepoint content" />
  <row Id="535" PostHistoryTypeId="3" PostId="215" RevisionGUID="bc0ca5c5-eb11-47fd-ab81-568203d0fcb6" CreationDate="2013-03-04T22:52:06.710" UserId="76" Text="&lt;software&gt;&lt;accession&gt;&lt;sharepoint&gt;&lt;significant-properties&gt;" />
  <row Id="536" PostHistoryTypeId="10" PostId="83" RevisionGUID="76629565-698f-4d8d-b54e-d5d8f0329baf" CreationDate="2013-03-05T11:11:27.163" UserId="-1" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:119,&quot;DisplayName&quot;:&quot;Ben Fino-Radin&quot;},{&quot;Id&quot;:76,&quot;DisplayName&quot;:&quot;Trevor Owens&quot;},{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;luser droog&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;mopennock&quot;},{&quot;Id&quot;:94,&quot;DisplayName&quot;:&quot;Paul Wheatley&quot;}]}" />
  <row Id="537" PostHistoryTypeId="5" PostId="207" RevisionGUID="9efbc7d9-1756-4393-ba85-00deb71b9cb6" CreationDate="2013-03-05T15:12:17.763" UserId="97" Comment="Removed tagline" Text="I'm working on my PhD about emulation as preservation method. I did my bachelor's thesis and master's thesis from this same subject. Many times I've found myself thinking this question: Is it impossible to produce old electronics in present factories? If, let's say Intel would get an order to produce one 8080 processor, just as &quot;clumsy&quot; (from present day perspective) as it was back then, with the same technology and same architecture. 100% the same without using any modern techniques?&#xD;&#xA;&#xD;&#xA;I have asked this question from few engineers and usually they avoid the question by counterquestion or then they say this is a wrong question to ask, because &quot;who would want to have old ones?&quot;&#xD;&#xA;&#xD;&#xA;I have started to believe that with this counterquestion or correcting my question they are trying to hide the fact that they do not know the answer.&#xD;&#xA;&#xD;&#xA;In digital preservation this question is interesting because, if we don't have a single working machine available, where the program was supposed to run, could we produce one? (Let's play now we don't have to care about the costs).&#xD;&#xA;&#xD;&#xA;From my own studies I have come to conclusion that producing replicas from old ones that would be 100% the same as 30 years ago is theoretically possible but in practice impossibility, because machines that make machines in factories have been made to produce new technology and the old technology is scrapped. These new machines that make machines could make a machine that logically functions just like the old one, but they would do it in a new, more developed way.&#xD;&#xA;&#xD;&#xA;To produce a new series of old 8-bit computers would require whole factories and assembly lines that are similar to the ones that was used in 80's and rebuilding these would be too expensive even for the most gigantic companies.&#xD;&#xA;&#xD;&#xA;Is my conclusion right or wrong or partially right/wrong?" />
  <row Id="538" PostHistoryTypeId="24" PostId="207" RevisionGUID="9efbc7d9-1756-4393-ba85-00deb71b9cb6" CreationDate="2013-03-05T15:12:17.763" Comment="Proposed by 97 approved by 101 edit id of 21" />
  <row Id="539" PostHistoryTypeId="4" PostId="215" RevisionGUID="11708b7a-50db-4c28-83b4-f201ecac5504" CreationDate="2013-03-05T15:12:21.677" UserId="94" Comment="Corrected typos in the title" Text="What issues do archives need to consider when developing a plan for accessioning an organization's Sharepoint content" />
  <row Id="540" PostHistoryTypeId="24" PostId="215" RevisionGUID="11708b7a-50db-4c28-83b4-f201ecac5504" CreationDate="2013-03-05T15:12:21.677" Comment="Proposed by 94 approved by 101 edit id of 20" />
  <row Id="541" PostHistoryTypeId="6" PostId="56" RevisionGUID="90a947e1-4577-45b7-9b4b-65e37c580380" CreationDate="2013-03-05T15:27:52.510" UserId="91" Comment="edited tags" Text="&lt;organization&gt;&lt;checksum&gt;&lt;deduplication&gt;&lt;corruption&gt;&lt;bagit&gt;" />
  <row Id="542" PostHistoryTypeId="5" PostId="138" RevisionGUID="10731666-b1da-4321-8f98-7b8b312e7b51" CreationDate="2013-03-05T16:41:48.293" UserId="4" Comment="generalize to any rss reader" Text="I know that Google Reader often archives all of the items in a RSS feed, which for example allows me to access the RSS feeds of blogs that no longer exist. How can I mass-export the content of those feeds onto a local disk?" />
  <row Id="543" PostHistoryTypeId="4" PostId="138" RevisionGUID="10731666-b1da-4321-8f98-7b8b312e7b51" CreationDate="2013-03-05T16:41:48.293" UserId="4" Comment="generalize to any rss reader" Text="How can I archive all the data in an RSS feed?" />
  <row Id="544" PostHistoryTypeId="24" PostId="138" RevisionGUID="10731666-b1da-4321-8f98-7b8b312e7b51" CreationDate="2013-03-05T16:41:48.293" Comment="Proposed by 4 approved by 101 edit id of 22" />
  <row Id="545" PostHistoryTypeId="2" PostId="216" RevisionGUID="3b5de690-0dd2-4fc9-b3ba-23b966615b2a" CreationDate="2013-03-05T18:33:20.007" UserId="113" Text="I work for the University of Michigan Library on HathiTrust, and I know that TRAC certification was indeed a lot of work for my colleagues involved in the process. The following is a quote from HathiTrust's press release in March of 2011 announcing TRAC certification.&#xD;&#xA;&#xD;&#xA;&gt; Certification represents a major achievement for the partnership,&#xD;&#xA;&gt; which has defined itself by the transparency of its operations, the&#xD;&#xA;&gt; openness of its systems and services, and its reliance on broadly&#xD;&#xA;&gt; accepted standards and best practices for archiving and preserving&#xD;&#xA;&gt; digital content. Certification confirms HathiTrust on its trajectory&#xD;&#xA;&gt; to preserve and provide access to an increasingly comprehensive&#xD;&#xA;&gt; representation of the published record, and advances it’s strategic&#xD;&#xA;&gt; goal to stimulate coordinated efforts among libraries surrounding the&#xD;&#xA;&gt; storage and management of print collections.&#xD;&#xA;&gt; &#xD;&#xA;&gt; HathiTrust commits to maintaining this important certification in the&#xD;&#xA;&gt; future, providing a reliable and valuable resource for its&#xD;&#xA;&gt; institutional partners and a public good to the broader community.&#xD;&#xA;&#xD;&#xA; - [Full press release][1]&#xD;&#xA;&#xD;&#xA;The above doesn't provide a deep answer to the questions posed, but it provides a little insight into the value HathiTrust places on certification. The following are links that might be useful to organizations considering certification because they show the scope of the questions, and quality of the answers.   &#xD;&#xA;&#xD;&#xA; - [CRL's full TRAC audit report][2]&#xD;&#xA; - [HathiTrust's TRAC compliance documentation][3]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.hathitrust.org/hathitrust-certified-as-trustworthy-repository&#xD;&#xA;  [2]: http://www.crl.edu/archiving-preservation/digital-archives/certification-and-assessment-digital-repositories/hathitrust&#xD;&#xA;  [3]: http://www.hathitrust.org/trac" />
  <row Id="551" PostHistoryTypeId="2" PostId="219" RevisionGUID="c0a91169-ec9a-4521-b1f9-e08d976959a9" CreationDate="2013-03-06T18:17:33.187" UserId="110" Text="I have some extremely valuable film footage that currently only exists on multiple copies of a DVD+RW disc. The film was digitized in 2003 but I currently don't know the specifications of either the digitization process or output format on the DVD.&#xD;&#xA;&#xD;&#xA;Currently, for reasons of storage and available expertise, I'm converting our digital video files to MPEG-4 File Format, Version 2 with AVC, High Profile (MP4_FF_2_AVC_HP or H.264) as it is widely used and according to LOC specs it is [&quot;designed to enable a high compression capability for a desired image quality&quot;][1]. However, my goal is to create uncompressed video preservation files and figured this important footage merits the extra storage needed.&#xD;&#xA;&#xD;&#xA;But what uncompressed preservation format to extract the DVD to, and how to do this? I see that [JPEG2000 in an MXF container][2] is used and recommended by LOC, but haven't found how they convert to this format. Daisy-chaining open source tools seems possible, but [complicated and error-prone][3]. There also seems to be disagreement on [JPEG-2000 as a preservation format][4]. Should I extract to uncompressed AVI? Is the DVD footage already compressed, and thus not worth extracting as uncompressed footage?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[1]: http://www.digitalpreservation.gov/formats/fdd/fdd000081.shtml&#xD;&#xA;[2]: http://www.digitalpreservation.gov/formats/fdd/fdd000013.shtml&#xD;&#xA;[3]: https://groups.google.com/forum/?fromgroups=#!topic/archivematica/JnoAisQjuCA&#xD;&#xA;[4]: http://blogs.loc.gov/digitalpreservation/2013/01/is-jpeg-2000-a-preservation-risk/" />
  <row Id="552" PostHistoryTypeId="1" PostId="219" RevisionGUID="c0a91169-ec9a-4521-b1f9-e08d976959a9" CreationDate="2013-03-06T18:17:33.187" UserId="110" Text="What target preservation format and extraction process to use for digitized film currently on DVD+RW?" />
  <row Id="553" PostHistoryTypeId="3" PostId="219" RevisionGUID="c0a91169-ec9a-4521-b1f9-e08d976959a9" CreationDate="2013-03-06T18:17:33.187" UserId="110" Text="&lt;file-formats&gt;&lt;storage-media&gt;" />
  <row Id="554" PostHistoryTypeId="2" PostId="220" RevisionGUID="049df1d9-4a85-4928-9bbb-22f7f74f501d" CreationDate="2013-03-07T03:31:57.313" UserId="99" Text="I've spent the past year backing up old personal media. &#xD;&#xA;I've moved floppy disks, CD's, DVD's, hard-drives, Zip disks and Jazz drives all to hard-drive format. All of this data was collected by me over the past 10-15 years and currently amounts to roughly 20TB of data. (Yes. I am a modern-day digital hoarder, i literally collect almost everything i view online and have done since i was about 12 years old)&#xD;&#xA;&#xD;&#xA;The problem is, that I've never been particularly good at organised files/folders keeping track of data.&#xD;&#xA;A key component of preserving data also relates to finding it afterwards (otherwise it may as well be lost). &#xD;&#xA;So. I'm looking for a method to sort through multi-terabyte archives of files so i can then back them up at my discretion. &#xD;&#xA;&#xD;&#xA;Issues i'm currently facing:&#xD;&#xA;&#xD;&#xA;- Wide variety of file types, Audio/Visual/documents/databases/programs/etc&#xD;&#xA;- Large amount of data (Approx 20TB)&#xD;&#xA;- No Sorting previously done on files&#xD;&#xA;- Multiple duplicates of files&#xD;&#xA;- Multiple versions of duplicate files&#xD;&#xA;- Some sets of files (ie databases) span more than 3-4 TB, which means multiple drives and these drives get mixed up and files get 'lost' &#xD;&#xA;&#xD;&#xA;What I have tried:&#xD;&#xA;&#xD;&#xA;- De-duping files using various program&#xD;&#xA; - To no avail, multiple versions of similar files with different dates/checksums. Also takes an inordinate amount of time to scan and compare every file. &#xD;&#xA;&#xD;&#xA;- Sorting some files manually&#xD;&#xA; - Is taking to long, especially when reviewing individual files (documents and databases) that have changes in the contents&#xD;&#xA; - Have had limited success with visual sorting (images and videos don't change much and de-duping works)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;What i want to do:&#xD;&#xA;&#xD;&#xA;- How can I minimize the amount of space that my files are taking up currently?&#xD;&#xA;- How can I organize the files i have into some kind of structure that's logical (to a human)?&#xD;&#xA;- How can I index and search quickly though this size data set?&#xD;&#xA;- Is it possible for me to generate 'meta-data' about things like documents automatically for indexing or something? About the contents of the files rather than the existing metadata. &#xD;&#xA;&#xD;&#xA;If i need to create some kind of 'baseline' that takes several days to accomplish, that's acceptable if it produces usable results.&#xD;&#xA;&#xD;&#xA;Hopefully someone has some good ideas. &#xD;&#xA;Thanks!" />
  <row Id="555" PostHistoryTypeId="1" PostId="220" RevisionGUID="049df1d9-4a85-4928-9bbb-22f7f74f501d" CreationDate="2013-03-07T03:31:57.313" UserId="99" Text="How can i sort large amounts of data?" />
  <row Id="556" PostHistoryTypeId="3" PostId="220" RevisionGUID="049df1d9-4a85-4928-9bbb-22f7f74f501d" CreationDate="2013-03-07T03:31:57.313" UserId="99" Text="&lt;storage&gt;&lt;digital-archivist&gt;" />
  <row Id="557" PostHistoryTypeId="2" PostId="221" RevisionGUID="9e35ca29-66a4-4277-8dfb-ac93ab198a8f" CreationDate="2013-03-07T07:03:22.073" UserId="74" Text="Finding almost-duplicates is a very challenging tasks, in oposite to exact-duplicates which can be found by checksum indexing. But for multimedia content, the approach that **could** be helpful in your task in **MPEG-7** indexing. &#xD;&#xA;&#xD;&#xA;In short, MPEG-7 is the set of descriptors using to index multimedia content and find &quot;similar&quot; images, audios or videos. I had once project based on dominant colors descriptor, which was very effective in finding pictures made in similar scenario. After calculating the descriptors, it's much easier to compare each-with-each (though still requiring a lot of computing power) to find the almost-duplicate candidates. Such could be processed with other algorithms, that will check if, for example, two pictures aren't the same picture, only cadred, or two music files aren't the same file, with a few seconds removed. In worst case, you can check such duplicate candidates manually.&#xD;&#xA;&#xD;&#xA;The descriptors, as well, could help you organise multimedia content in groups, but it would require other algorithms, such as **centroids**. " />
  <row Id="559" PostHistoryTypeId="2" PostId="222" RevisionGUID="500e489a-b467-4094-87a4-ff36221fa817" CreationDate="2013-03-07T13:13:17.730" UserId="21" Text="what tools are you using for identification of **epub** or **mobi** formats nowadays?&#xD;&#xA;&#xD;&#xA;[fido][1], the tool i prefer, has some [issues][2].&#xD;&#xA;either jhove or fits are recognizing this files as bitstream.&#xD;&#xA;the only working tool seems [epubcheck][3], but only for epub files.&#xD;&#xA;&#xD;&#xA;i'm removing drm from .mobi or .azw files i'm preserving (it's my private library), should i consider to [convert][4] everything to epub and give up on mobi?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/openplanets/fido&#xD;&#xA;  [2]: https://github.com/openplanets/fido/issues/32&#xD;&#xA;  [3]: https://code.google.com/p/epubcheck/wiki/Extraction&#xD;&#xA;  [4]: http://manual.calibre-ebook.com/cli/ebook-convert.html" />
  <row Id="560" PostHistoryTypeId="1" PostId="222" RevisionGUID="500e489a-b467-4094-87a4-ff36221fa817" CreationDate="2013-03-07T13:13:17.730" UserId="21" Text="identification of ebook (mobi, epub) formats" />
  <row Id="561" PostHistoryTypeId="3" PostId="222" RevisionGUID="500e489a-b467-4094-87a4-ff36221fa817" CreationDate="2013-03-07T13:13:17.730" UserId="21" Text="&lt;file-formats&gt;&lt;file-format-migration&gt;&lt;jhove&gt;&lt;fits&gt;&lt;fido&gt;" />
  <row Id="562" PostHistoryTypeId="2" PostId="223" RevisionGUID="8abacccc-1c0a-48d2-bc14-b02c72d7a490" CreationDate="2013-03-07T14:24:04.373" UserId="105" Text="Whenever I need to identify the content of a video file I use [Media Info](http://mediainfo.sourceforge.net/en). It should be able to give you all of the information you need.&#xD;&#xA;&#xD;&#xA;As for chaining together open source programs, that's often the best way to go. You can eliminate some of the potential for errors by writing a script or Makefile that will perform all of the steps for you. This makes it repeatable, should you need to do this for another dvd in the future, or in case you decide to revise the process later. I can also serve as a form of documentation of what you actually did, especially if you use version control.&#xD;&#xA;&#xD;&#xA;I don't know what format is best for you at this point. If it's an ordinary dvd then I would just rip the MPEG2 stream to a file and save that. MPEG2 is a lossy encoding scheme, as are virtually all other video formats (including MotionJPEG). Reencoding to another lossy format is just going to reduce the overall quality of the data, even if you go to a more efficient MPEG4/H.264-based format." />
  <row Id="563" PostHistoryTypeId="2" PostId="224" RevisionGUID="fcd64004-df4c-4668-943e-4afafedddf3e" CreationDate="2013-03-07T14:28:31.657" UserId="64" Text="The use of encryption during archival of digital information could potentially lead to the inability to decrypt it in the future, but are there any situations wherein such complexity is warranted and appropriate in an archival process?  For example, if you were archiving something under copyright and not owned by you, perhaps you would want to encrypt it until the copyright expired to justify fair use?" />
  <row Id="564" PostHistoryTypeId="1" PostId="224" RevisionGUID="fcd64004-df4c-4668-943e-4afafedddf3e" CreationDate="2013-03-07T14:28:31.657" UserId="64" Text="Under what circumstances would it ever make sense to use encryption during archival of digital information?" />
  <row Id="565" PostHistoryTypeId="3" PostId="224" RevisionGUID="fcd64004-df4c-4668-943e-4afafedddf3e" CreationDate="2013-03-07T14:28:31.657" UserId="64" Text="&lt;copyright&gt;&lt;digital-archivist&gt;&lt;encryption&gt;" />
  <row Id="566" PostHistoryTypeId="2" PostId="225" RevisionGUID="1f341d06-957c-4dce-9177-99d24a017378" CreationDate="2013-03-07T15:00:45.833" UserId="45" Text="I think you have the primary reason in the question: legal or contractual requirements which obligate you to limit access and you want more protection against human error or technical lapses. If you had copyrighted materials it might make sense to encrypt the master files so a security intrusion which resulted in access to your file servers but not the keys wouldn't be a potential legal disaster. This might be particularly handy if your institution is not normally in the business of handling sensitive materials and the difficult process of retrofitting strong access controls throughout the system is too challenging.&#xD;&#xA;&#xD;&#xA;There is one interesting technical aspect, however, which is worth reviewing: encryption requires you to treat the keys as rigorously as the actual archived material but the keys are considerably smaller which allows you to do things which would otherwise not be feasible given the size or sensitivity of the material. This could include using third-party services to transfer content and exchanging the keys over a separate channel, storing encrypted backups on a cloud provider or entering into a long-term partnership with a peer institution with the plan to release the keys later when the material leaves copyright or a donor restriction has expired. One particularly interesting aspect in some cases is that it may be possible to meet requirements for deleting content (horrible, I know, but in some cases it's necessary) by deleting the keys rather than having to prove that you've purged every copy of the data from your servers, backups, etc.&#xD;&#xA;&#xD;&#xA;One practical implication: each item (however you defined that logically) should be encrypted with a separate, very strong key. Large data files at rest are both hard to change and easy for an attacker to brute-force so you want to pick a very strong encryption system &amp; key and use a separate system to track the keys so you avoid the need to e.g. re-encrypt your entire HD video archive because someone wrote a password down or shared it. This is also a great spot to build in access control and logging if that's a requirement." />
  <row Id="567" PostHistoryTypeId="2" PostId="226" RevisionGUID="2a238cc2-d8a4-42db-b8ce-1103bd13aec0" CreationDate="2013-03-07T15:24:10.280" UserId="105" Text="Encryption is usually used for keeping secrets, and if all you are concerned with is copyright (which only governs who is allowed to distribute a work), then the material is probably not secret.&#xD;&#xA;&#xD;&#xA;I can see it being useful for a corporation that is archiving or backing up its internal communications, source code, etc. You could also use it whenever you have to store your data in the clown^Hcloud, since in that case you cannot trust the provider not to get raided or shut down, even if the government is after someone else.&#xD;&#xA;&#xD;&#xA;Just remember that encrypted files cannot be compressed or deduplicated, although you can of course compress the data before you encrypt it." />
  <row Id="568" PostHistoryTypeId="5" PostId="222" RevisionGUID="6270aebe-33ab-432c-b09c-ecf835214195" CreationDate="2013-03-07T20:16:57.437" UserId="21" Comment="added 204 characters in body" Text="what tools are you using for identification of **epub** or **mobi** formats nowadays?&#xD;&#xA;&#xD;&#xA;for my private library i save every book i've bought (either epub or mobi) removing drm. i would keep also detailed metadata about these files.&#xD;&#xA;&#xD;&#xA;[fido][1], the tool i prefer, has some [issues][2].&#xD;&#xA;either jhove or fits are recognizing this files as bitstream.&#xD;&#xA;the only working tool seems [epubcheck][3], but only for epub files.&#xD;&#xA;&#xD;&#xA;[converting][4] everything to epub could be a solution (because it's an open format against amazon proprietary .mobi o .azw), but sometimes the converted file looks different than original one with an ebook reader.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/openplanets/fido&#xD;&#xA;  [2]: https://github.com/openplanets/fido/issues/32&#xD;&#xA;  [3]: https://code.google.com/p/epubcheck/wiki/Extraction&#xD;&#xA;  [4]: http://manual.calibre-ebook.com/cli/ebook-convert.html" />
  <row Id="569" PostHistoryTypeId="2" PostId="227" RevisionGUID="21e9534e-f21c-484d-83f4-983ed224fedd" CreationDate="2013-03-07T21:29:24.093" UserId="31" Text="During last year's file format ID hack, the BL team came up with some Apache Tika signatures for some eBook formats (you can find them in [this magic file](https://github.com/openplanets/format-corpus/blob/master/tools/fidget/src/main/resources/org/apache/tika/mime/custom-mimetypes.xml)). Although set up on Tika, these should be easy to port to Fido." />
  <row Id="570" PostHistoryTypeId="5" PostId="227" RevisionGUID="8398f747-d06e-47e4-a7f0-e44fdd2217aa" CreationDate="2013-03-07T23:03:13.380" UserId="31" Comment="Added links to make things clearer." Text="During [last year's file format ID hack](http://wiki.curatecamp.org/index.php/CURATEcamp_24_hour_worldwide_file_id_hackathon_Nov_16_2012), the [British Library team](http://www.openplanetsfoundation.org/blogs/2012-11-19-identifying-ebooks-file-id-hackathon) came up with some Apache Tika signatures for some eBook formats (you can find them in [this magic file](https://github.com/openplanets/format-corpus/blob/master/tools/fidget/src/main/resources/org/apache/tika/mime/custom-mimetypes.xml)). Although set up on Tika, these should be easy to port to Fido." />
  <row Id="571" PostHistoryTypeId="5" PostId="222" RevisionGUID="db822fec-03dd-4dcf-b3c0-58d865278637" CreationDate="2013-03-08T09:05:43.423" UserId="21" Comment="deleted 276 characters in body" Text="which tools are you using for identification of **epub** or **mobi** formats nowadays?&#xD;&#xA;&#xD;&#xA;for my private library i save every book i've bought (either epub or mobi) removing drm. i would keep also detailed metadata about these files.&#xD;&#xA;&#xD;&#xA;[fido][1], the tool i prefer, has some [issues][2].&#xD;&#xA;either jhove or fits are recognizing this files as bitstream.&#xD;&#xA;the only working tool seems [epubcheck][3], but only for epub files.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/openplanets/fido&#xD;&#xA;  [2]: https://github.com/openplanets/fido/issues/32&#xD;&#xA;  [3]: https://code.google.com/p/epubcheck/wiki/Extraction&#xD;&#xA;" />
  <row Id="572" PostHistoryTypeId="5" PostId="222" RevisionGUID="3dadc144-eac5-4845-9c94-d26df66c921a" CreationDate="2013-03-08T14:44:18.480" UserId="94" Comment="Sharpened up the title to make it clearer, and edited some typos and punctuation" Text="Which tools are you using for identification of **epub** or **mobi** formats?&#xD;&#xA;&#xD;&#xA;For my private library I save every book I've bought (either epub or mobi), removing drm. I would keep also detailed metadata about these files.&#xD;&#xA;&#xD;&#xA;[Fido][1], the tool I prefer, has some [issues][2].&#xD;&#xA;Either jhove or fits are recognizing these files as bitstreams only.&#xD;&#xA;The only working tool seems to be [epubcheck][3], but only for epub files.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/openplanets/fido&#xD;&#xA;  [2]: https://github.com/openplanets/fido/issues/32&#xD;&#xA;  [3]: https://code.google.com/p/epubcheck/wiki/Extraction&#xD;&#xA;" />
  <row Id="573" PostHistoryTypeId="4" PostId="222" RevisionGUID="3dadc144-eac5-4845-9c94-d26df66c921a" CreationDate="2013-03-08T14:44:18.480" UserId="94" Comment="Sharpened up the title to make it clearer, and edited some typos and punctuation" Text="Which tools are most effective for identifying the format of ebook (mobi, epub) files?" />
  <row Id="574" PostHistoryTypeId="6" PostId="222" RevisionGUID="3dadc144-eac5-4845-9c94-d26df66c921a" CreationDate="2013-03-08T14:44:18.480" UserId="94" Comment="Sharpened up the title to make it clearer, and edited some typos and punctuation" Text="&lt;file-formats&gt;&lt;jhove&gt;&lt;fits&gt;&lt;fido&gt;&lt;file-format-id&gt;" />
  <row Id="575" PostHistoryTypeId="24" PostId="222" RevisionGUID="3dadc144-eac5-4845-9c94-d26df66c921a" CreationDate="2013-03-08T14:44:18.480" Comment="Proposed by 94 approved by 21 edit id of 23" />
  <row Id="576" PostHistoryTypeId="5" PostId="64" RevisionGUID="dc6339be-0beb-4c53-8c05-3517a2eea501" CreationDate="2013-03-08T14:57:12.317" UserId="94" Comment="Minor grammar fixes" Text="PDF is almost a de facto standard when it comes to exchanging documents. One of the best things is that always, on each machine, the page numbers stay the same, so it can be easily cited in academic publications etc.&#xD;&#xA;&#xD;&#xA;But de facto standard is also opening PDFs with Acrobat Reader. So the single company is making it all functioning fluently. &#xD;&#xA;&#xD;&#xA;However, thinking in longer perspective, say 50 years, is it a good idea to store documents as PDFs? Is the PDF format documented good enough to ensure that after 50 years it will be relatively easy to write software that will read such documents, taking into account that PDF may be then completely deprecated and  no longer supported?" />
  <row Id="577" PostHistoryTypeId="4" PostId="64" RevisionGUID="dc6339be-0beb-4c53-8c05-3517a2eea501" CreationDate="2013-03-08T14:57:12.317" UserId="94" Comment="Minor grammar fixes" Text="Is the PDF format appropriate for preserving documents with long perspective?" />
  <row Id="578" PostHistoryTypeId="24" PostId="64" RevisionGUID="dc6339be-0beb-4c53-8c05-3517a2eea501" CreationDate="2013-03-08T14:57:12.317" Comment="Proposed by 94 approved by 82 edit id of 24" />
  <row Id="579" PostHistoryTypeId="4" PostId="171" RevisionGUID="c1f17c40-83f3-43d2-8338-545574baa6e3" CreationDate="2013-03-08T17:40:01.507" UserId="94" Comment="Made the question title more explanatitive" Text="What are the benefits of certifying a repository as a &quot;Trusted Digital Repository&quot; (ISO 16363)" />
  <row Id="580" PostHistoryTypeId="24" PostId="171" RevisionGUID="c1f17c40-83f3-43d2-8338-545574baa6e3" CreationDate="2013-03-08T17:40:01.507" Comment="Proposed by 94 approved by 101 edit id of 25" />
  <row Id="581" PostHistoryTypeId="2" PostId="228" RevisionGUID="abf7f4ef-324c-4f7d-9db9-67ffe104aff6" CreationDate="2013-03-09T07:06:42.990" UserId="9" Text="There is a lot of good info on this site about long term storage, but what about short term where security is an issue.  I'm thinking about things like in house or public SSL/PKI keys or password lists where a replacement is not possible or not feasible.&#xD;&#xA;&#xD;&#xA;These things are small enough that multiple copies is not a problem, but the more copies the more opportunity there is for someone to break the encryption and limiting copies means you really have to be sure not to lose it.&#xD;&#xA;&#xD;&#xA;What kind of media would you use? And where/how would you store it?&#xD;&#xA;&#xD;&#xA;At the moment, we burn the encrypted data to CD and a USB stick and put them in a safe deposit box. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="582" PostHistoryTypeId="1" PostId="228" RevisionGUID="abf7f4ef-324c-4f7d-9db9-67ffe104aff6" CreationDate="2013-03-09T07:06:42.990" UserId="9" Text="Secure Short Term Archiving" />
  <row Id="583" PostHistoryTypeId="3" PostId="228" RevisionGUID="abf7f4ef-324c-4f7d-9db9-67ffe104aff6" CreationDate="2013-03-09T07:06:42.990" UserId="9" Text="&lt;storage-media&gt;&lt;security&gt;" />
  <row Id="584" PostHistoryTypeId="10" PostId="228" RevisionGUID="e7028b4e-dee0-4526-9777-99675da0d378" CreationDate="2013-03-09T16:59:24.157" UserId="101" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:14,&quot;DisplayName&quot;:&quot;gmcgath&quot;},{&quot;Id&quot;:101,&quot;DisplayName&quot;:&quot;Robert Cartaino&quot;}]}" />
  <row Id="585" PostHistoryTypeId="5" PostId="228" RevisionGUID="cd6bcaea-60e6-4600-a751-402962a38cf0" CreationDate="2013-03-09T19:46:06.457" UserId="9" Comment="refactor based on feedback" Text="There is a lot of good info on this site about long term storage, but what about short term where security is an issue.  I'm thinking about things like in house or public SSL/PKI keys or password lists where a replacement is not possible or not feasible.&#xD;&#xA;&#xD;&#xA;These things are small enough that multiple copies is not a problem, but the more copies the more opportunity there is for someone to break the encryption and limiting copies means you really have to be sure not to lose it.&#xD;&#xA;&#xD;&#xA;At the moment, we burn the encrypted data to CD and a USB stick and put them in a safe deposit box. &#xD;&#xA;&#xD;&#xA;What kind of media would you use? And where/how would you store it?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="586" PostHistoryTypeId="5" PostId="200" RevisionGUID="07cdbcaa-3337-4cb3-9f3b-9a8e3c753b70" CreationDate="2013-03-11T13:28:35.653" UserId="4" Comment="formatting and clarity" Text="A few weeks ago a [message][1] on the [DIGITAL-PRESERVATION@JISCMAIL.AC.UK][2] list announced the TDR certification of Scholars Portal.&#xD;&#xA;&#xD;&#xA;I've found their [wiki][3] extremely useful and clear with a well documented audit progress. For example:&#xD;&#xA;&#xD;&#xA; - [Document Checklist][4] &#xD;&#xA; - [TRAC2 Progress Overview][5] &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ocul.on.ca/node/1637&#xD;&#xA;  [2]: https://www.jiscmail.ac.uk/cgi-bin/webadmin?A0=digital-preservation&#xD;&#xA;  [3]: http://spotdocs.scholarsportal.info/display/OAIS/Home&#xD;&#xA;  [4]: http://spotdocs.scholarsportal.info/display/OAIS/Document+Checklist&#xD;&#xA;  [5]: http://spotdocs.scholarsportal.info/display/OAIS/TRAC2+Progress+Overview" />
  <row Id="587" PostHistoryTypeId="24" PostId="200" RevisionGUID="07cdbcaa-3337-4cb3-9f3b-9a8e3c753b70" CreationDate="2013-03-11T13:28:35.653" Comment="Proposed by 4 approved by 21 edit id of 26" />
  <row Id="589" PostHistoryTypeId="2" PostId="229" RevisionGUID="1ca72ca6-04a9-4982-8898-c2cbfcee81cd" CreationDate="2013-03-11T19:30:08.053" UserId="91" Text="I tend to use the command line. All modern computers except for those running Windows have the `file` tool which can be used like this:&#xD;&#xA;&#xD;&#xA;    $ file *.epub *.mobi&#xD;&#xA;    Natural Language Processing with Python - Steven Bird.epub: EPUB ebook data&#xD;&#xA;    pg8086.mobi:                                                Mobipocket E-book &quot;Down_and_Out-_Magic_Kingdom&quot;&#xD;&#xA;&#xD;&#xA;Here, I used `file` to identify the format of all files that have the `.epub` or `.mobi` file extension, but I could have used the asterisk alone to identify all non-hidden files in the current directory. So in this little experiment, `file` successfully identified the two e-book formats, and for the mobi(pocket) format, it was able to extract the title (or a short form of it)." />
  <row Id="590" PostHistoryTypeId="2" PostId="230" RevisionGUID="ee3a72b3-054b-4f0f-b902-7ff379cfdad5" CreationDate="2013-03-12T05:37:57.563" UserId="124" Text="What are some ways to automatically generate descriptive metadata for warcs, or what are the best tools for parsing warcs?&#xD;&#xA;&#xD;&#xA;I'm looking to generate as much descriptive metadata (DC) as possible for a given crawl to then be ingested into a repository. &#xD;&#xA;&#xD;&#xA;I've come across the Internet Archive's [warc][1], a Python library, and [warc-tools][2], another Python library.&#xD;&#xA;&#xD;&#xA;warc looks like it can put out a fair bit of what could be used as descriptive metadata. But, what about parsing some actual html tags (e.g., `&lt;title&gt;foo&lt;/title&gt;`)?&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/internetarchive/warc&#xD;&#xA;  [2]: http://code.hanzoarchives.com/warc-tools" />
  <row Id="591" PostHistoryTypeId="1" PostId="230" RevisionGUID="ee3a72b3-054b-4f0f-b902-7ff379cfdad5" CreationDate="2013-03-12T05:37:57.563" UserId="124" Text="What are some ways to automatically generate descriptive metadata for warcs?" />
  <row Id="592" PostHistoryTypeId="3" PostId="230" RevisionGUID="ee3a72b3-054b-4f0f-b902-7ff379cfdad5" CreationDate="2013-03-12T05:37:57.563" UserId="124" Text="&lt;web-archiving&gt;&lt;metadata&gt;" />
  <row Id="593" PostHistoryTypeId="2" PostId="231" RevisionGUID="92eb8c6c-b0e3-4516-b62a-b24ce81d40c8" CreationDate="2013-03-12T07:24:18.167" UserId="21" Text="refer to this discussion on [library.stackexchange][1] : [Characterization of WARC files contents?][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://libraries.stackexchange.com/&#xD;&#xA;  [2]: http://libraries.stackexchange.com/questions/1303/characterization-of-warc-files-contents/" />
  <row Id="594" PostHistoryTypeId="2" PostId="232" RevisionGUID="17c51c68-4e83-4f6b-a14a-b7e9ff783d06" CreationDate="2013-03-12T11:30:07.013" UserId="21" Text="i played a bit with [warc][1]. with the following python script (it's quick and dirty) you can analyse all response records with tika, and save the json output in a directory (files named as record-uuid.json)&#xD;&#xA;&#xD;&#xA;for html content the result is good, otherwise images are recognized as application/octet-stream. i guess that is *record.payload* including also http headers&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    import warc&#xD;&#xA;    import subprocess&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    if len(sys.argv) &lt; 2:&#xD;&#xA;        sys.exit('Usage: %s warcfile' % sys.argv[0])&#xD;&#xA;    &#xD;&#xA;    warcfile = sys.argv[1]&#xD;&#xA;    &#xD;&#xA;    f = warc.open(warcfile)&#xD;&#xA;    for record in f:&#xD;&#xA;    	if record.header.type == &quot;response&quot;:&#xD;&#xA;    		uuid = record.header.record_id.split(&quot;:&quot;)[2][:-1]&#xD;&#xA;    		&#xD;&#xA;    		process = subprocess.Popen([&quot;tika&quot;, &quot;-m&quot;, &quot;-j&quot;], &#xD;&#xA;    			stdin=subprocess.PIPE, stdout=subprocess.PIPE)&#xD;&#xA;    &#xD;&#xA;    		process.stdin.write(&quot;{}\n&quot;.format(record.payload.read()))&#xD;&#xA;    		process.stdin.flush()&#xD;&#xA;    		process.stdin.close()&#xD;&#xA;    		&#xD;&#xA;    		out = open(&quot;metadata/{}.json&quot;.format(uuid), &quot;w&quot;)&#xD;&#xA;    		out.write(process.stdout.read())&#xD;&#xA;    		print uuid&#xD;&#xA;    		out.close&#xD;&#xA;    		process.wait()&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/internetarchive/warc" />
  <row Id="595" PostHistoryTypeId="2" PostId="233" RevisionGUID="978b73be-331f-4310-bd89-8013572675b2" CreationDate="2013-03-12T15:06:05.693" UserId="119" Text="No, I am **not** about to ask what type of DVDs are &quot;archival.&quot;&#xD;&#xA;&#xD;&#xA;I am pondering the following… For the preservation of a DVD, where the DVD format itself is significant (i.e. the interactive experience of the menus, etc), what could be considered a preservation format?&#xD;&#xA;&#xD;&#xA;Is a rip that produces audio TS / video TS folders sufficient for longevity, or is a raw, sector level disc image the best approach?" />
  <row Id="596" PostHistoryTypeId="1" PostId="233" RevisionGUID="978b73be-331f-4310-bd89-8013572675b2" CreationDate="2013-03-12T15:06:05.693" UserId="119" Text="DVD preservation format" />
  <row Id="597" PostHistoryTypeId="3" PostId="233" RevisionGUID="978b73be-331f-4310-bd89-8013572675b2" CreationDate="2013-03-12T15:06:05.693" UserId="119" Text="&lt;disk-image&gt;&lt;dvd&gt;&lt;rip&gt;&lt;dump&gt;" />
  <row Id="598" PostHistoryTypeId="2" PostId="234" RevisionGUID="bc5244bc-e4ee-4824-91fe-1cb5683c26bd" CreationDate="2013-03-12T21:43:36.250" UserId="31" Text="Well, DVDs are in [Universal Disk Format](http://en.wikipedia.org/wiki/Universal_Disk_Format) - extended from the ISO9660 format - so if you rip them directly as block devices that's what you get, and you get everything. I believe the region encryption is done within this container, and there are no 'tracks' as with CD-ROMS (or rather there is always just one data track).&#xD;&#xA;&#xD;&#xA;Disclaimer: There may be fancy DVD formats that use more tracks, weird copy protection, or other unusual stuff." />
  <row Id="599" PostHistoryTypeId="2" PostId="235" RevisionGUID="74a38b42-9221-41d5-9141-d3d8e96005ec" CreationDate="2013-03-14T15:06:34.363" UserId="142" Text="I am wondering what the recommended practice is for retaining event metadata generated by fixity checks.  Let's say my collection is in the range of 100 TB. If I have a process running fixity checks on stored archival data continuously (because it's going to take a while to get through everything), this will produce a sizeable log of fixity check event data: file path, hash value, date/time, outcome, etc. Storing this data for every fixity event in a PREMIS XML file seems inefficient and unwieldy (of course, storing the original hash value/algorithm/datetime created in PREMIS makes sense -- this is presumed to be the data that future fixity checks would validate against).  Likewise, I'm not sure if storing this data in a database is worthwhile either. It seems that text files with fixity event logs would be the more lightweight option.  My interrelated questions on this issue are:&#xD;&#xA;&#xD;&#xA; - How do most repositories store fixity event outcome data? Log, data store, XML (PREMIS or other)?&#xD;&#xA; - If the outcome of the fixity check is positive, should I retain the data? Or should the system overwrite the old data when the process begins again? &#xD;&#xA; - If the fixity outcome data is retained, for how many events, and for how long?  Should I only keep the data for the last 2 or 3 events, or keep them all indefinitely?&#xD;&#xA;&#xD;&#xA;I know some of this is a question of retention policy and specific to my repository's architecture, and that could be case by case for each repository, but I'm wondering what the general recommended practice is for retention and storage of this data." />
  <row Id="600" PostHistoryTypeId="1" PostId="235" RevisionGUID="74a38b42-9221-41d5-9141-d3d8e96005ec" CreationDate="2013-03-14T15:06:34.363" UserId="142" Text="What is the recommended practice for retaining logs of fixity checks?" />
  <row Id="601" PostHistoryTypeId="3" PostId="235" RevisionGUID="74a38b42-9221-41d5-9141-d3d8e96005ec" CreationDate="2013-03-14T15:06:34.363" UserId="142" Text="&lt;checksum&gt;&lt;fixity&gt;&lt;hash&gt;&lt;integrity&gt;&lt;retention&gt;" />
  <row Id="602" PostHistoryTypeId="2" PostId="236" RevisionGUID="b51f9c7b-f6dd-41f2-ab4e-53017c175c24" CreationDate="2013-03-14T15:25:38.300" UserId="142" Text="If this is the only copy you have of this film that you can use for preservation purposes, you have to now consider it the original, and treat it as such. If this is an &quot;authored&quot; DVD, and not a data DVD, the video has already been encoded and compressed (using lossy compression) to a particular format: [MPEG-2][1] with a colorspace of 4:2:0. In order to create an archival video file from this DVD, you should capture it at its native encoding and resolution.  Using VLC or ffmpeg (if you are comfortable on the command line), capture the MPEG-PS or TS file without applying addition compression or transcoding. You will have the native file and the highest possible resolution in a self-contained MPEG file. &#xD;&#xA;&#xD;&#xA;To answer your question &quot;Is the DVD footage already compressed, and thus not worth extracting as uncompressed footage?&quot; the answer is yes: this is heavily compressed content and you will not gain anything by upconverting to uncompressed or transcoding to JPEG2000 except for a bunch of bits and the need for additional storage space.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;If your archive has a normalization policy for video such as what you describe above, you could consider transcoding to this format (taking care to preserve characteristics such as the aspect ratio and frame size). But I would advise keeping the MPEG file as your original master, in line with the archival practices of retaining originals.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/MPEG-2#DVD-Video" />
  <row Id="603" PostHistoryTypeId="2" PostId="237" RevisionGUID="8ed54cbe-6228-45a4-b800-c212d79f8df5" CreationDate="2013-03-14T15:38:12.733" UserId="142" Text="The short answer is: none of these are archival quality. Optical media will be obsolete in just a few years and drives for these are already becoming scarce (my new MacBook Air doesn't have an optical drive!). Optical media also pose digital preservation management challenges as they are limited in how you can provide access, replicate, manage integrity, perform QC, etc.  As access copies for designated communities who want and prefer these formats, that's fine. But preservation media they are not.&#xD;&#xA;&#xD;&#xA;I would strongly advise against giving anyone the impression that optical media should in any way considered to be archival or suitable for long-term preservation. As digital preservation professionals, we have a responsibility to the public and other non-specialists to demystify this notion that there is something archival about optical discs that manufacturers seems to love to claim. What good is a low error rate if there is no drive to read the content? It seems to be a waste of effort to even talk about these issues, and is potentially extremely misleading.  " />
  <row Id="604" PostHistoryTypeId="2" PostId="238" RevisionGUID="7130a2c9-6bcd-49c3-8fc7-c02cb8a6f058" CreationDate="2013-03-14T21:00:13.983" UserId="142" Text="I am assuming that the George Blood paper you are referencing is [Refining Conversion Contract Specifications (2011-10-01)][1].  The section that covers &quot;Digital Source (media independent &quot;file based&quot;)&quot; video is dealing with what is also referred to as &quot;born-digital&quot; video.  These video files are born with specific characteristics, chroma subsampling being one of them. These characteristics should be considered significant properties of those files, for preservation purposes. Chroma subsampling refers to color compression, just as data rate refers to temporal compression.  When we talk about &quot;uncompressed&quot; video, we are usually talking about temporal compression.  When an archive chooses &quot;uncompressed&quot; as a target video spec for reformatting analog material, they often still have color compression, typically 4:2:2. We accept this degree of color sampling because [the human eye is less sensitive to color than it is to luminance][2]. &#xD;&#xA;&#xD;&#xA;But since you referenced born-digital, the issue is slightly different. For the vast majority of these files, the native chroma subsampling is 4:2:2, 4:1:1, or 4:2:0.  There are only a handful of cameras on the market today that are capable of producing 4:4:4 video, and those are generally high-end, professional digital cinema cameras. &#xD;&#xA;&#xD;&#xA;So, in short answer to your question, if we only accept 4:4:4 files we will not be preserving 99%+ of the video content being produced today. And that is unacceptable. &#xD;&#xA;&#xD;&#xA;Digital video production and preservation has changed dramatically in the past 9 years since Jerry MacDonough wrote about 4:4:4 video.  At that time, digital (file based) preservation of video was very theoretical, and certainly not practical.  That's changed to the point where today it is the ONLY option for video preservation.  And costs have dropped to the point where it is feasible and realistic to create temporally uncompressed video. But generally speaking, there is no advantage to 4:4:4, and few systems that support its creation at this stage. That may change in the future, but for now, we are just not there.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://dl.dropbox.com/u/81562888/George%20Blood%20LIbrary%20of%20Congress%20IntrmMastVidFormatRecs_20111114.pdf&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Chroma_subsampling" />
  <row Id="605" PostHistoryTypeId="5" PostId="226" RevisionGUID="7019d7fa-b9e2-4880-af70-23aee6a9029f" CreationDate="2013-03-15T00:40:51.220" UserId="64" Comment="Avoid commentary unrelated to question" Text="Encryption is usually used for keeping secrets, and if all you are concerned with is copyright (which only governs who is allowed to distribute a work), then the material is probably not secret.&#xD;&#xA;&#xD;&#xA;I can see it being useful for a corporation that is archiving or backing up its internal communications, source code, etc. You could also use it whenever you have to store your data in the cloud, since in that case you cannot trust the provider not to get raided or shut down, even if the government is after someone else.&#xD;&#xA;&#xD;&#xA;Just remember that encrypted files cannot be compressed or deduplicated, although you can of course compress the data before you encrypt it." />
  <row Id="606" PostHistoryTypeId="24" PostId="226" RevisionGUID="7019d7fa-b9e2-4880-af70-23aee6a9029f" CreationDate="2013-03-15T00:40:51.220" Comment="Proposed by 64 approved by 101 edit id of 27" />
  <row Id="607" PostHistoryTypeId="2" PostId="239" RevisionGUID="432ec7d3-d22d-4910-8984-d1120c40b8bc" CreationDate="2013-03-15T01:27:00.450" UserId="166" Text="I work in data management for an electron microscopy centre.  One of the things I hear &quot;from people&quot; is that Journals in the field do not like data compression.  The most extreme version I've heard is that some Journals insist that images not be compressed at all ... ever.  Apparently, no distinction between lossy and lossless compression.  (Or at least that's what I was told.)&#xD;&#xA;&#xD;&#xA;Is there any truth in this?&#xD;&#xA;&#xD;&#xA;I've seen Journal instructions that say that images should be captured and stored without lossy compression ... which is fair enough.  But what about lossless compression?&#xD;&#xA;&#xD;&#xA;(Obviously, it is nonsensical from a technical perspective to blanket ban all lossless compression ...)" />
  <row Id="608" PostHistoryTypeId="1" PostId="239" RevisionGUID="432ec7d3-d22d-4910-8984-d1120c40b8bc" CreationDate="2013-03-15T01:27:00.450" UserId="166" Text="Microscopy, journals and compression" />
  <row Id="609" PostHistoryTypeId="3" PostId="239" RevisionGUID="432ec7d3-d22d-4910-8984-d1120c40b8bc" CreationDate="2013-03-15T01:27:00.450" UserId="166" Text="&lt;imaging&gt;&lt;compression&gt;" />
  <row Id="610" PostHistoryTypeId="14" PostId="212" RevisionGUID="fc7c47f8-a7b7-41bc-9d9f-b6dde78b87fb" CreationDate="2013-03-18T17:46:22.003" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:-1,&quot;DisplayName&quot;:&quot;Community&quot;}]}" />
  <row Id="611" PostHistoryTypeId="35" PostId="212" RevisionGUID="383c7c0d-d0e0-4cbb-b9e3-6bfa891d54e6" CreationDate="2013-03-18T17:46:22.003" UserId="10" Comment="to http://libraries.stackexchange.com/questions/1434/how-to-deal-with-preservation-of-software-with-unknown-author-and-licence" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:10,&quot;DisplayName&quot;:&quot;Anna Lear&quot;}]}" />
  <row Id="614" PostHistoryTypeId="10" PostId="212" RevisionGUID="df6da1b0-7abe-4ca6-b7ac-968ff2293978" CreationDate="2013-03-18T17:46:22.003" UserId="10" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:10,&quot;DisplayName&quot;:&quot;Anna Lear&quot;}]}" />
  <row Id="615" PostHistoryTypeId="14" PostId="61" RevisionGUID="5e197f3e-4059-4a47-8fca-ee8b3100cbb4" CreationDate="2013-03-18T17:47:32.363" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:-1,&quot;DisplayName&quot;:&quot;Community&quot;}]}" />
  <row Id="616" PostHistoryTypeId="35" PostId="61" RevisionGUID="7011f51e-6389-47e0-b3d9-5d7bb6f2220b" CreationDate="2013-03-18T17:47:32.363" UserId="10" Comment="to http://superuser.com/questions/567869/how-to-store-cd-discs-to-preserve-the-data-longer" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:10,&quot;DisplayName&quot;:&quot;Anna Lear&quot;}]}" />
  <row Id="618" PostHistoryTypeId="10" PostId="61" RevisionGUID="d8493569-4c41-45ac-815e-48e4468f6116" CreationDate="2013-03-18T17:47:32.363" UserId="10" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:10,&quot;DisplayName&quot;:&quot;Anna Lear&quot;}]}" />
  <row Id="619" PostHistoryTypeId="14" PostId="64" RevisionGUID="8863f2a5-1b77-4bbf-ab00-5d19342ed48f" CreationDate="2013-03-18T17:57:56.320" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:-1,&quot;DisplayName&quot;:&quot;Community&quot;}]}" />
  <row Id="620" PostHistoryTypeId="35" PostId="64" RevisionGUID="5b9cb9b9-8a63-442e-9054-93918cfc75f7" CreationDate="2013-03-18T17:57:56.320" UserId="10" Comment="to http://libraries.stackexchange.com/questions/1437/is-the-pdf-format-appropriate-for-preserving-documents-with-long-perspective" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:10,&quot;DisplayName&quot;:&quot;Anna Lear&quot;}]}" />
  <row Id="625" PostHistoryTypeId="10" PostId="64" RevisionGUID="484b5fee-c2c5-4c63-89c6-55f5155b884f" CreationDate="2013-03-18T17:57:56.320" UserId="10" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:10,&quot;DisplayName&quot;:&quot;Anna Lear&quot;}]}" />
  <row Id="626" PostHistoryTypeId="14" PostId="65" RevisionGUID="a1469a67-8dc8-4164-a1d7-0b34a5764d41" CreationDate="2013-03-19T20:23:46.260" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:-1,&quot;DisplayName&quot;:&quot;Community&quot;}]}" />
  <row Id="627" PostHistoryTypeId="35" PostId="65" RevisionGUID="a8b0f0a3-2640-4887-b7b8-fcb14e72d654" CreationDate="2013-03-19T20:23:46.260" UserId="82" Comment="to http://superuser.com/questions/568555/most-efficient-way-to-generate-and-validate-file-checksums" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:82,&quot;DisplayName&quot;:&quot;Tim Post&quot;}]}" />
  <row Id="634" PostHistoryTypeId="10" PostId="65" RevisionGUID="3d06696a-3796-4ec7-a1e3-0f315646382e" CreationDate="2013-03-19T20:23:46.260" UserId="82" CloseReasonId="2" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:82,&quot;DisplayName&quot;:&quot;Tim Post&quot;}]}" />
</posthistory>