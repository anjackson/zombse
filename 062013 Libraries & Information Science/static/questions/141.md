---
title: "Metrics for value of online resources"
layout: default
---
Metrics for value of online resources
=====================
Our library (science-based government department, but we write policy
too) is increasingly focused on providing online access to resources.
This is great because it facilitates access to information for the
department's employees, but it invalidates many traditional metrics for
library use (circulation, visits, etc). We're finding that that the
number of searches isn't the right number for conveying ROI to senior
management. What metrics are you using? Is there a simple answer here? I
can use Scopus to measure citations - have you used bibliometrics
successfully in this domain? That would show the impact of our org's
published research, but not necessarily which library resources
contributed to that research. I'm trying to get our "story" straight in
an organizational context of budget cuts.

Emily Gusba

### Comments ###


Answer by David Rothman
----------------
It really depends what you're attempting to measure.

Your subject line indicates interest in metrics for online resources,
then you mention scopus, which provides some metrics for journal
articles...which aren't 'online resources. They are items which can be
accessed online, but that's not necessarily the same thing.

If you're trying to measure the impact/importance/significance of a
particular article, we're talking about Bibliometrics.
[http://en.wikipedia.org/wiki/Bibliometrics](http://en.wikipedia.org/wiki/Bibliometrics)

See also:

-   [http://en.wikipedia.org/wiki/SCImago\_Journal\_Rank](http://en.wikipedia.org/wiki/SCImago_Journal_Rank)
-   [http://en.wikipedia.org/wiki/Impact\_factor](http://en.wikipedia.org/wiki/Impact_factor)
-   [http://en.wikipedia.org/wiki/Eigenfactor](http://en.wikipedia.org/wiki/Eigenfactor)
-   [http://en.wikipedia.org/wiki/H-index](http://en.wikipedia.org/wiki/H-index)
-   [http://en.wikipedia.org/wiki/G-index](http://en.wikipedia.org/wiki/G-index)

Hope that helps!

### Comments ###
* Ashley Nunn: I know that Wikipedia isn't prone to link rot, but just in case, would
you terribly mind summing up the important bits of those articles in
your answer?
* David Rothman: Ashley- No. Bibliometrics is a largish topic taht doesn't lend itself
well to summarizing. Any librarian should be able to find what he/she is
looking for just with the keywords given.
* David Rothman: Another thought: If the goal is to find out what journals YOUR partrons
use, please advise if there is a particular system through which they
mostly access full text digitally? (SCOPUS, OVID, etc.) Most of these
have functionality to measure usage. For instance, SCOPUS (which you
seem to have access to, can do this through the Admin Tool.
http://www.info.sciverse.com/sciverse/admintool\#usage
* Joe: If you're going to mention bibliometrics, it's worth mentioning the
[altmetrics](http://altmetrics.org/manifesto/) effort to find better
measures of article's worth. See also the [article in the Chronicle for
Higher
Ed](http://chronicle.com/article/As-Scholarship-Goes-Digital/130482/)
and consider signing the [Open impact metrics
petition](http://total-impact.tumblr.com/post/23718739004/open-impact-metrics-need-openaccess-please-sign)

Answer by Ed Summers
----------------
Web analytics software like Google Analytics offer some really exciting
possibilities similar to what circulation statistics have traditionally
offered libraries. For example you can see what Web resources are viewed
more than others, how long people spend on various pages, when they
leave your website, etc which can help guide digitization efforts. If
you have an institutional repository, spending a bit of time making your
resources crawlable, and tracking visits can pay dividends also in user
centered design. For an example of this I recommend Chris Prom's
excellent [Using Web Analytics to Improve Online Access to Archival
Resources](http://archivists.metapress.com/content/h56018515230417v/?p=8656de47de654caaa1e0ca74d3d745dc&pi=2)
which appeared in American Archivist last year.

### Comments ###

Answer by phette23
----------------
I interpreted this question very differently from David so I thought I
would share a couple more resources that might help. While David's
resources are all related to *journal prominence*, one can also evaluate
usage statistics at a particular library.

The most pertinent project here is
[COUNTER](http://www.projectcounter.org/), an attempt at getting
standardized & thus comparable statistics from database vendors. One can
retrieve the number of searches, sessions, downloads, & other metrics at
various levels of granularity: by database, by journal, by ebook. I tend
to rely on Database Report 1 which gives "Total Searches, Result Clicks
and Record Views by Month and Database" but other libraries might be
better suited to Journal Report 5 ("Number of Successful Full-Text
Article Requests by Year-of-Publication (YOP) and Journal") or Book
Report 1 ("Number of Successful Title Requests by Month and Title")
depending on whether they focus on e-journals or e-books. See the
[COUNTER Code of Practice](http://www.projectcounter.org/r4/COPR4.pdf)
[.pdf] for details & sample reports.

There are also [ICOLC](http://icolc.net/2001webstats.htm) statistics
which is a similar project but I've only seen a few resources report
those.

I'm not sure if that's what you were asking. If you want to know how the
library enables faculty research output in general and not by resource,
then the best analysis would probably be to review the citation lists of
faculty publications to see how many come from library resources. You
could then make powerful statements such as "these would cost 15
trillion dollars total at \$15/article if the library did not provide
access."

### Comments ###
* David Rothman: Agree with Ed and Phette- if usage at a single institution is what's
needed. In re-reading the question, I suspect Ed and Phette read the
question properly and I did not.

Answer by trevormunoz
----------------
I think it's important to take a broader view of this question than just
measuring journal impact but there is ambiguity in the OP from the
examples cited.

Another relevant resource is a recent study that Simon Tanner of King's
Digital Consultancy Service completed for JISC, a major funder in the
UK, of the "values, benefits and impacts of digitised resources." The
study is available from here:
[http://www.kdcs.kcl.ac.uk/innovation/inspiring.html](http://www.kdcs.kcl.ac.uk/innovation/inspiring.html)
This report is explicitly aimed at policy-makers.

A few findings from the report to add to list above include:

-   Measurements of use in classrooms (uploads to learning management
    software
-   Use of materials in student portfolios, distance-learning modules
-   Amount user-uploaded or user-generated content (if library systems
    enable this)


### Comments ###

Answer by Stephanie Willen Brown
----------------
I look at usage data for various databases, and I find that the raw
numbers (ie, 7,000 searches in LexisNexis) are not particularly helpful.
However, comparing the number of searches in one database vs. another is
helpful (ie, 7,000 searches in LexisNexis vs 25,000 searches in
America's News tells me something about which database is used more by
my patrons). Also helpful is comparing the number of searches over time
(ie 7,000 searches in LexisNexis in 2010 vs 5,000 searches in LexisNexis
in 2011 means that something happened between 2010 & 2011.)

Another useful comparison, if you can get it from COUNTER or the vendor,
is the number of sessions vs searches vs downloads. If there are 2,000
sessions and 500 searches, it could mean that users aren't able to
navigate the database interface (if there are way fewer searches than
sessions); my sense is that there should be at least as many searches as
sessions on average. If there are 500 searches and only 10 downloads,
that means users aren't finding what they want in the search results --
or that they aren't able to figure out *how* to download the articles
they want to read.

hope that helps.

### Comments ###

