---
title: "Is JPEG2000 really a good preservation format?"
layout: default
---
Is JPEG2000 really a good preservation format?
=====================
I've been wondering of late about JPEG2000. I appreciate there are some
good reasons to use it as a preservation format, but I'm becoming
increasingly alarmed by lack of tool support. This format has been
around for 12 years, and yet there are very few tools that
manipulate/render these images and of these, it seems only OpenJPEG is
open source and freely available. Sun/Oracle's Java Advanced Imaging API
is defunct and along with it Java support for JPEG2000.

If lack of popularity is a problem for long-term digital preservation,
why are we still banging the JPEG2000 drum?

Peter Cliff

<ul class="tags"><li class="tag">digital-preservation</li></ul>

### Comments ###


Answer by Paul Wheatley
----------------
There are significant concerns about longevity as you note in the
question, however there are two big wins for the archival community in
using JP2: access/delivery features and compression. The latter in
particular has led to JP2 being adopted quite rapidly by libraries for
storing master files from mass digitisation projects (i.e. 1 million+
images). Short term storage cost savings can be quite significant when
the data is squashed down to around 10% of its original size without
huge loss of quality. Whether this turns out to be a a sensible choice
over the medium to long-term is less clear.

JPEG2000 expert, [Johan van der Knijff identifies 5 specific
preservation risks for JP2](http://wiki.opf-labs.org/display/TR/JP2):

1.  Files identified as JP2 are really JPX (JPEG 2000 Part 2)
2.  Resolution not in expected header fields
3.  Handling of ICC profiles
4.  Effects of bit and byte corruption
5.  Lack of performant open source decoding software

A number of these risks have been reduced or mitigated by the emergence
of a thorough validation tool:
[Jpylyzer](http://wiki.opf-labs.org/display/TR/Jpylyzer), which was
developed in direct response to a number of specific [JP2 related risks
and use
cases](http://wiki.opf-labs.org/display/AQuA/JISC1+19th+Century+Digitised+Newspapers+%28BL%29).

The expansion of the last risk lists 3 open source decoders but provides
little in the way of further reassurance on this point.

### Comments ###
* Chris Adams: +1 for mentioning storage cost as a preservation threat - I think this
deserves more attention than it often receives
* Chris Adams: I would also expand the corruption point: for me, one of the most
concerning areas is that some software bends over backwards to load
broken files, potentially moving content well down the pipeline before
anyone notices that the files cannot be processed using other tools. (I
don't want to single out a vendor as I think we've seen cases of this
for every commercial implementation)

Answer by Ed Summers
----------------
Sadly, I think you are right to be alarmed. Johan van der Knijff's [JPEG
2000 for Long-term
Preservation](http://www.dlib.org/dlib/may11/vanderknijff/05vanderknijff.html)
and [Ensuring the suitability of JPEG 2000 for
preservation](http://jpeg2000wellcomelibrary.blogspot.com/2010/12/guest-post-ensuring-suitability-of-jpeg.html)
provide some useful information. But in the end I think having to do
practical work with JP2, particularly using opensource software, is the
best measure of JP2's use as an image preservation format.

We are heavily dependent on the closed source, proprietary [Aware
SDK](http://www.aware.com/imaging/jpeg2000sdk.html) for using JP2 files
as the access copy of newspaper images in the National Digital Newspaper
Program's [Chronicling America](http://chroniclingamerica.loc.gov) web
application. We just haven't found any opensource solutions that let us
use the JP2 at scale. This is a big barrier to us making our software
available to other NDNP partners. It's so much of a barrier that we have
talked about using a high-res JPEG for the access copy, since our
informal testing in the project's web-based zoomviewer failed to show
any noticeable differences with the JPEG2000.

If we have trouble providing access to JP2 data today, this doesn't bode
well for tomorrow. As my colleague [David
Brunton](http://davidbrunton.com) has quipped:

> Preservation is just access, in the future.

### Comments ###
* Chris Adams: Ed: just to clarify, where does "use the JP2 at scale" break down? My
understanding as a bystander is that it's not unusable if you are
reading a master and transcoding the entire image to a new format but
that it's punishingly slower for generating tiles because you're
essentially paying the whole-file decode cost for each tile.
* Ed Summers: JP2 at scale for http://chroniclingamerica.loc.gov/ means \~60 tile and
thumbnail requests/sec. I think paying the whole-file decode would be
acceptable if it performed more like JPEG w/ a library like PIL, than
JP2 implementation's like kakadu or jasper. Perhaps its time to take
another look at OpenJPEG though.
* Ed Summers: my apologies if the sound of my axe grinding is preventing me from
communicating properly :-)
* Peter Cliff: If I have a tool that I pay for and I want to run that on a Hadoop
cluster of say 50 machines do I need to have a licence for every Hadoop
node? Certainly I'd expect the cost to be more in that instance.
Checking software sites, licence details are, naturally, unclear but
clearly this could be a problem for using commercial software 'at
scale'.
* Ed Summers: Peter, yes that's another angle. I believe that the Aware license we
have is per-CPU.
* johan: Since you're referring to my 2011 D-Lib paper: it is worth mentioning
that this mainly focused on 2 ambiguities in the filespec (and the way
different software vendors were dealing with them). These are fixed in
an amendment to the standard, which will be published soon. Here's a
link to the draft text:
http://www.itscj.ipsj.or.jp/sc29/open/29view/29n12288t.doc

Answer by Lars
----------------
I think this whole discussion is about the quality of implementations.
The points that were mentioned only compare pure encoding. Nobody
actually talked about the whole suite like JPIP for streaming. The
things that we did for medicine yielded better results in terms of
quality and speed than standard JPEG implementations. Especially our
current developments for iOS based on Kakadu showed results that are
beyond belief. I know that this sounds like self-marketing but even the
biggest companies haven't implemented what is possible with JPEG2000 and
I talked to many of them. Kakadu is by far the best API, especially
performance-wise.

Noboy will ever implement JPEG2000 with a single line of code like we
were used to and that is the only reason why the adoption rate is low.
What is worse, we all got used to simply copying images. Especially with
mobility coming more and more into play, JPIP beats all current encoding
and streaming approaches. The downside is of course the complexity and
time you need to invest.

Although it is self-marketing, I am happy to show samples to people who
are interested. iOS unfortunately takes until Q1/13 for the public App
Store.

### Comments ###
* Peter Cliff: It isn't just about quality - though that of course is important - it is
also about long-term support, cost and access. As I say above, if I want
to use Kakadu on 50 nodes in a Hadoop cluster what happens? I suspect
it'd cost more than doing it more traditionally on a single machine?
More widely, my main issue is that jpeg2000 just isn't popular or
supported. If it is too hard to implement and maintain, it really isn't
going to stand the test of time is it?
* Lars: The popularity is driven by web and browsers. In that respect you are
fully right and that is where simple JPEG is of course easier. Kakadu's
business model does NOTdepend on workstations but it is a one-time fee.
250 US\$ are required for non-commercial use and then you can use it on
millions of nodes or computers. The commercial licence means that the
costs increase to \~2500 US\$. So your costs should be 250US\$ once,
doable I would say. About long-term: PDF/A-2 now allows JPEG2000, so
that is at least a hint. JPEG will hit its limits soon, thinking about
the increasing size of images.
* Peter Cliff: Kakadu do not make that information available on their site so it is
useful to know - you're right \$250 is probably doable, but perhaps not
for all institutions. I also understand from a licence I've seen that
this is in fact an annual fee?
* Lars: I was a bit wrong, you are right.
http://www.kakadusoftware.com/index.php?option=com\_content&task=blogcategory&id=6&Itemid=12.
Public service is the one you need and this is annual. This helps me to
understand why the adoption rate is not high in some areas.
* Joe: In talking to the folks from the [Helioviewer
Project](http://www.helioviewer.org/), they're using Kakadu for encoding
(I didn't get specifics on which license), but they've released their
own [JPIP server](http://wiki.helioviewer.org/wiki/JPIP\_Server)

Answer by gmcgath
----------------
As another data point, Luratech was strong on supporting JPEG2000 when
the Harvard Library moved to their ICS from Aware (largely on my
recommendation). Since then, they've lost interest, and the last couple
of times we asked for improvements, they asked if we would help finance
them. The last I looked for alternative implementations, there weren't a
lot. I found Kakadu's licensing information very confusing.

### Comments ###

Answer by johan
----------------
In addition to the risks mentioned by Paul Wheatley above, I recently
found out that Adobe already stopped supporting JPEG 2000 altogether in
Photoshop Elements in 2011. More details here:

[http://wiki.opf-labs.org/display/TR/JPEG+2000+support+discontinued+in+Photoshop+Elements](http://wiki.opf-labs.org/display/TR/JPEG+2000+support+discontinued+in+Photoshop+Elements)

This definitely isn't going to help further acceptance by general
audiences (outside the niche markets where JPEG 2000 is widely used,
e.g. digital cinema). It also says something about Adobe's confidence in
the format. This wouldn't necessarily matter, were it not for the fact
that they're leading the market in this segment.

### Comments ###

Answer by Lars
----------------
JPIP for me is about efficiency. If you buy a shirt and your size is XL,
you don't order XXL and cut it back down to XL once it is delivered. But
that happens every day when large images are copied to mobile devices
when only a fraction of the data could be streamed via JPIP. This drives
me crazy on a daily basis in my main area medicine. With the technology
change towards mobile instead of Desktop, the time is right for a change
from jpg to jpeg2000, at least in scientific areas.

### Comments ###

